---
title: 垃圾分类论文
categories:
  - 学习笔记
tags:
  - 垃圾分类论文
toc: false# 是否启用内容索引
---

**数据增强、改进Backbone、改进FPN、改进检测头、改进loss、改进后处理**


## 基础网络

### R-CNN

[B 站论文](https://www.bilibili.com/video/BV1CZ4y1a7NP/?spm_id_from=333.999.0.0&vd_source=602787b9249cd70cfca4def5e041f060)

```python
# 之前都是人工提取特征,用机器学习分类

# 把人工提取特征改成CNN提取特征

# 三个模块
# 1.候选区域生成(Selective search)(2000个)(统一大小)
# 2.特征抽取(扩展16个像素)(AlexNet)
# 3.分类,框回归
```

### SPPnet

```python
# 用CNN提取整个图的特征，把候选区域映射到特征图上
# 最后用空间金字塔(三个层)，一个动态池化层，对候选区域特征图得到固定大小的输出特征，拼接在一起
# 还是用svm分类

# 不能很好的更新cnn权重

```
[FPN,ASFF,PANet,NASFPN,BiFPN结构](https://blog.csdn.net/W1995S/article/details/118028384)
### FPN

`1612`

![](https://image.yayan.xyz/20230331104242.png)

### PANet

`1803:FPN是自上而下,首次提出了自下而上`
![](https://image.yayan.xyz/20230331104133.png)


### CSPnet

`梯度分流,减少计算量和内存`
![](https://image.yayan.xyz/20230425170230.png)

### SPP

`1406`

![](https://image.yayan.xyz/20230331103844.png)

### SPPF

![](https://image.yayan.xyz/20230331152040.png)

### Fast R-CNN

[B 站论文](https://www.bilibili.com/video/BV1y94y1Q7QJ/?spm_id_from=333.999.0.0&vd_source=602787b9249cd70cfca4def5e041f060)

```python
# 针对R-CNN和SPPnet
# 多阶段模型,不同的模块都要分别训练
# 要把提取到的特征存到磁盘里，再给分类器

# 单阶段，不用存特征

# 用CNN得到整张图片的特征
# 根据候选区域在图片上的位置（输入坐标），利用ROI投影获得候选区域在特征图上的特征
# 用ROI池化层（空间金字塔特殊情况，一个层）把候选区域特征转为固定大小的特征图
# 两个并行全连接层，分类、预测坐标

# 其他验证性实验 

# 测试阶段输入图像和候选区域坐标
# 候选区域建议是单独的

```

### Faster R-CNN

[B 站论文](https://www.bilibili.com/video/BV1y94y1Q7QJ/?spm_id_from=333.999.0.0&vd_source=602787b9249cd70cfca4def5e041f060)

```python
# Fast R-CNN还是需要单独的模块生成候选区域投影

# 解决候选区域选择的问题

# RPNs和特征提取层 共享卷积层
# 用n*n的滑动窗口在特征图上提取，传给small network 判断是否能生成候选区域
# 使用三个尺度（128，256，512 1:1,1:2,2:1，九个框）生成k个anchor boxes，（根据数据集设置框的大小），非极大值抑制
# 与标注狂IOU值最大，与标注框IOU值大于0.7  分给正标签

# 交替训练

```

![](https://image.yayan.xyz/20230315150418.png)
![](https://image.yayan.xyz/20230315150330.png)

### Mask R-CNN

```python
# Faster R-CNN的RoI Pooling 是直接取整,会导致实例偏移,对于像素级  不可取

# 把候选区域的特征图转换为固定大小的ROI feature时 也会取整

# 两次误差

# 骨干网络换成ResNet-FPN

# Mask R-CNN使用双线性插值解决缩放的问题

# 增加MASK 分支,三路并行,MASK head两种实现 1.ResNet  2.ResNet+FPN 变成K*M*M 
# K*M*M 大小,K个类别
# 与FCN方法是不同，FCN是对每个像素进行多类别softmax分类，然后计算交叉熵损失，这种做法是会造成类间竞争的

```

---

## 论文

### Analysis of Object Detection Performance Based on Faster RCNN

`基于Faster R-CNN的目标检测性能分析`

`介绍了R-CNN->Fast R-CNN-> Faster R-CNN的变化过程`
`Faster R-CNN的大概结构`

`对比三个模型在不同数据集上的效果`

### End-to-End Object Detection with Transformers

`里程碑:端到端的方法,不用非极大值抑制`
`变成集合预测问题`
`CNN抽取特征－＞送入Transformer学习全局特征->输出100个框->二分图loss匹配真实框->计算loss`

`问题:小目标,训练epoch长`

### EfficientDet: Scalable and Efficient Object Detection

![](https://image.yayan.xyz/20230317135128.png)

`新的结构,多层特征融合`

### Deformable DETR: Deformable Transformers for End-to-End Object Detection

`解决DETR的两个问题`
![](http://image.yayan.xyz/20230316214546.png)
![](http://image.yayan.xyz/20230316215004.png)

```
1.不用TRansformer原有的自注意力,改为可变注意力(可变卷积变来的)
一个像素向量z根据偏移量选择四个其他像素,然后一层Liner得到权重,和选出的像素进行运算更新

2.多尺度的注意力机制(Mulit-Scale),不同尺度的特征图上做,多头可变注意力机制,然后相加
```

### ★Deep learning-based waste detection in natural and urban environments

`传统图像分类网络:ResNet,DenseNet,EfficientNet,EfficientNet-B2,EfficientNetv2`

`经典目标检测网络:R-CNN,Fast R-CNN ,Faster R-CNN,SSD,Yolo,DETR,Deformable DETR,EfficientDet`

`垃圾数据集`

![](https://image.yayan.xyz/20230316155331.png)

`对所有数据集进行处理`

`对比模型:Efficentdet, DETR和Mask RCNN，发现Efficentdet能产生最高的mAP`

`一个目标检测网络EfficientDet-D2,一个图像分类网络EfficientNet-B2`

`训练步骤:分开训练,先训练目标检测网络,再训练图像分类网络`

`问题:小目标,推理时间`

### Garbage object detection method based on improved Faster R-CNN

`对Faster R-CNN进行了两点改进:`

`1.基础网络从VGG16改成了ResNet50`

`2.增加了FPN特征金字塔`

`3.将原本的ROI改成ROI Align(Mask R-CNN)`

`4.修改了RPN结构参数`

### 基于改进 Faster R⁃CNN 的垃圾检测与分类方法

```python
# 把Faster R-CNN 的网络换成了ResNet50

# 把非极大值抑制（NMS）换成了Soft-NMS

# 对比实验把VGG16的7*7 5*5 换成了叠加的3*3

# 五折交叉验证

# 用FasterR-CNN相同的交替训练训练

# 73->81%
```

$NMS:0,IoU(M,b_i) \geq N_t$
$Soft-NMS:s_i(1-IoU(M,b_i)),IoU(M,b_i) \geq N_t$

### Object detection for autonomous trash and litter collection(毕业论文)

`针对垃圾收集机器人,管道方法:从数据收集到预测出结果的一系列`

1. 介绍:在机器人上部署最先进的目标检测模型
2. 背景:深度学习(MLP,CNN,YOLO),目标检测,垃圾检测数据集
3. 管道方法组成(收集,预处理,增强,训练,验证)
4. 自己的管道定义与实现(tile数据增强方法,光强归一化,不同模型)
5. 结果与分析
6. Future work

```python
1. 根据数据集中物体大小不同使用不同模型,参数
2. 专门为小目标设计一个检测器(Yolo-z)
3. 无监督学习,创建全功能检测器
4. 对图像的不同位置使用不同的检测器
5. 稍微扩大边界框,获取更多上下文信息
6. 利用GAN生成更多的类别均衡数据集
7. 开发减少假阳性数量的方法
8. 集成学习,使用多个较小的模型,加权盒融合
9. 更多的数据增广
10. 统一处理类别不可知的垃圾,避免对垃圾进行分类
```

### Tiny Object Detection based on YOLOv5

```python
1. 生成4幅特征图像进行融合
2. 在Neck部分,使用FPN和PANet对特征进行融合
3. 使用GIoU损失函数替换IoU
4. 用SWISH激活函数替换ReLU
5. 马赛克数据增强和学习率余弦退火

# 自己数据集上
# map@50: 45+  =>  55+
# mAP@0.5:0.95 : 25+   =>   30+

# recall,precision,....

```

### YOLO-Z: Improving small object detection in YOLOv5 for autonomous vehicles

`在自动驾驶领域,对小物体检测和检测速度要求很高`

`很多模型没有修改模型的架构,修改的不痛不痒`

![](https://image.yayan.xyz/20230330170112.png)

```python
修改:

1. ResNet50与DenseNet作为主干网络的比较

2. PanNet换成FPN和BiFPN

3. 对head部分的输入,将neck不同的特征图送入head(最有效)

结果分析:

1.DenseNet效果比ResNet好,这可能是由于网络深度不够，无法获得ResNet主干的好处，而DenseNet在保存特征图的细节方面做得很好

2. 在小模型上FPN比BiFPN效果好,因为较简单的模型受益于保持特征图相对不变，而其他比例需要额外的步骤来适应添加的特征图处理，并最终优于前者。但都优于传统模型.


3. 特征图的修改最有效果,因为在头部包含更高分辨率的特征图后，小对象最终会占用更多像素，因此具有更大的影响，而不是在主干的卷积阶段丢失。同样，删除原始较低分辨率的要素特征图会减少所需的处理量，并防止模型抵消较高分辨率贴图提供的细节级别。


4. 小模型anchor少比较好,大模型多了好,因为更复杂或更深入的模型确实可能受益于额外的锚，或者换句话说，可能更有能力利用额外锚提供的细节


5.其他学习率,深度宽度影响,
```



### Accuracy and Efficiency Comparison of Object Detection Open-Source Models

```
自己构建的杂草检测数据集,使用多种数据增强方法

使用EfficientDet,Faster R-CNN,YOLOv5,Detectron2 四个开源模型实验

```
![](https://image.yayan.xyz/20230414103141.png)


### The Object Detection of Underwater Garbage with an Improved YOLOv5 Algorithm

```
使用K-means对anchor进行聚类，产生九个新的框大小

将IoU或者GIoU损失函数改为CIoU损失函数

没有使用数据增强

```


### An Irregularly Dropped Garbage Detection Method Based on Improved YOLOv5s

```
CBAM 注意力模块
EIoU Loss
DeepSort 过滤静态物品


--
只选取了小部分垃圾种类
```



### Towards Lightweight Neural Networks for Garbage Object Detection
```
Yolov3
用DarkNet53做分类器

Yolov4的backbone是CSPDarkNet53
对CSPResNet优化得到DCSPResNet
对结构进行替换

轻量型Yolov4 的1/10 的参数
激活函数的修改 SiLU LReLU
DCSPDarkNet+膨胀卷积/膨胀变形卷积
使用膨胀变形卷积对CSPResNet结构进行了改进


没有考虑垃圾堆场景
不能在价格较低、性能较低的CPU设备上实时运行
对于目标遮挡和相对少见的目标识别，YOLOG的识别效果较差
```


### Real-Time Garbage Object Detection With Data Augmentation and Feature Fusion Using SUAV Low-Altitude Remote Sensing Images


```
修改Yolov4
使用不同结构
FPN,ASFF,PANet,NASFPN,BiFPN

加上数据增强

```














---

## Yolov5


### Yolox





### Yolov3
```
提出了Darknet53

修修补补

```


### Yolov4
[Yolov4分析](https://blog.csdn.net/WZZ18191171661/article/details/113765995)
```
针对input,backbone,neck,head选择不同的结构

Bag of freebies:在训练时的技巧不影响推测时间
数据增广,损失函数,归一化


Bag of special:应用到模型中的技巧
特征聚合结构块,注意力机制,激活函数,NMS,骨干网络选择

检测头继续用yolov3

```


### Yolov5

[yolov5分析](https://blog.csdn.net/WZZ18191171661/article/details/113789486)
![](https://image.yayan.xyz/20230330184730.png)

![](https://image.yayan.xyz/20230331152436.png)

```

# YOLOv5 v6.0 backbone
backbone:
  # [from, number, module, args]
  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2
   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
   [-1, 3, C3, [128]],
   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
   [-1, 6, C3, [256]],
   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
   [-1, 9, C3, [512]],
   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
   [-1, 3, C3, [1024]],
   [-1, 1, SPPF, [1024, 5]],  # 9
  ]

# YOLOv5 v6.0 head
head:
  [[-1, 1, Conv, [512, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 6], 1, Concat, [1]],  # cat backbone P4
   [-1, 3, C3, [512, False]],  # 13

   [-1, 1, Conv, [256, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 4], 1, Concat, [1]],  # cat backbone P3
   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)

   [-1, 1, Conv, [256, 3, 2]],
   [[-1, 14], 1, Concat, [1]],  # cat head P4
   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)

   [-1, 1, Conv, [512, 3, 2]],
   [[-1, 10], 1, Concat, [1]],  # cat head P5
   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)

   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
  ]

```

---

## 开源复现

### FasterR-CNN

[模型链接](https://github.com/chenyuntc/simple-faster-rcnn-pytorch)

```python
1. 下载到Google Colab

2. 安装依赖
!pip install ipdb visdom torchnet fire

3. 修改代码
data\voc_dataset.py中的VOC_BBOX_LABEL_NAMES修改成自己类别
utils\vis_tool.py 中的VOC_BBOX_LABEL_NAMES修改成自己类别


4. 源代码直接运行会报错
raise ValueError('need at least one array to stack')
ValueError: need at least one array to stack
因为只训练有物体的图片,
在data\voc_dataset.py
76行替换如下
'''
id_list_file = os.path.join(data_dir, 'ImageSets/Main/{0}.txt'.format(split))
id_list_read = [id_.strip() for id_ in open(id_list_file)]
id_list = list()
for i in id_list_read:
  obj = ET.parse(os.path.join(data_dir, 'Annotations', i + '.xml'))
  if obj.findall('object'):
    id_list.append(i)
self.ids = id_list
'''

5.在Google Colab运行不能可视化会报错
! npm install -g localtunnel
get_ipython().system_raw('python3 -m pip install visdom')
get_ipython().system_raw('python3 -m visdom.server -port 8097 >> visdomlog.txt 2>&1 &')
get_ipython().system_raw('lt --port 8097 >> url.txt 2>&1 &')

在运行,打开url.txt 查看可视化的窗口

code_root/
└── data/
    └── VOC2007/
        ├── Annotations/
        ├── JPEGImages/
        └── ImageSets/
        	└── Main/
        	      ├── test.txt
                ├── train.txt
                ├── val.txt
                └── trainval.txt
```

### EfficientDet

[模型链接](https://github.com/rwightman/efficientdet-pytorch)

```python
# 运行环境Google Colab

1.下载到工作区

2.安装依赖

3. 代码:effdet\data\parsers\parser_voc.py 文件中的DEFAULT_CLASSES改成自己的类别名称

4.! python ..../efficientdet-pytorch-master/train.py /content --dataset VOC2007 --num-classes 自己的类别 

# JPEGImages:所有图片  Annotations:所有xml标注  txt:用作训练测试的文件名,不带后缀
code_root/
└── data/
    └── VOC2007/
        ├── Annotations/
        ├── JPEGImages/
        └── ImageSets/
        	└── Main/
        	      ├── test.txt
                ├── train.txt
                ├── val.txt
                └── trainval.txt
```

### Deformable-DETR

[模型链接](https://github.com/fundamentalvision/Deformable-DETR)

```python
# 运行环境Google Colab
1.下载到工作区

2.安装相关依赖

3.!python ..(绝对路径..)/Deformable-DETR-main/models/ops/setup.py build install (用的jupyter)(可以用ops文件夹下的test.py测试环境是否正确)

#错误名称:找不到....h文件
如果报错,添加 export CUDA_PATH=/usr/local/cuda-你的版本
# 错误名
吧这一段修改到setup.py里面
extra_compile_args["nvcc"] = [
            "-DCUDA_HAS_FP16=1",
            "-D__CUDA_NO_HALF_OPERATORS__",
            "-D__CUDA_NO_HALF_CONVERSIONS__",
            "-D__CUDA_NO_HALF2_OPERATORS__",
            "-arch=sm_60",
            "-gencode=arch=compute_60,code=sm_60",
            "-gencode=arch=compute_61,code=sm_61",
            "-gencode=arch=compute_70,code=sm_70",
            "-gencode=arch=compute_75,code=sm_75",
]

5.代码:...../Deformable-DETR-main/util/misc.py 里面的
# float(torchvision.__version__[:3]) < 0.5/0.7 
需要改动,因为对于0.10.x版本的不适用,自己改成了
# float(torchvision.__version__[:4]) < 0.05:
否则报错cannot import name '_NewEmptyTensorOp' from 'torchvision.ops.misc'

6.! python ...../Deformable-DETR-main/main.py --coco_path ..../coco 

# train2017:训练图片  val2017:测试图片   annotations两个文件下面的标注图片
code_root/
└── data/
    └── coco/
        ├── train2017/
        ├── val2017/
        └── annotations/
        	├── instances_train2017.json
        	└── instances_val2017.json

```

### YOLOv5

[模型链接](https://github.com/ultralytics/yolov5)

```python
# 运行环境Google Colab
1.下载到工作区

2.安装相关依赖

3.指定数据路径 运行

```

### YOLOv8

[模型链接](https://github.com/ultralytics/ultralytics)

```python
# 运行环境Google Colab
1.!pip install ultralytics==8.0.20

2.安装相关依赖

3.指定数据路径 运行

```
