

一种介乎CNN和selfAttention之间的操作

将空间不变性(平移不变性)与通道变换性 交换了


#### 普通卷积
好处
1.不同位置之间重用卷积核,减少参数
2.不同通道代表不同含义的信息

缺点
1.通道冗余

2.卷积的接受范围

3.不能根据输入自适应卷积核大小

#### Involution

空间互异,通道不变


在通道之间共享卷积核,不同位置卷积核不同


Involution 卷积核大小
$H×W×K×K×G$
$G$是group
$K$是邻域

表示:对于HW(核的HW是根据输出特征图大小计算得到)个像素点,每个像素点都有一个K*K大小的卷积核,把C个通道分成G组,组内的通道共享卷积核

![](https://image.yayan.xyz/20230413170754.png)

对于一个像素点
$1×1×C$ -(两层线性变换)> $1×1×K^2×G$ -(Reshape)> $1×1×K×K×G$ -(注意力机制)> $1×1×K×K×C$ -(聚合)> $1×1×C$

![](https://image.yayan.xyz/20230413170813.png)

#### 与自注意力的区别


是一种更加简洁通用的自注意力机制

自注意力公式:
$Q=XW^Q$, $K=XW^K$ ,$V=XW^V$
![](https://image.yayan.xyz/20230414101940.png)


Involution:
通用描述:
$H_{i,j}=(XW^Q)(XW^K)$

![](https://image.yayan.xyz/20230414101953.png)


多头注意力头数<->通道组数

相似矩阵:$QK$<->核:H

位置编码<->生成的核有序的
