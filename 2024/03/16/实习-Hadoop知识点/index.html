<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>Hadoop学习笔记 | Gladdduck</title>
  <meta name="description" content="Hadoop概述  高可靠 高扩展 高效 高容错   大数据  大量(Volume),多样(Variety),高速(Velocity),价值(Volue) 全体数据取代随机样本 混杂性取代精确性 相关关系取代因果关系   Hadoop的体系 Hadoop是对海量数据进行大规模分布式处理的开源软件框架    HDFS—分布式文件系统； MapReduce—分布式处理模型； HBase—分布式数据库">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop学习笔记">
<meta property="og:url" content="https://gladdduck.github.io/2024/03/16/%E5%AE%9E%E4%B9%A0-Hadoop%E7%9F%A5%E8%AF%86%E7%82%B9/index.html">
<meta property="og:site_name" content="Gladdduck">
<meta property="og:description" content="Hadoop概述  高可靠 高扩展 高效 高容错   大数据  大量(Volume),多样(Variety),高速(Velocity),价值(Volue) 全体数据取代随机样本 混杂性取代精确性 相关关系取代因果关系   Hadoop的体系 Hadoop是对海量数据进行大规模分布式处理的开源软件框架    HDFS—分布式文件系统； MapReduce—分布式处理模型； HBase—分布式数据库">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://image.yayan.xyz/20240316163048.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319210705.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319211236.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319212120.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319212357.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319213144.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319213700.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321100323.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321102616.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321102633.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321103211.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321104917.png">
<meta property="og:image" content="https://image.yayan.xyz/20240320212227.png">
<meta property="og:image" content="https://image.yayan.xyz/20240320212048.png">
<meta property="og:image" content="https://image.yayan.xyz/20240320212829.png">
<meta property="og:image" content="https://image.yayan.xyz/20240320212841.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318211258.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318211326.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318210658.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318191544.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318192200.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318195105.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318195306.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318200601.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318201159.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318202739.png">
<meta property="og:image" content="https://image.yayan.xyz/20240318202833.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319141546.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319144209.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319144253.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319145314.png">
<meta property="og:image" content="https://image.yayan.xyz/20240319145324.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321160544.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321160753.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321160832.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321160854.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321162116.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321164946.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321165013.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321165111.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321165134.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321165440.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321170245.png">
<meta property="og:image" content="https://image.yayan.xyz/20240321171748.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322144039.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322144651.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322150635.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322151910.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322151942.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322153646.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322154052.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322154235.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322155110.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322155130.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322160405.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322160416.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322160744.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322161205.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322164454.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322165344.png">
<meta property="og:image" content="https://image.yayan.xyz/20240322165355.png">
<meta property="og:image" content="https://image.yayan.xyz/20240323115325.png">
<meta property="og:image" content="https://image.yayan.xyz/20240323115335.png">
<meta property="og:image" content="https://image.yayan.xyz/20240323115936.png">
<meta property="article:published_time" content="2024-03-16T08:26:52.655Z">
<meta property="article:modified_time" content="2024-03-23T04:06:21.855Z">
<meta property="article:author" content="syxue">
<meta property="article:tag" content="Hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://image.yayan.xyz/20240316163048.png">
  <!-- Canonical links -->
  <link rel="canonical" href="https://gladdduck.github.io/2024/03/16/%E5%AE%9E%E4%B9%A0-Hadoop%E7%9F%A5%E8%AF%86%E7%82%B9/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Gladdduck" type="application/atom+xml">
  
  
    <link rel="icon" href="/icon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1.4.0/dist/gitalk.min.css">
  
<meta name="generator" content="Hexo 5.4.2"></head>


<body class="main-center theme-purple" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/gladdduck" target="_blank">
          <img class="img-circle img-rotate" src="/images/icon.png" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Gladdduck</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">KB Master</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Suzhou, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">项目</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-picture">
          <a href="/picturewall">
            
            <i class="icon icon-starfish"></i>
            
            <span class="menu-title">图片墙</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">关于</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/gladdduck" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>自己JB的记录</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/hexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE/">hexo博客配置</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><span class="category-list-count">30</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BF%AB%E6%8D%B7%E5%91%BD%E4%BB%A4/">快捷命令</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%E9%85%8D%E7%BD%AE/">杂七杂八配置</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E8%B0%88/">杂谈</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/">论文记录</a><span class="category-list-count">6</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BingChat/" rel="tag">BingChat</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blazor/" rel="tag">Blazor</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BlazorLocalization/" rel="tag">BlazorLocalization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CSharp/" rel="tag">CSharp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cypher/" rel="tag">Cypher</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataloader/" rel="tag">Dataloader</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataset/" rel="tag">Dataset</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Few-shotObjectDetection/" rel="tag">Few-shotObjectDetection</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kaggle/" rel="tag">Kaggle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux%E5%BF%AB%E6%8D%B7%E5%91%BD%E4%BB%A4/" rel="tag">Linux快捷命令</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL/" rel="tag">MySQL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neo4j%E5%BF%AB%E6%8D%B7%E5%91%BD%E4%BB%A4/" rel="tag">Neo4j快捷命令</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PaddleOCRSharp/" rel="tag">PaddleOCRSharp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">Python学习笔记</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/" rel="tag">SQL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sampler/" rel="tag">Sampler</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VS2022/" rel="tag">VS2022</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VSCode-Github/" rel="tag">VSCode Github</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vscode/" rel="tag">Vscode</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vscode-%E9%98%BF%E9%87%8C%E4%BA%91/" rel="tag">Vscode 阿里云</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zero-shot-Detection/" rel="tag">Zero-shot Detection</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/collate-fn/" rel="tag">collate_fn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git-Time-out/" rel="tag">git Time out</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AEgitalk/" rel="tag">hexo博客配置gitalk</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AEpicgo/" rel="tag">hexo博客配置picgo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AEpure/" rel="tag">hexo博客配置pure</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mmfewshot/" rel="tag">mmfewshot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/neo4j%E5%AE%89%E8%A3%85/" rel="tag">neo4j安装</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python3-%E6%A0%87%E5%87%86%E5%BA%93/" rel="tag">python3 标准库</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/" rel="tag">代码阅读</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9E%83%E5%9C%BE%E6%A3%80%E6%B5%8B/" rel="tag">垃圾检测</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E4%B8%AA%E5%9F%9F%E5%90%8D%E8%AE%BF%E9%97%AE%E5%90%8C%E4%B8%80%E4%B8%AA%E6%9C%8D%E5%8A%A1%E5%99%A8/" rel="tag">多个域名访问同一个服务器</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AF%95/" rel="tag">实习笔试</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F/" rel="tag">实习面经</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" rel="tag">快捷键</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" rel="tag">快速傅里叶变换</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" rel="tag">排序算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" rel="tag">搭建博客</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BC%9A%E8%AE%AE/" rel="tag">目标检测会议</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A8%E7%A4%BA/" rel="tag">知识图谱表示</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%9F%A5%E8%AF%86%E7%82%B9/" rel="tag">知识点</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/" rel="tag">算法刷题笔记</a><span class="tag-list-count">7</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/BingChat/" style="font-size: 13px;">BingChat</a> <a href="/tags/Blazor/" style="font-size: 13px;">Blazor</a> <a href="/tags/BlazorLocalization/" style="font-size: 13px;">BlazorLocalization</a> <a href="/tags/CSharp/" style="font-size: 13px;">CSharp</a> <a href="/tags/Cypher/" style="font-size: 13px;">Cypher</a> <a href="/tags/Dataloader/" style="font-size: 13px;">Dataloader</a> <a href="/tags/Dataset/" style="font-size: 13px;">Dataset</a> <a href="/tags/Few-shotObjectDetection/" style="font-size: 13.33px;">Few-shotObjectDetection</a> <a href="/tags/Git/" style="font-size: 13px;">Git</a> <a href="/tags/Hadoop/" style="font-size: 13px;">Hadoop</a> <a href="/tags/Hexo/" style="font-size: 13.33px;">Hexo</a> <a href="/tags/Kaggle/" style="font-size: 13px;">Kaggle</a> <a href="/tags/Linux%E5%BF%AB%E6%8D%B7%E5%91%BD%E4%BB%A4/" style="font-size: 13px;">Linux快捷命令</a> <a href="/tags/MySQL/" style="font-size: 13px;">MySQL</a> <a href="/tags/Neo4j%E5%BF%AB%E6%8D%B7%E5%91%BD%E4%BB%A4/" style="font-size: 13px;">Neo4j快捷命令</a> <a href="/tags/PaddleOCRSharp/" style="font-size: 13px;">PaddleOCRSharp</a> <a href="/tags/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="font-size: 13.67px;">Python学习笔记</a> <a href="/tags/SQL/" style="font-size: 13px;">SQL</a> <a href="/tags/Sampler/" style="font-size: 13px;">Sampler</a> <a href="/tags/VS2022/" style="font-size: 13px;">VS2022</a> <a href="/tags/VSCode-Github/" style="font-size: 13px;">VSCode Github</a> <a href="/tags/Vscode/" style="font-size: 13px;">Vscode</a> <a href="/tags/Vscode-%E9%98%BF%E9%87%8C%E4%BA%91/" style="font-size: 13px;">Vscode 阿里云</a> <a href="/tags/Zero-shot-Detection/" style="font-size: 13px;">Zero-shot Detection</a> <a href="/tags/collate-fn/" style="font-size: 13px;">collate_fn</a> <a href="/tags/git-Time-out/" style="font-size: 13px;">git Time out</a> <a href="/tags/hexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AEgitalk/" style="font-size: 13px;">hexo博客配置gitalk</a> <a href="/tags/hexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AEpicgo/" style="font-size: 13px;">hexo博客配置picgo</a> <a href="/tags/hexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AEpure/" style="font-size: 13px;">hexo博客配置pure</a> <a href="/tags/mmfewshot/" style="font-size: 13px;">mmfewshot</a> <a href="/tags/neo4j%E5%AE%89%E8%A3%85/" style="font-size: 13px;">neo4j安装</a> <a href="/tags/python3-%E6%A0%87%E5%87%86%E5%BA%93/" style="font-size: 13.67px;">python3 标准库</a> <a href="/tags/%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/" style="font-size: 13px;">代码阅读</a> <a href="/tags/%E5%9E%83%E5%9C%BE%E6%A3%80%E6%B5%8B/" style="font-size: 13px;">垃圾检测</a> <a href="/tags/%E5%A4%9A%E4%B8%AA%E5%9F%9F%E5%90%8D%E8%AE%BF%E9%97%AE%E5%90%8C%E4%B8%80%E4%B8%AA%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 13px;">多个域名访问同一个服务器</a> <a href="/tags/%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AF%95/" style="font-size: 13px;">实习笔试</a> <a href="/tags/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F/" style="font-size: 13px;">实习面经</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 13px;">快捷键</a> <a href="/tags/%E5%BF%AB%E9%80%9F%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" style="font-size: 13px;">快速傅里叶变换</a> <a href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" style="font-size: 13px;">排序算法</a> <a href="/tags/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" style="font-size: 13.33px;">搭建博客</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BC%9A%E8%AE%AE/" style="font-size: 13px;">目标检测会议</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A8%E7%A4%BA/" style="font-size: 13px;">知识图谱表示</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E7%82%B9/" style="font-size: 13px;">知识点</a> <a href="/tags/%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/" style="font-size: 14px;">算法刷题笔记</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">三月 2024</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">四月 2023</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">三月 2023</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">十二月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">十一月 2022</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/10/">十月 2022</a><span class="archive-list-count">8</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled ">
        
          <li>
            
            <div class="item-thumb">
              <a href="/2024/03/26/%E5%AE%9E%E4%B9%A0-SQL%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
              </p>
              <p class="item-title">
                <a href="/2024/03/26/%E5%AE%9E%E4%B9%A0-SQL%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/" class="title">SQL刷题笔记</a>
              </p>
              <p class="item-date">
                <time datetime="2024-03-26T11:45:00.778Z" itemprop="datePublished">2024-03-26</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/2024/03/26/%E5%AE%9E%E4%B9%A0-2024%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F/" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
              </p>
              <p class="item-title">
                <a href="/2024/03/26/%E5%AE%9E%E4%B9%A0-2024%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F/" class="title">2024暑期实习面经</a>
              </p>
              <p class="item-date">
                <time datetime="2024-03-26T08:12:27.672Z" itemprop="datePublished">2024-03-26</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/2024/03/23/%E5%AE%9E%E4%B9%A0-MySQL%E7%9F%A5%E8%AF%86%E7%82%B9/" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
              </p>
              <p class="item-title">
                <a href="/2024/03/23/%E5%AE%9E%E4%B9%A0-MySQL%E7%9F%A5%E8%AF%86%E7%82%B9/" class="title">MySQL学习笔记</a>
              </p>
              <p class="item-date">
                <time datetime="2024-03-23T04:16:40.569Z" itemprop="datePublished">2024-03-23</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/2024/03/18/%E5%AE%9E%E4%B9%A0-2024%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AF%95/" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
              </p>
              <p class="item-title">
                <a href="/2024/03/18/%E5%AE%9E%E4%B9%A0-2024%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AF%95/" class="title">2024美团春招笔试</a>
              </p>
              <p class="item-date">
                <time datetime="2024-03-18T13:28:14.565Z" itemprop="datePublished">2024-03-18</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/2024/03/16/%E5%AE%9E%E4%B9%A0-Hadoop%E7%9F%A5%E8%AF%86%E7%82%B9/" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
              </p>
              <p class="item-title">
                <a href="/2024/03/16/%E5%AE%9E%E4%B9%A0-Hadoop%E7%9F%A5%E8%AF%86%E7%82%B9/" class="title">Hadoop学习笔记</a>
              </p>
              <p class="item-date">
                <time datetime="2024-03-16T08:26:52.655Z" itemprop="datePublished">2024-03-16</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
  <aside class="sidebar sidebar-toc collapse   in  " id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    <nav id="toc" class="article-toc">
      <h3 class="toc-title">文章目录</h3>
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#hadoop%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text"> Hadoop概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.</span> <span class="toc-text"> 大数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hadoop%E7%9A%84%E4%BD%93%E7%B3%BB"><span class="toc-number">1.2.</span> <span class="toc-text"> Hadoop的体系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E4%BA%91%E8%AE%A1%E7%AE%97%E5%85%B3%E7%B3%BB"><span class="toc-number">1.3.</span> <span class="toc-text"> 大数据与云计算关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="toc-number">1.4.</span> <span class="toc-text"> Hadoop集群搭建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9Fhdfs"><span class="toc-number">2.</span> <span class="toc-text"> 分布式文件系统HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs%E6%9E%B6%E6%9E%84"><span class="toc-number">2.1.</span> <span class="toc-text"> HDFS架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.</span> <span class="toc-text"> HDFS操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#shell%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.1.</span> <span class="toc-text"> Shell操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#api%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.2.</span> <span class="toc-text"> API操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs%E7%9A%84%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text"> HDFS的读写流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%99%E6%B5%81%E7%A8%8B"><span class="toc-number">2.3.1.</span> <span class="toc-text"> 写流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E6%B5%81%E7%A8%8B"><span class="toc-number">2.3.2.</span> <span class="toc-text"> 读流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="toc-number">2.4.</span> <span class="toc-text"> HDFS高可用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#namenode%E5%92%8Csecondarynamenode"><span class="toc-number">2.4.1.</span> <span class="toc-text"> NameNode和SecondaryNameNode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#datanode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%E5%BF%83%E8%B7%B3%E6%A3%80%E6%B5%8B"><span class="toc-number">2.4.2.</span> <span class="toc-text"> DataNode工作机制(心跳检测)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7-%E5%86%97%E4%BD%99%E5%A4%87%E4%BB%BD-%E5%AD%98%E5%82%A8%E7%A9%BA%E9%97%B4%E5%BB%B6%E6%97%B6%E5%9B%9E%E6%94%B6-%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%BC%93%E5%AD%98-%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%BC%8F%E5%A4%8D%E5%88%B6%E5%A4%87%E4%BB%BD"><span class="toc-number">2.4.3.</span> <span class="toc-text"> 数据完整性 &amp; 冗余备份 &amp; 存储空间延时回收 &amp; 客户端缓存 &amp; 流水线式复制备份</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8Bmapreduce"><span class="toc-number">3.</span> <span class="toc-text"> 分布式计算模型MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#mapreduce%E7%AE%80%E4%BB%8B"><span class="toc-number">3.1.</span> <span class="toc-text"> MapReduce简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapreduce%E6%9E%B6%E6%9E%84"><span class="toc-number">3.2.</span> <span class="toc-text"> MapReduce架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#inputformat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5"><span class="toc-number">3.2.1.</span> <span class="toc-text"> InputFormat数据输入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mapreduce%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">3.2.2.</span> <span class="toc-text"> MapReduce工作机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#shuffle%E9%98%B6%E6%AE%B5"><span class="toc-number">3.2.3.</span> <span class="toc-text"> Shuffle阶段</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#shuffle%E6%9C%BA%E5%88%B6"><span class="toc-number">3.2.3.1.</span> <span class="toc-text"> Shuffle机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#parition%E5%88%86%E5%8C%BA"><span class="toc-number">3.2.3.2.</span> <span class="toc-text"> Parition分区</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#writablecomparable%E6%8E%92%E5%BA%8F"><span class="toc-number">3.2.3.3.</span> <span class="toc-text"> WritableComparable排序</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#combiner%E5%90%88%E5%B9%B6"><span class="toc-number">3.2.3.4.</span> <span class="toc-text"> Combiner合并</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#outputformat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA"><span class="toc-number">3.2.4.</span> <span class="toc-text"> OutputFormat数据输出</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapreduce%E7%BC%96%E7%A0%81"><span class="toc-number">3.3.</span> <span class="toc-text"> MapReduce编码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mapreduce%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83"><span class="toc-number">3.3.1.</span> <span class="toc-text"> MapReduce编程规范</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hadoop%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">3.3.2.</span> <span class="toc-text"> Hadoop序列化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mapreduce%E7%BC%96%E7%A8%8B"><span class="toc-number">3.3.3.</span> <span class="toc-text"> MapReduce编程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9"><span class="toc-number">3.4.</span> <span class="toc-text"> Hadoop数据压缩</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9Fyarn"><span class="toc-number">4.</span> <span class="toc-text"> 通用资源管理系统Yarn</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn%E7%AE%80%E4%BB%8B"><span class="toc-number">4.1.</span> <span class="toc-text"> Yarn简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn%E6%9E%B6%E6%9E%84"><span class="toc-number">4.2.</span> <span class="toc-text"> Yarn架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-number">4.3.</span> <span class="toc-text"> Yarn工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn%E7%9A%84%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6"><span class="toc-number">4.4.</span> <span class="toc-text"> Yarn的容错机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn%E7%9A%84%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6"><span class="toc-number">4.5.</span> <span class="toc-text"> ☆Yarn的资源调度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#fifo%E8%B0%83%E5%BA%A6"><span class="toc-number">4.5.1.</span> <span class="toc-text"> FIFO调度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6"><span class="toc-number">4.5.2.</span> <span class="toc-text"> 容量调度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6"><span class="toc-number">4.5.3.</span> <span class="toc-text"> 公平调度</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn%E5%91%BD%E4%BB%A4"><span class="toc-number">4.6.</span> <span class="toc-text"> Yarn命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn%E7%9A%84%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0"><span class="toc-number">4.7.</span> <span class="toc-text"> Yarn的生产环境核心参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E5%99%A8zookeeper"><span class="toc-number">5.</span> <span class="toc-text"> 分布式协调器Zookeeper</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#zookerper%E7%AE%80%E4%BB%8B"><span class="toc-number">5.1.</span> <span class="toc-text"> Zookerper简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zookerper%E6%9E%B6%E6%9E%84"><span class="toc-number">5.2.</span> <span class="toc-text"> Zookerper架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zookerper%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.3.</span> <span class="toc-text"> Zookerper数据模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zookeeper%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7"><span class="toc-number">5.4.</span> <span class="toc-text"> Zookeeper的一致性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%9A%E8%AF%9Dsession%E6%9C%BA%E5%88%B6"><span class="toc-number">5.4.1.</span> <span class="toc-text"> 会话（Session）机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%91%E8%A7%86watcher%E6%9C%BA%E5%88%B6"><span class="toc-number">5.4.2.</span> <span class="toc-text"> 监视（Watcher）机制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zookeeper%E7%9A%84%E8%87%AA%E8%BA%AB%E4%B8%80%E8%87%B4%E6%80%A7"><span class="toc-number">5.5.</span> <span class="toc-text"> Zookeeper的自身一致性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E5%AD%90%E5%B9%BF%E6%92%ADzab-zookeeper-atomic-broadcast"><span class="toc-number">5.5.1.</span> <span class="toc-text"> 原子广播ZAB (ZooKeeper Atomic Broadcast)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zookeeper%E7%9A%84%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6"><span class="toc-number">5.6.</span> <span class="toc-text"> Zookeeper的选举机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%89%E4%B8%BE%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E5%88%99"><span class="toc-number">5.6.1.</span> <span class="toc-text"> 选举的基本原则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%89%E4%B8%BE%E6%B5%81%E7%A8%8B"><span class="toc-number">5.6.2.</span> <span class="toc-text"> 选举流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#shell%E6%93%8D%E4%BD%9C-2"><span class="toc-number">5.6.3.</span> <span class="toc-text"> Shell操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#api%E6%93%8D%E4%BD%9C-2"><span class="toc-number">5.6.4.</span> <span class="toc-text"> API操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93hbase"><span class="toc-number">6.</span> <span class="toc-text"> 分布式数据库HBase</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">6.1.</span> <span class="toc-text"> 简介</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nosql%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80-cap%E7%90%86%E8%AE%BA%E4%B8%8Ebase%E7%90%86%E8%AE%BA"><span class="toc-number">6.1.1.</span> <span class="toc-text"> NoSQL理论基础: CAP理论与BASE理论</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E9%80%BB%E8%BE%91%E7%BB%93%E6%9E%84"><span class="toc-number">6.1.2.</span> <span class="toc-text"> HBase逻辑结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E7%89%A9%E7%90%86%E7%BB%93%E6%9E%84"><span class="toc-number">6.1.3.</span> <span class="toc-text"> HBase物理结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.1.4.</span> <span class="toc-text"> HBase数据模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84"><span class="toc-number">6.1.5.</span> <span class="toc-text"> 基本架构</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hbase%E6%93%8D%E4%BD%9C"><span class="toc-number">6.2.</span> <span class="toc-text"> HBase操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#shell%E6%93%8D%E4%BD%9C-3"><span class="toc-number">6.2.1.</span> <span class="toc-text"> Shell操作</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">6.2.1.1.</span> <span class="toc-text"> 基本操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#ddl%E6%93%8D%E4%BD%9C"><span class="toc-number">6.2.1.2.</span> <span class="toc-text"> DDL操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#dml%E6%93%8D%E4%BD%9C"><span class="toc-number">6.2.1.3.</span> <span class="toc-text"> DML操作</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#api%E6%93%8D%E4%BD%9C-3"><span class="toc-number">6.2.2.</span> <span class="toc-text"> API操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hbase%E4%B8%80%E4%BA%9B%E5%8E%9F%E7%90%86"><span class="toc-number">6.3.</span> <span class="toc-text"> HBase一些原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E6%9E%B6%E6%9E%84"><span class="toc-number">6.3.1.</span> <span class="toc-text"> HBase架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E7%9A%84%E5%86%99%E6%B5%81%E7%A8%8B"><span class="toc-number">6.3.2.</span> <span class="toc-text"> HBase的写流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E7%9A%84%E8%AF%BB%E6%B5%81%E7%A8%8B"><span class="toc-number">6.3.3.</span> <span class="toc-text"> HBase的读流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E7%9A%84flush%E6%93%8D%E4%BD%9C"><span class="toc-number">6.3.4.</span> <span class="toc-text"> HBase的Flush操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E7%9A%84compaction%E6%93%8D%E4%BD%9C"><span class="toc-number">6.3.5.</span> <span class="toc-text"> HBase的Compaction操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E7%9A%84split%E6%93%8D%E4%BD%9C"><span class="toc-number">6.3.6.</span> <span class="toc-text"> HBase的Split操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A6%BB%E7%BA%BF%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93hive"><span class="toc-number">7.</span> <span class="toc-text"> 离线数据仓库Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#hive%E7%AE%80%E4%BB%8B"><span class="toc-number">7.1.</span> <span class="toc-text"> Hive简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hive%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="toc-number">7.2.</span> <span class="toc-text"> Hive的架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93%E9%85%8D%E7%BD%AE"><span class="toc-number">7.2.1.</span> <span class="toc-text"> 元数据库配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hiveserver2%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE"><span class="toc-number">7.2.2.</span> <span class="toc-text"> Hiveserver2服务配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#metastore%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE"><span class="toc-number">7.2.3.</span> <span class="toc-text"> MetaStore服务配置</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hive%E6%93%8D%E4%BD%9C"><span class="toc-number">7.3.</span> <span class="toc-text"> Hive操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ddldata-definition-language"><span class="toc-number">7.3.1.</span> <span class="toc-text"> DDL(Data Definition Language)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C"><span class="toc-number">7.3.1.1.</span> <span class="toc-text"> 数据库操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A1%A8%E6%93%8D%E4%BD%9C"><span class="toc-number">7.3.1.2.</span> <span class="toc-text"> 表操作</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dmldata-manipulation-language"><span class="toc-number">7.3.2.</span> <span class="toc-text"> DML(Data Manipulation Language)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hive%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5"><span class="toc-number">7.4.</span> <span class="toc-text"> Hive查询语句</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95"><span class="toc-number">7.4.1.</span> <span class="toc-text"> 基础语法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%9F%A5%E8%AF%A2-%E5%88%86%E7%BB%84%E6%9F%A5%E8%AF%A2"><span class="toc-number">7.4.2.</span> <span class="toc-text"> 基础查询 &amp; 分组查询</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#join%E6%9F%A5%E8%AF%A2"><span class="toc-number">7.4.3.</span> <span class="toc-text"> Join查询</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%92%E5%BA%8F%E6%9F%A5%E8%AF%A2"><span class="toc-number">7.4.4.</span> <span class="toc-text"> 排序查询</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BD%E6%95%B0"><span class="toc-number">7.5.</span> <span class="toc-text"> 函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E8%A1%8C%E5%87%BD%E6%95%B0"><span class="toc-number">7.5.1.</span> <span class="toc-text"> 单行函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0"><span class="toc-number">7.5.2.</span> <span class="toc-text"> 聚合函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%82%B8%E8%A3%82udtf%E5%87%BD%E6%95%B0"><span class="toc-number">7.5.3.</span> <span class="toc-text"> 炸裂(UDTF)函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-number">7.5.4.</span> <span class="toc-text"> 窗口函数(开窗函数)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0"><span class="toc-number">7.5.5.</span> <span class="toc-text"> 常用窗口函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-number">7.5.6.</span> <span class="toc-text"> 自定义函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89udf%E5%87%BD%E6%95%B0"><span class="toc-number">7.5.7.</span> <span class="toc-text"> 自定义UDF函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8"><span class="toc-number">7.6.</span> <span class="toc-text"> 分区表和分桶表</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%8C%BA"><span class="toc-number">7.6.1.</span> <span class="toc-text"> 分区</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E8%A1%A8%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">7.6.1.1.</span> <span class="toc-text"> 分区表基本操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%8C%E7%BA%A7%E5%88%86%E5%8C%BA"><span class="toc-number">7.6.1.2.</span> <span class="toc-text"> 二级分区</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA"><span class="toc-number">7.6.1.3.</span> <span class="toc-text"> 动态分区</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E6%A1%B6"><span class="toc-number">7.6.2.</span> <span class="toc-text"> 分桶</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95"><span class="toc-number">7.6.2.1.</span> <span class="toc-text"> 基本语法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E6%A1%B6%E6%8E%92%E5%BA%8F%E8%A1%A8"><span class="toc-number">7.6.2.2.</span> <span class="toc-text"> 分桶排序表</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E5%92%8C%E5%8E%8B%E7%BC%A9"><span class="toc-number">7.7.</span> <span class="toc-text"> 文件格式和压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F"><span class="toc-number">7.7.1.</span> <span class="toc-text"> 文件格式</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#textfile"><span class="toc-number">7.7.1.1.</span> <span class="toc-text"> TextFile</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#orc"><span class="toc-number">7.7.1.2.</span> <span class="toc-text"> ORC</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#parquet"><span class="toc-number">7.7.1.3.</span> <span class="toc-text"> Parquet</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9"><span class="toc-number">7.7.2.</span> <span class="toc-text"> 压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%B9hive%E6%95%B0%E6%8D%AE%E8%A1%A8%E8%BF%9B%E8%A1%8C%E5%8E%8B%E7%BC%A9"><span class="toc-number">7.7.2.1.</span> <span class="toc-text"> 对Hive数据表进行压缩</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9"><span class="toc-number">7.7.2.2.</span> <span class="toc-text"> 计算过程中使用压缩</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%81%E4%B8%9A%E7%BA%A7%E8%B0%83%E4%BC%98"><span class="toc-number">7.8.</span> <span class="toc-text"> ★企业级调优</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#yarn%E8%B5%84%E6%BA%90%E9%85%8D%E7%BD%AE"><span class="toc-number">7.8.1.</span> <span class="toc-text"> Yarn资源配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mapreduce%E8%B5%84%E6%BA%90%E9%85%8D%E7%BD%AE"><span class="toc-number">7.8.2.</span> <span class="toc-text"> MapReduce资源配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#explain%E6%9F%A5%E7%9C%8B%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92"><span class="toc-number">7.8.3.</span> <span class="toc-text"> Explain查看执行计划</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#explain%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E6%A6%82%E8%BF%B0"><span class="toc-number">7.8.3.1.</span> <span class="toc-text"> Explain执行计划概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#explain%E8%AF%AD%E6%B3%95"><span class="toc-number">7.8.3.2.</span> <span class="toc-text"> Explain语法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#explain%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C%E8%A7%A3%E8%AF%BB"><span class="toc-number">7.8.3.3.</span> <span class="toc-text"> Explain输出结果解读</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88%E4%BC%98%E5%8C%96"><span class="toc-number">7.8.4.</span> <span class="toc-text"> 分组聚合优化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E8%AF%B4%E6%98%8E"><span class="toc-number">7.8.4.1.</span> <span class="toc-text"> 优化说明</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#join%E4%BC%98%E5%8C%96"><span class="toc-number">7.8.5.</span> <span class="toc-text"> ★Join优化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#join%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D"><span class="toc-number">7.8.5.1.</span> <span class="toc-text"> Join算法介绍</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#map-join%E4%BC%98%E5%8C%96"><span class="toc-number">7.8.5.2.</span> <span class="toc-text"> Map Join优化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#map-join%E4%BC%98%E5%8C%96%E6%A1%88%E4%BE%8B"><span class="toc-number">7.8.5.3.</span> <span class="toc-text"> Map Join优化案例</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#bucket-map-join%E4%BC%98%E5%8C%96"><span class="toc-number">7.8.5.4.</span> <span class="toc-text"> Bucket Map Join优化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sort-merge-bucket-map-join%E4%BC%98%E5%8C%96"><span class="toc-number">7.8.5.5.</span> <span class="toc-text"> Sort Merge Bucket Map Join优化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#join%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93"><span class="toc-number">7.8.5.6.</span> <span class="toc-text"> Join优化总结</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BC%98%E5%8C%96"><span class="toc-number">7.8.6.</span> <span class="toc-text"> ★数据倾斜优化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88%E5%AF%BC%E8%87%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C"><span class="toc-number">7.8.6.1.</span> <span class="toc-text"> 分组聚合导致的数据倾斜</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#join%E5%AF%BC%E8%87%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C"><span class="toc-number">7.8.6.2.</span> <span class="toc-text"> Join导致的数据倾斜</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E5%B9%B6%E8%A1%8C%E5%BA%A6%E4%BC%98%E5%8C%96"><span class="toc-number">7.8.7.</span> <span class="toc-text"> 任务并行度优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6%E4%BC%98%E5%8C%96"><span class="toc-number">7.8.8.</span> <span class="toc-text"> 小文件合并优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E4%BC%98%E5%8C%96"><span class="toc-number">7.8.9.</span> <span class="toc-text"> 其他优化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#cbo%E4%BC%98%E5%8C%96"><span class="toc-number">7.8.9.1.</span> <span class="toc-text"> CBO优化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8"><span class="toc-number">7.8.9.2.</span> <span class="toc-text"> 谓词下推</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%9F%A2%E9%87%8F%E5%8C%96%E6%9F%A5%E8%AF%A2"><span class="toc-number">7.8.9.3.</span> <span class="toc-text"> 矢量化查询</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#fetch%E6%8A%93%E5%8F%96"><span class="toc-number">7.8.9.4.</span> <span class="toc-text"> Fetch抓取</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F"><span class="toc-number">7.8.9.5.</span> <span class="toc-text"> 本地模式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C"><span class="toc-number">7.8.9.6.</span> <span class="toc-text"> 并行执行</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%A5%E6%A0%BC%E6%A8%A1%E5%BC%8F"><span class="toc-number">7.8.9.7.</span> <span class="toc-text"> 严格模式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E7%BB%83%E4%B9%A0"><span class="toc-number">7.9.</span> <span class="toc-text"> 案例练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark"><span class="toc-number">8.</span> <span class="toc-text"> Spark</span></a></li></ol>
    </nav>
  </div>
</aside>

<main class="main" role="main">
  <div class="content">
  <article id="post-实习-Hadoop知识点" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      Hadoop学习笔记
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2024/03/16/%E5%AE%9E%E4%B9%A0-Hadoop%E7%9F%A5%E8%AF%86%E7%82%B9/" class="article-date">
	  <time datetime="2024-03-16T08:26:52.655Z" itemprop="datePublished">2024-03-16</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/Hadoop/" rel="tag">Hadoop</a>
  </span>


        
	<span class="article-read hidden-xs">
	    <i class="icon icon-eye-fill" aria-hidden="true"></i>
	    <span id="busuanzi_container_page_pv">
			<span id="busuanzi_value_page_pv">0</span>
		</span>
	</span>


        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2024/03/16/%E5%AE%9E%E4%B9%A0-Hadoop%E7%9F%A5%E8%AF%86%E7%82%B9/#comments" class="article-comment-link">评论</a></span>
        
	
		<span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 31.1k(字)</span>
	
	
		<span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 120(分)</span>
	

      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h2 id="hadoop概述"><a class="markdownIt-Anchor" href="#hadoop概述"></a> Hadoop概述</h2>
<ol>
<li>高可靠</li>
<li>高扩展</li>
<li>高效</li>
<li>高容错</li>
</ol>
<h3 id="大数据"><a class="markdownIt-Anchor" href="#大数据"></a> 大数据</h3>
<ul>
<li>大量(Volume),多样(Variety),高速(Velocity),价值(Volue)</li>
<li>全体数据取代随机样本</li>
<li>混杂性取代精确性</li>
<li>相关关系取代因果关系</li>
</ul>
<h3 id="hadoop的体系"><a class="markdownIt-Anchor" href="#hadoop的体系"></a> Hadoop的体系</h3>
<p>Hadoop是对海量数据进行大规模分布式处理的开源软件框架</p>
<p><img src="https://image.yayan.xyz/20240316163048.png" alt="" /><br />
<img src="https://image.yayan.xyz/20240319210705.png" alt="" /></p>
<ul>
<li>HDFS—分布式文件系统；</li>
<li>MapReduce—分布式处理模型；</li>
<li>HBase—分布式数据库（非SQL）</li>
<li>HCatalog—Hadoop内部数据整合工具，实现不同数据处理工具的数据类型相互转换机制；</li>
<li>Pig—流式数据的数据处理语言及其运行环境；</li>
<li>Hive—数据仓库管理工具，提供SQL查询功能；</li>
<li>ZooKeeper—分布式协调器。</li>
</ul>
<h3 id="大数据与云计算关系"><a class="markdownIt-Anchor" href="#大数据与云计算关系"></a> 大数据与云计算关系</h3>
<ul>
<li>云计算就是把一大堆廉价机器组织起来，通过网络向用户提供海量资源的高性能可靠服务。</li>
<li>云计算为大数据处理提供了可能和途径</li>
<li>大数据为云计算具有的大规模与分布式计算能力提供了应用空间，利用云计算解决了传统数据管理系统无法解决的问题</li>
</ul>
<h3 id="hadoop集群搭建"><a class="markdownIt-Anchor" href="#hadoop集群搭建"></a> Hadoop集群搭建</h3>
<p>略😁</p>
<h2 id="分布式文件系统hdfs"><a class="markdownIt-Anchor" href="#分布式文件系统hdfs"></a> 分布式文件系统HDFS</h2>
<p>HDFS(Hadoop Distributed File System)是一个分布式的文件系统,适合一次写入多次读出的场景,不适合低延时访问,不适合小文件,不支持并发写入</p>
<h3 id="hdfs架构"><a class="markdownIt-Anchor" href="#hdfs架构"></a> HDFS架构</h3>
<p><img src="https://image.yayan.xyz/20240319211236.png" alt="" /></p>
<ol>
<li>NameNode:集群的Master
<ul>
<li>管理HDFS的命名空间</li>
<li>配置副本策略</li>
<li>管理块映射信息</li>
<li>处理客户端读写请求</li>
<li>负责监控各个DataNode的状态</li>
</ul>
</li>
<li>DataNode:集群的Slave
<ul>
<li>存储实际的数据块</li>
<li>处理数据块的读写请求</li>
<li>每次启动时扫描本地文件发送给NameNode</li>
</ul>
</li>
<li>Client:访问HDFS的客户端
<ul>
<li>文件切分,文件长传时将文件切分为一个个block</li>
<li>与NameNode进行交互,获取文件的位置信息</li>
<li>与DataNode进行交互,读写数据</li>
</ul>
</li>
<li>Secondary NameNode:辅助NameNode
<ul>
<li>定期合并FsImage和Edits文件,生成新的FsImage文件</li>
<li>在紧急情况下，可辅助恢复NameNode. 但是不是NameNode的热备份</li>
</ul>
</li>
</ol>
<p><strong>HDFS对文件快大小的设置</strong></p>
<ul>
<li>HDFS的默认块大小是128M</li>
<li>HDFS的块大小设置是全局的,不支持单个文件设置</li>
<li>原因:
<ul>
<li>文件块越大，寻址时间越短，但磁盘传输时间越长；</li>
<li>文件块越小，寻址时间越长，但磁盘传输时间越短。</li>
<li>经过前人的大量测试发现，寻址时间为传输时间的1%时，为最佳状态</li>
<li>HDFS中平均寻址时间大概为10ms；目前磁盘的传输速率普遍为100MB/s</li>
<li>所以最佳大小为100MB,所以设置最接近的128M</li>
<li>不固定,根据磁盘传输速率设置</li>
</ul>
</li>
</ul>
<h3 id="hdfs操作"><a class="markdownIt-Anchor" href="#hdfs操作"></a> HDFS操作</h3>
<h4 id="shell操作"><a class="markdownIt-Anchor" href="#shell操作"></a> Shell操作</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上传</span></span><br><span class="line">hadoop fs -put /home/hadoop/hadoop-3.3.0.tar.gz /user/hadoop</span><br><span class="line">hadoop fs -copyFromLocal /home/hadoop/hadoop-3.3.0.tar.gz /user/hadoop</span><br><span class="line">hadoop fs -moveFromLocal /home/hadoop/hadoop-3.3.0.tar.gz /user/hadoop</span><br><span class="line">hadoop fs -appendToFile /home/hadoop/hadoop-3.3.0.tar.gz /user/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载</span></span><br><span class="line">hadoop fs -get /user/hadoop/hadoop-3.3.0.tar.gz /home/hadoop</span><br><span class="line">hadoop fs -copyToLocal /user/hadoop/hadoop-3.3.0.tar.gz /home/hadoop</span><br><span class="line"><span class="comment"># 其他,类似Linux命令</span></span><br></pre></td></tr></table></figure>
<h4 id="api操作"><a class="markdownIt-Anchor" href="#api操作"></a> API操作</h4>
<p>略</p>
<h3 id="hdfs的读写流程"><a class="markdownIt-Anchor" href="#hdfs的读写流程"></a> HDFS的读写流程</h3>
<h4 id="写流程"><a class="markdownIt-Anchor" href="#写流程"></a> 写流程</h4>
<p><img src="https://image.yayan.xyz/20240319212120.png" alt="" /></p>
<ol>
<li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</li>
<li>NameNode返回是否可以上传。</li>
<li>客户端请求第一个Block上传到哪几个DataNode服务器上。</li>
<li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</li>
<li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</li>
<li>dn1、dn2、dn3逐级应答客户端。</li>
<li>客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</li>
<li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）</li>
</ol>
<p><strong>节点距离计算</strong></p>
<ul>
<li>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据</li>
<li>两个节点到达最近的共同祖先的距离总和</li>
</ul>
<p><strong>副本策略</strong></p>
<ul>
<li>一个Block的第一个副本放在上传数据的DataNode上，</li>
<li>第二个副本放在与第一个副本在不同的机架上的另一个DataNode上，</li>
<li>第三个副本放在与第二个副本在相同的机架上的另一个DataNode上。</li>
</ul>
<h4 id="读流程"><a class="markdownIt-Anchor" href="#读流程"></a> 读流程</h4>
<p><img src="https://image.yayan.xyz/20240319212357.png" alt="" /></p>
<ol>
<li>客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li>
<li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</li>
<li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li>
<li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li>
</ol>
<h3 id="hdfs高可用"><a class="markdownIt-Anchor" href="#hdfs高可用"></a> HDFS高可用</h3>
<h4 id="namenode和secondarynamenode"><a class="markdownIt-Anchor" href="#namenode和secondarynamenode"></a> NameNode和SecondaryNameNode</h4>
<p><strong>NameNode工作机制</strong><br />
<img src="https://image.yayan.xyz/20240319213144.png" alt="" /></p>
<ol>
<li>NameNode将集群的文件镜像(FsImage)读到内存,当有新的操作来的时候,先将操作写到Edits文件中,然后再修改内存中文件镜像.</li>
<li>NameNode每次启动时,加载FsImage文件和Edits文件,进行合并</li>
</ol>
<p><strong>NameNode长时间操作会导致Edits文件过大</strong></p>
<ol>
<li>NameNode滚动正在写的Edits日志到一个新的文件edits_inprogress_002</li>
<li>NameNode将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode</li>
<li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并</li>
<li>生成新的镜像文件fsimage.chkpoint,拷贝fsimage.chkpoint到NameNode</li>
<li>此时fsimage.chkpoint加上edits_inprogress_002就是最新的文件影像</li>
</ol>
<p><strong>问题:</strong></p>
<ul>
<li>如果Secondary NameNode正在合并的时候出问题了,则会导致期间NameNode的操作丢失</li>
<li>一般不会使用SecondaryNameNode,而是结合Zookeeper配置高可用</li>
</ul>
<p><strong>SecondaryNameNode合并时机</strong></p>
<ol>
<li>每隔一小时合并一次</li>
<li>每一分钟检测一下操作次数,如果到了100万,合并一次</li>
</ol>
<h4 id="datanode工作机制心跳检测"><a class="markdownIt-Anchor" href="#datanode工作机制心跳检测"></a> DataNode工作机制(心跳检测)</h4>
<p><img src="https://image.yayan.xyz/20240319213700.png" alt="" /></p>
<ul>
<li>DataNode启动后向NameNode注册，通过后，周期性（6小时）的向NameNode上报所有的块信息</li>
<li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</li>
<li>集群运行中可以安全加入和退出一些机器。</li>
</ul>
<p><strong>DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信</strong></p>
<ul>
<li>NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。</li>
<li>默认超时时长10分钟+30秒</li>
</ul>
<h4 id="数据完整性-冗余备份-存储空间延时回收-客户端缓存-流水线式复制备份"><a class="markdownIt-Anchor" href="#数据完整性-冗余备份-存储空间延时回收-客户端缓存-流水线式复制备份"></a> 数据完整性 &amp; 冗余备份 &amp; 存储空间延时回收 &amp; 客户端缓存 &amp; 流水线式复制备份</h4>
<p><strong>数据完整性</strong></p>
<ul>
<li>DataNode读取Block的时候会计算校验码,如果与创建的时候不一样,则Block孙华</li>
<li>读取其他DataNode的Block,使用crc32校验码</li>
<li>DataNode在其文件创建后周期验证CheckSum</li>
</ul>
<p><strong>冗余备份</strong></p>
<ul>
<li>NameNode定期检测各个数据块的备份数，并根据复制因子来增加或减少相应数据块的备份</li>
<li>文件备份数量= min(复制因子,DataNode的数量)</li>
</ul>
<p><strong>存储空间延时回收</strong></p>
<ul>
<li>删除文件在目录/trash内存放超过6小时，就会被系统彻底清除，并回收其存储空间</li>
</ul>
<p><strong>负载均衡</strong></p>
<ul>
<li>当某个DataNode上的空余磁盘空间下降到一定值，系统就把其部分数据块迁移到其它合适节点上去；</li>
<li>当出现对某个文件的访问频率超过一定值时，系统会创建该文件的新备份，对访问实施分流</li>
</ul>
<p><strong>客户端缓存</strong></p>
<ul>
<li>客户端写入文件的请求不是立即到达NameNode，而是先把写入数据存入本地缓存；</li>
<li>当本地缓存内数据达到一个数据块的大小（默认为128MB）时，客户端就请求NameNode分配一个文件数据块，并把本地缓存内的数据写入NameNode分配的数据块中</li>
<li>客户端的本地缓存可以极大地减少对网络的访问</li>
</ul>
<p><strong>流水线式复制备份</strong></p>
<ul>
<li>客户端写入数据块时，系统同时建立数据块的备份；</li>
<li>当一个DataNode接收数据进行写入操作时，随即把数据传给下一个节点写入，好似流水线一般</li>
</ul>
<h2 id="分布式计算模型mapreduce"><a class="markdownIt-Anchor" href="#分布式计算模型mapreduce"></a> 分布式计算模型MapReduce</h2>
<h3 id="mapreduce简介"><a class="markdownIt-Anchor" href="#mapreduce简介"></a> MapReduce简介</h3>
<p>MapReduce是一个分布式计算框架，MapReduce的设计思想是将计算过程分为两个阶段：Map阶段和Reduce阶段。</p>
<p>MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上</p>
<p>缺点:</p>
<ol>
<li>不适合实时计算</li>
<li>不适合流式计算</li>
</ol>
<h3 id="mapreduce架构"><a class="markdownIt-Anchor" href="#mapreduce架构"></a> MapReduce架构</h3>
<p><img src="https://image.yayan.xyz/20240321100323.png" alt="" /></p>
<h4 id="inputformat数据输入"><a class="markdownIt-Anchor" href="#inputformat数据输入"></a> InputFormat数据输入</h4>
<p><strong>切片与MapTask并行度决定机制</strong></p>
<ul>
<li>数据切片是MapReduce程序计算输入数据的单位，一个切片会对应启动一个MapTask</li>
<li>切片大小通常会与HDFS的块大小一致，但是也可以通过InputFormat来自定义切片大小</li>
<li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li>
</ul>
<p><strong>Job提交流程源码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 等待任务完成</span></span><br><span class="line">waitForCompletion();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 建立连接</span></span><br><span class="line">connect();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 创建作业提交代理</span></span><br><span class="line"><span class="type">Cluster</span> <span class="variable">cluster</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Cluster</span>(getConfiguration());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. 初始化作业提交</span></span><br><span class="line">initialize(jobTrackAddr, conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5. 提交作业到集群</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="built_in">this</span>, cluster);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6. 创建作业的暂存区</span></span><br><span class="line"><span class="type">Path</span> <span class="variable">jobStagingArea</span> <span class="operator">=</span> JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 7. 获取新的作业ID</span></span><br><span class="line"><span class="type">JobID</span> <span class="variable">jobId</span> <span class="operator">=</span> submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 8. 复制并配置作业提交所需的文件</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 9. 写入切片并生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">input.getSplits(job);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 10. 将XML配置文件写入暂存区</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">conf.writeXml(out);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 11. 提交作业并获取提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>
<p><strong>FileInputFormat切片源码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">（<span class="number">1</span>）程序先找到你数据存储的目录。</span><br><span class="line">（<span class="number">2</span>）开始遍历处理（规划切片）目录下的每一个文件。</span><br><span class="line">（<span class="number">3</span>）遍历第一个文件 ss.txt</span><br><span class="line">    a）获取文件大小 fs.sizeOf(ss.txt)</span><br><span class="line">    b）计算切片大小 computeSplitSize(Math.max(minSize, Math.min(maxSize, blocksize))) = blocksize = 128M</span><br><span class="line">    c）默认情况下，切片大小 = blocksize</span><br><span class="line">    d）开始切，形成第<span class="number">1</span>个切片：ss.txt—<span class="number">0</span>:128M 第<span class="number">2</span>个切片 ss.txt—<span class="number">128</span>:256M 第<span class="number">3</span>个切片 ss.txt—256M:300M</span><br><span class="line">       （每次切片时，都要判断切完剩下的部分是否大于块的<span class="number">1.1</span>倍，不大于<span class="number">1.1</span>倍就划分一块切片）</span><br><span class="line">    e）将切片信息写到一个切片规划文件中</span><br><span class="line">    f）整个切片的核心过程在 getSplit() 方法中完成</span><br><span class="line">    g）InputSplit 只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等。</span><br><span class="line">（<span class="number">4</span>）提交切片规划文件到 YARN 上，YARN 上的 MrAppMaster 就可以根据切片规划文件计算开启 MapTask 个数</span><br></pre></td></tr></table></figure>
<p><strong>FileInputFormat切片机制</strong></p>
<ul>
<li>（1）简单地按照文件的内容长度进行切片</li>
<li>（2）切片大小，默认等于Block大小</li>
<li>（3）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li>
<li><strong>如果文件大小小于Block大小的1.1倍,默认也是不切片的</strong></li>
</ul>
<p><strong>TextInputFormat切片机制</strong></p>
<ul>
<li>TextInputFormat是默认的FileInputFormat实现类。按行读取每条记录。</li>
<li>键是存储该行在整个文件中的起始字节偏移量，LongWritable类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text类型</li>
</ul>
<p><strong>CombineTextInputFormat切片机制</strong></p>
<ul>
<li>框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下</li>
<li>生成切片过程包括：虚拟存储过程和切片过程二部分</li>
<li>虚拟存储过程：</li>
<li>
<ul>
<li>将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件平均分成2个虚拟存储块（防止出现太小切片）</li>
</ul>
</li>
<li>
<ul>
<li>例如：setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。<br />
剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件平均切分成（2.01M和2.01M）两个文件。</li>
</ul>
</li>
<li>切片过程：</li>
<li>
<ul>
<li>判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。</li>
</ul>
</li>
<li>
<ul>
<li>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</li>
</ul>
</li>
<li>
<ul>
<li>测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M。这四个小文件，则虚拟存储之后形成6个文件块，大小分别为：1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M）.最终会形成3个切片，大小分别为：（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M</li>
</ul>
</li>
</ul>
<h4 id="mapreduce工作机制"><a class="markdownIt-Anchor" href="#mapreduce工作机制"></a> MapReduce工作机制</h4>
<p><img src="https://image.yayan.xyz/20240321102616.png" alt="" /></p>
<p><strong>MaperTask工作机制</strong></p>
<ol>
<li>Read阶段：MapTask通过InputFormat获得的RecordReader，从输入InputSplit中解析出一个个key/value</li>
<li>Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value</li>
<li>Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中</li>
<li>Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作,溢写阶段详情：
<ul>
<li>步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序</li>
<li>步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作</li>
<li>步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中</li>
</ul>
</li>
<li>Merge阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件,MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index
<ul>
<li>在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并mapreduce.task.io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件</li>
<li>让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销</li>
</ul>
</li>
</ol>
<p><img src="https://image.yayan.xyz/20240321102633.png" alt="" /></p>
<p><strong>ReducerTask工作机制</strong></p>
<ol>
<li>Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中</li>
<li>Sort阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可</li>
<li>Reduce阶段：reduce()函数将计算结果写到HDFS上</li>
</ol>
<p><strong>注意事项</strong></p>
<ol>
<li>MapTask并行度由切片个数决定，切片个数由输入文件和切片规则决定</li>
<li>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置</li>
<li>ReduceTask=0，表示没有Reduce阶段，输出文件个数和Map个数一致</li>
<li>ReduceTask默认值就是1，所以输出文件个数为一个</li>
<li>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜</li>
<li>ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask</li>
<li>如果分区数不是1，但是ReduceTask为1，不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行分区</li>
</ol>
<h4 id="shuffle阶段"><a class="markdownIt-Anchor" href="#shuffle阶段"></a> Shuffle阶段</h4>
<h5 id="shuffle机制"><a class="markdownIt-Anchor" href="#shuffle机制"></a> Shuffle机制</h5>
<p><img src="https://image.yayan.xyz/20240321103211.png" alt="" /></p>
<p>Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。</p>
<ul>
<li>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快</li>
<li>缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb默认100M</li>
<li>当缓冲区达到80%的时候，会启动一个后台线程将缓冲区中的数据写到磁盘上，同时继续接收新的数据</li>
</ul>
<h5 id="parition分区"><a class="markdownIt-Anchor" href="#parition分区"></a> Parition分区</h5>
<p>将MapTask输出的数据按照key进行分区，每个分区交给一个ReduceTask处理，默认分区是根据key的hashCode对ReduceTasks个数取模得到的</p>
<ul>
<li>（1）如果ReduceTask的数量&gt;getPartition的结果数，则会多产生几个空的输出文件part-r-000xx</li>
<li>（2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；</li>
<li>（3）如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件part-r-00000；</li>
<li>（4）分区号必须从零开始，逐一累加</li>
</ul>
<h5 id="writablecomparable排序"><a class="markdownIt-Anchor" href="#writablecomparable排序"></a> WritableComparable排序</h5>
<p>MapTask和ReduceTask均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。</p>
<p>默认排序是按照字典顺序排序，且实现该排序的方法是快速排序</p>
<ul>
<li>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序</li>
<li>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序</li>
<li>bean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序</li>
</ul>
<h5 id="combiner合并"><a class="markdownIt-Anchor" href="#combiner合并"></a> Combiner合并</h5>
<ul>
<li>（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。</li>
<li>（2）Combiner组件的父类就是Reducer。</li>
<li>（3）Combiner和Reducer的区别在于运行的位置Combiner是在每一个MapTask所在的节点运行;</li>
<li>（4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。</li>
<li>（5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟Reducer的输入kv类型要对应起来</li>
</ul>
<h4 id="outputformat数据输出"><a class="markdownIt-Anchor" href="#outputformat数据输出"></a> OutputFormat数据输出</h4>
<p>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。默认输出格式是TextOutputFormat</p>
<ul>
<li>自定义一个LogOutputFormat类集成FileOutputFormat</li>
<li>重写getRecordWriter方法，返回一个自定义的LogRecordWriter</li>
<li>编写LogRecordWriter类, 具体改写RecordWriter的write方法，实现输出的逻辑</li>
</ul>
<h3 id="mapreduce编码"><a class="markdownIt-Anchor" href="#mapreduce编码"></a> MapReduce编码</h3>
<h4 id="mapreduce编程规范"><a class="markdownIt-Anchor" href="#mapreduce编程规范"></a> MapReduce编程规范</h4>
<p>用户编写的程序分为三个部分:Mapper,Reducer,Driver</p>
<ol>
<li>Mapper阶段
<ul>
<li>用户自定义Mapper要集成的类</li>
<li>Mapper的输入数据是KV对的形式(确定KV类型)</li>
<li>Mapper的业务逻辑写在map()方法中</li>
<li>Mapper的输出数据是KV对的形式(确定KV类型)</li>
<li>map()对每个KV调用一次</li>
</ul>
</li>
<li>Reducer阶段
<ul>
<li>用户自定义Reducer要集成的类</li>
<li>Reducer的输入数据是KV对的形式(确定KV类型)</li>
<li>Reducer的业务逻辑写在reduce()方法中</li>
<li>Reducer的输出数据是KV对的形式(确定KV类型)</li>
<li>reduce()对每一组相同的K调用一次</li>
</ul>
</li>
<li>Driver阶段
<ul>
<li>相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象</li>
</ul>
</li>
</ol>
<h4 id="hadoop序列化"><a class="markdownIt-Anchor" href="#hadoop序列化"></a> Hadoop序列化</h4>
<p>序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。</p>
<p>对象一般只存在内存里,序列化后可以存储到磁盘,网络传输. Java序列化是一个重量级框架,一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）</p>
<p><strong>自定义bean对象实现序列化接口</strong></p>
<ol>
<li>实现Writable接口</li>
<li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造</li>
<li>重写序列化方法,重写反序列化方法(反序列化的顺序和序列化的顺序完全一致)</li>
<li>如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序</li>
</ol>
<h4 id="mapreduce编程"><a class="markdownIt-Anchor" href="#mapreduce编程"></a> MapReduce编程</h4>
<p>1）输入数据接口：InputFormat<br />
（1）默认使用的实现类是：TextInputFormat<br />
（2）TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。<br />
（3）CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。</p>
<p>2）逻辑处理接口：Mapper<br />
用户根据业务需求实现其中三个方法：map(), setup(), cleanup()</p>
<p>3）Partitioner分区<br />
（1）有默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces<br />
（2）如果业务上有特别的需求，可以自定义分区。</p>
<p>4）Comparable排序<br />
（1）当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法。<br />
（2）部分排序：对最终输出的每一个文件进行内部排序。<br />
（3）全排序：对所有数据进行排序，通常只有一个Reduce。<br />
（4）二次排序：排序的条件有两个。</p>
<p>5）Combiner合并<br />
Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果。</p>
<p>6）逻辑处理接口：Reducer<br />
用户根据业务需求实现其中三个方法：reduce(), setup(), cleanup()</p>
<p>7）输出数据接口：OutputFormat<br />
（1）默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对，向目标文本文件输出一行。<br />
（2）用户还可以自定义OutputFormat。</p>
<h3 id="hadoop数据压缩"><a class="markdownIt-Anchor" href="#hadoop数据压缩"></a> Hadoop数据压缩</h3>
<p>1）压缩的好处和坏处压缩的优点：</p>
<ul>
<li>以减少磁盘IO、减少磁盘存储空间。</li>
<li>压缩的缺点：增加CPU开销。</li>
</ul>
<p>2）压缩原则</p>
<ul>
<li>（1）运算密集型的Job，少用压缩</li>
<li>（2）IO密集型的Job，多用压缩</li>
</ul>
<p>3）压缩方式选择</p>
<ul>
<li>压缩方式选择时重点考虑：压缩/解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片</li>
</ul>
<p>4）压缩可以在MapReduce作用的任意阶段启用<br />
<img src="https://image.yayan.xyz/20240321104917.png" alt="" /></p>
<p>5）MR支持的压缩编码</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>Hadoop自带</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否支持切片</th>
<th>原程序是否需要修改</th>
</tr>
</thead>
<tbody>
<tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>是，直接使用</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody>
</table>
<h2 id="通用资源管理系统yarn"><a class="markdownIt-Anchor" href="#通用资源管理系统yarn"></a> 通用资源管理系统Yarn</h2>
<h3 id="yarn简介"><a class="markdownIt-Anchor" href="#yarn简介"></a> Yarn简介</h3>
<p>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p>
<p><strong>有任务推测执行功能</strong></p>
<h3 id="yarn架构"><a class="markdownIt-Anchor" href="#yarn架构"></a> Yarn架构</h3>
<p><img src="https://image.yayan.xyz/20240320212227.png" alt="" /></p>
<ol>
<li>ResourceManager:集群的Master
<ul>
<li>负责整个集群的资源分配和调度(调度策略)</li>
<li>监控NodeManager的运行状态</li>
<li>启动或监控ApplicationMaster的运行状态</li>
<li>处理客户端的请求</li>
</ul>
</li>
<li>NodeManager:集群的Slave
<ul>
<li>负责单个节点上的资源管理和任务管理</li>
<li>与ResourceManager通信,汇报节点资源使用情况</li>
<li>接收ResourceManager的命令,启动或停止Container</li>
<li>处理来自ApplicationMaster的请求</li>
</ul>
</li>
<li>ApplicationMaster:每个应用程序的Master
<ul>
<li>负责应用程序的管理和协调</li>
<li>与ResourceManager通信,申请资源,释放资源</li>
<li>与NodeManager通信,启动或停止Container</li>
<li>监控任务与容错</li>
</ul>
</li>
<li>Container:资源分配的基本单位</li>
</ol>
<h3 id="yarn工作机制"><a class="markdownIt-Anchor" href="#yarn工作机制"></a> Yarn工作机制</h3>
<p><img src="https://image.yayan.xyz/20240320212048.png" alt="" /></p>
<p>（1）MR程序提交到客户端所在的节点。<br />
（2）YarnRunner向ResourceManager申请一个Application。<br />
（3）RM将该应用程序的资源路径返回给YarnRunner。<br />
（4）该程序将运行所需资源提交到HDFS上。<br />
（5）程序资源提交完毕后，申请运行mrAppMaster。<br />
（6）RM将用户的请求初始化成一个Task。<br />
（7）其中一个NodeManager领取到Task任务。<br />
（8）该NodeManager创建容器Container，并产生MRAppmaster。<br />
（9）Container从HDFS上拷贝资源到本地。<br />
（10）MRAppmaster向RM 申请运行MapTask资源。<br />
（11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。（12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。<br />
（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。<br />
（14）ReduceTask向MapTask获取相应分区的数据。<br />
（15）程序运行完毕后，MR会向RM申请注销自己。</p>
<p><img src="https://image.yayan.xyz/20240320212829.png" alt="" /><br />
<img src="https://image.yayan.xyz/20240320212841.png" alt="" /></p>
<ol>
<li><strong>作业提交</strong>
<ol>
<li>Client调用job.waitForCompletion()方法向集群提交作业</li>
<li>Client向ResourceManager申请一个做业的id</li>
<li>ResourceManager返回一个job资源的提交路径和id</li>
<li>Client提交jar包,切片信息和配置文件到HDFS指定路径</li>
<li>Client提交完资源后,向ResourceManager申请运行MRAppMaster</li>
</ol>
</li>
<li><strong>作业初始化</strong><br />
6. ResourceManager收到请求后,将该job添加到调度器中<br />
7. 某个空闲的NodeManager领取到任务<br />
8. 该NodeManager创建容器Container,并启动MRAppMaster<br />
9. 从HDFS下载Client提交的资源到本地</li>
<li><strong>任务分配</strong><br />
10. MRAppMaster向ResourceManager申请运行多个MapTask资源<br />
11. ResourceManager将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器</li>
<li><strong>任务执行</strong><br />
12. MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序<br />
13. MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask<br />
14. ReduceTask向MapTask获取相应分区的数据<br />
15. 程序运行完毕后，MR会向RM申请注销自己</li>
<li><strong>进度和状态更新</strong><br />
16. YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒向应用管理器请求进度更新, 展示给用户。</li>
<li><strong>作业完成</strong><br />
17. 除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</li>
</ol>
<h3 id="yarn的容错机制"><a class="markdownIt-Anchor" href="#yarn的容错机制"></a> Yarn的容错机制</h3>
<p><strong>ResourceManager的容错</strong></p>
<ul>
<li>借助Zookeeper实现高可用</li>
</ul>
<p><strong>ApplicationMaster的容错</strong></p>
<ul>
<li>RM中的ASM负责监控AM的运行状态，一旦发现它运行失败或超时（在规定时间段内未使用为其分配的Container），就为其重新分配资源并启动它</li>
<li>AM重新启动后如何恢复其内部的状态，则需由AM自己保证，比如MRAppMaster:</li>
<li>
<ul>
<li>在作业运行过程中把状态信息动态记入HDFS中</li>
</ul>
</li>
<li>
<ul>
<li>在出现故障重启后，它就从HDFS中读取并恢复之前的状态，以减少重新计算带来的开销</li>
</ul>
</li>
</ul>
<p><strong>NodeManager的容错</strong></p>
<ul>
<li>如果某个NM在规定的时间段内未向RM发送心跳消息（可能是网络方面的原因或NM自身的原因），RM则认为它已经宕机</li>
<li>RM将该NM上所有正在运行的Container（任务）的状态置为失败，并分别通知它们所属作业的AM，由AM对这些Container中运行的任务作出处理</li>
<li>AM替失败的任务向RM重新申请一个Container，并重新启动</li>
<li>如果AM自身使用的Container运行失败，则由RM中的ASM为其重新申请一个Container，并重启AM</li>
</ul>
<p><strong>Contrainer的容错</strong></p>
<ul>
<li>对于运行任务的Container，RM收回Container，并通知其申请者（AM），由它决定如何处理</li>
<li>对于运行AM的Container，RM收回Container，由RM中的ASM重新为它申请一个Container，并重启</li>
</ul>
<h3 id="yarn的资源调度"><a class="markdownIt-Anchor" href="#yarn的资源调度"></a> ☆Yarn的资源调度</h3>
<p><strong>对当前请求任务的节点进行检查</strong></p>
<ol>
<li>若该节点上的磁盘容量小于某阈值，则不再给该节点分配任务</li>
<li>若一个作业在该节点上运行失败的任务数量超过某阈值，则不再给该节点分配此作业的任务</li>
</ol>
<p><strong>Mapper任务调度</strong></p>
<ul>
<li>优先选择运行失败的任务，以让其尽快获得重新运行的机会；</li>
<li>其次按照数据本地性策略选择尚未运行的任务；</li>
<li>最后从正在运行的任务中推测是否有“拖后腿”任务，若有则为其启动备份任务</li>
</ul>
<p><strong>Reducer任务调度</strong></p>
<ul>
<li>Reduce任务的数据来自多个节点，故没有数据本地性可言，即无须考虑本地性</li>
</ul>
<p>Yarn做业调度主要有三种方式:FIFO,容量调度(默认),公平调度</p>
<h4 id="fifo调度"><a class="markdownIt-Anchor" href="#fifo调度"></a> FIFO调度</h4>
<p>先到先得</p>
<h4 id="容量调度"><a class="markdownIt-Anchor" href="#容量调度"></a> 容量调度</h4>
<h4 id="公平调度"><a class="markdownIt-Anchor" href="#公平调度"></a> 公平调度</h4>
<h3 id="yarn命令"><a class="markdownIt-Anchor" href="#yarn命令"></a> Yarn命令</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有的队列</span></span><br><span class="line">yarn application -list</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">yarn application -list -appStates RUNNING</span><br><span class="line"><span class="comment"># Kill掉队列</span></span><br><span class="line">yarn application -<span class="built_in">kill</span> application_1616350000001_0001</span><br><span class="line"><span class="comment"># 查看日志</span></span><br><span class="line">yarn logs -applicationId application_1616350000001_0001</span><br><span class="line"><span class="comment"># 查看Container日志</span></span><br><span class="line">yarn logs -applicationId application_1616350000001_0001 -containerId container_1616350000001_0001_01_000001</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看容器</span></span><br><span class="line">yarn container -list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看节点状态</span></span><br><span class="line">yarn node -list -all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看队列</span></span><br><span class="line">yarn queue -list</span><br></pre></td></tr></table></figure>
<h3 id="yarn的生产环境核心参数"><a class="markdownIt-Anchor" href="#yarn的生产环境核心参数"></a> Yarn的生产环境核心参数</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.ResourceManager相关</span></span><br><span class="line">yarn.resourcemanager.scheduler.class <span class="comment"># 配置调度器,默认容量调度</span></span><br><span class="line">yarn.resourcemanager.scheduler.client.thread-count <span class="comment"># ResourceManger处理调度器请求的线程数,默认50</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.NodeManager相关</span></span><br><span class="line">yarn.nodemanager.resource.detect-hardware-capabilities <span class="comment"># 是否开启硬件检测,默认false</span></span><br><span class="line">yarn.nodemanager.resource.count-physical-cores <span class="comment"># 是否将物理核数当作CPU核数，默认false</span></span><br><span class="line">yarn.nodemanager.resource.pcores-vcores-multiplier  <span class="comment"># 虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2，默认1.0</span></span><br><span class="line">yarn.nodemanager.pmem-check-enabled <span class="comment"># 是否开启物理内存检查限制container，默认打开</span></span><br><span class="line">yarn.nodemanager.vmem-check-enabled <span class="comment"># 是否开启虚拟内存检查限制container，默认打开</span></span><br><span class="line">yarn.nodemanager.vmem-pmem-ratio <span class="comment"># 虚拟内存物理内存比例，默认2.1</span></span><br><span class="line">yarn.nodemanager.resource.memory-mbNodeManager <span class="comment"># 使用内存，默认8G</span></span><br><span class="line">yarn.nodemanager.resource.system-reserved-memory-mbNodeManager <span class="comment"># 为系统保留多少内存以上二个参数配置一个即可</span></span><br><span class="line">yarn.nodemanager.resource.cpu-vcoresNodeManager <span class="comment"># 使用CPU核数，默认8个</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.Contrainer相关</span></span><br><span class="line">yarn.scheduler.minimum-allocation-mb <span class="comment"># 容器最最小内存，默认1G</span></span><br><span class="line">yarn.scheduler.maximum-allocation-mb <span class="comment"># 容器最最大内存，默认8G</span></span><br><span class="line">yarn.scheduler.minimum-allocation-vcores <span class="comment"># 容器最小CPU核数，默认1个</span></span><br><span class="line">yarn.scheduler.maximum-allocation-vcores <span class="comment"># 容器最大CPU核数，默认4个</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="分布式协调器zookeeper"><a class="markdownIt-Anchor" href="#分布式协调器zookeeper"></a> 分布式协调器Zookeeper</h2>
<h3 id="zookerper简介"><a class="markdownIt-Anchor" href="#zookerper简介"></a> Zookerper简介</h3>
<p>Zookeeper是一个开源的分布式的，为分布式框架提供协调服务的Apache项目</p>
<p>是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应。</p>
<h3 id="zookerper架构"><a class="markdownIt-Anchor" href="#zookerper架构"></a> Zookerper架构</h3>
<p>ZooKeeper是一个由奇数（2n+1）台同构服务器组成的集群，它采用主-从结构</p>
<ul>
<li>一台主服务器Leader，若干台从服务器Follower，只要有超过半数的服务器能够工作，ZooKeeper集群就能正常工作</li>
<li>集群内所有服务器均保持同步，都接受客户端的读写请求</li>
<li>如果ZooKeeper集群中的Leader出现了故障，那么Follower们就会通过一定的策略，选举出一个新的Leader；</li>
</ul>
<p>Learder功能：</p>
<ul>
<li>启动或重启时恢复数据</li>
<li>检测Follower的心跳</li>
<li>结束Foller发来的请求根据不同的类型做出处理，比如写</li>
</ul>
<p>Follower功能：</p>
<ul>
<li>接受Leader的消息，并完成工作</li>
<li>向Leader发送心跳</li>
<li>接受客户端的请求，如果为读请求，直接返回结果，如果为写请求，转发给Leader</li>
</ul>
<h3 id="zookerper数据模型"><a class="markdownIt-Anchor" href="#zookerper数据模型"></a> Zookerper数据模型</h3>
<p>Zookeeper维护一个类似于树状的文件系统</p>
<ul>
<li>树状结构中有一个根节点，称为&quot;/&quot;，其他节点建立在根节点上，每个节点称为ZNode</li>
<li>每个节点可以存储不超过1MB的数据，持久性的ZNode还可以在创建子节点</li>
</ul>
<p>ZNode的类型：</p>
<ul>
<li>持久性节点：一旦创建，除非主动删除，否则一直存在</li>
<li>持久性顺序节点：一旦创建，除非主动删除，否则一直存在，节点名后面会有一个递增的序号</li>
<li>临时节点：客户端断开连接后，该节点会被删除</li>
<li>临时顺序节点：客户端断开连接后，该节点会被删除，节点名后面会有一个递增的序号</li>
</ul>
<p>ZNode组成：</p>
<ol>
<li>访问控制列表（ACL）：控制对ZNode的访问权限</li>
<li>ZNode自身状态信息：创建者ID等</li>
<li>ZNode数据：存储的数据</li>
</ol>
<h3 id="zookeeper的一致性"><a class="markdownIt-Anchor" href="#zookeeper的一致性"></a> Zookeeper的一致性</h3>
<h4 id="会话session机制"><a class="markdownIt-Anchor" href="#会话session机制"></a> 会话（Session）机制</h4>
<ul>
<li>当客户端成功连接到ZooKeeper时，就与之建立了一个会话，客户端通过定时向ZooKeeper发送心跳消息来保持会话有效</li>
<li>如果ZooKeeper在规定时间段（默认为180秒）内未能收到某个客户端的心跳消息，则使其会话失效，即导致该客户端与ZooKeeper断开</li>
<li>如果因服务器负载过重、网络阻塞等导致客户端与ZooKeeper集群内某个服务器断开，客户端只要在规定时间段内与ZooKeeper集群内的任何一个服务器连接上，该会话仍然有效。</li>
</ul>
<h4 id="监视watcher机制"><a class="markdownIt-Anchor" href="#监视watcher机制"></a> 监视（Watcher）机制</h4>
<ul>
<li>客户端可以在某个Znode上设置一个Watcher（监视器），来监视该Znode的状态；</li>
<li>Watcher一次性有效</li>
<li>一旦被设置了Watcher的Znode的状态发生变化，ZooKeeper服务端会将此事件通知设置Watcher的客户端，并根据事件类型触发回调客户端事先设置的处理逻辑</li>
</ul>
<h3 id="zookeeper的自身一致性"><a class="markdownIt-Anchor" href="#zookeeper的自身一致性"></a> Zookeeper的自身一致性</h3>
<p><strong>myid</strong>:Zookeeper集群中每个服务器的唯一标识，myid文件中只包含一个数字，这个数字就是这个服务器的编号<br />
<strong>ZXID</strong>:</p>
<ul>
<li>ZooKeeper为每个事务操作分配一个全局单调递增的事务编号（ZXID），每个ZXID对应于一次事务操作，它随事务一起被记入事务日志.</li>
<li>ZXID是一个64位二进制数，其高32位为Leader周期的编号，称作epoch，新选出的Leader的epoch为其前任Leader的epoch值加1</li>
<li>ZXID的低32位是一个单调递增的计数器，Leader在执行一个新的事务操作时，都会对该计数器作加1操作，其与epoch组合成此事务操作的ZXID</li>
<li>每当选举出一个新的Leader时，就从该Leader的事务日志内挑选出数值最大的事务编号ZXID，对其中的epoch值作加1操作，以此作为新Leader的周期编号，并将低32位置0，从而形成新的ZXID</li>
</ul>
<h4 id="原子广播zab-zookeeper-atomic-broadcast"><a class="markdownIt-Anchor" href="#原子广播zab-zookeeper-atomic-broadcast"></a> 原子广播ZAB (ZooKeeper Atomic Broadcast)</h4>
<p><strong>恢复模式</strong>：</p>
<ul>
<li>系统启动或Leader发生故障时，ZAB进入恢复模式</li>
<li>立即进行一次Leader选举，选出一个新的Leader</li>
<li>让集群中至少有超过半数的Follower与Leader具有相同的系统状态，即实现数据同步</li>
<li>ZAB退出恢复模式，进入广播模式</li>
</ul>
<p><strong>广播模式（写流程）</strong>：</p>
<ul>
<li>Leader收到客户端发来的事务操作请求时，Leader通过ZAB的广播模式向集群内的所有Follower进行消息广播，即发送事务操作请求消息</li>
<li>各个Follower收到Leader广播（发送）的事务操作请求后，把将要作的事务操作及其ZXID记入各自的事务日志，然后向Leader回复ack（确认）消息</li>
<li>若Leader收不到超过半数的Follower回复的ack消息，则取消本次更新操作</li>
<li>若Leader收到超过半数的Follower回复的ack（确认）消息，则向Follower们进行消息广播—向它们发送commit（许可）消息</li>
<li>Follower收到Leader的commit消息，就真正执行本次更新操作，即更新内存或Znode内的数据</li>
<li>整个ZAB的广播模式执行过程是一个整体，不能被打断，其结果只有成功或失败，不存在中间状态</li>
</ul>
<p><img src="https://image.yayan.xyz/20240318211258.png" alt="写流程之写入请求直接发送给Leader节点" /></p>
<p><img src="https://image.yayan.xyz/20240318211326.png" alt="写流程之写入请求发送给follower节点" /></p>
<h3 id="zookeeper的选举机制"><a class="markdownIt-Anchor" href="#zookeeper的选举机制"></a> Zookeeper的选举机制</h3>
<h4 id="选举的基本原则"><a class="markdownIt-Anchor" href="#选举的基本原则"></a> 选举的基本原则</h4>
<ul>
<li>选举开始时，ZooKeeper集群内各台服务器上的数据不一定会完全一致，在选出Leader之后，就要以该Leader为基准来同步其他服务器上的数据；</li>
<li>应该把集群内拥有最新数据的服务器选为Leader，故必须挑选其事务日志中具有最大ZXID的那台服务器作为Leader，因为该服务器进行了最新的事务操作，故其拥有的数据是最新的，以其作为基准来恢复和同步数据，则可以保证数据的完整性；</li>
<li>若具有最大ZXID的服务器不止一个，则选其中myid最大者为Leader</li>
</ul>
<h4 id="选举流程"><a class="markdownIt-Anchor" href="#选举流程"></a> 选举流程</h4>
<p><img src="https://image.yayan.xyz/20240318210658.png" alt="" /></p>
<ol>
<li>集群内各服务器均进入LOOKING状态，进行一轮选举投票，即各服务器均向其他服务器发送投票消息，消息的内容为自身的myid和自身最大的ZXID，也就是把自身定为候选Leader（争作Leader）</li>
<li>各服务器接收投票消息（包括自己的票），从中挑选出具有最大ZXID的服务器作为候选Leader ，若这样的服务器有多个，则挑选其中myid最大者</li>
<li>各服务器统计本轮投票中候选Leader的得票数
<ul>
<li>若未过半数，则把前一步挑选出的候选Leader的myid和ZXID记入投票消息，进行下一轮投票</li>
<li>如果候选Leader得票数过半，则判别候选Leader是否是自身，若是，则该服务器进入LEADING状态，否则该服务器进入FOLLOWING状态</li>
</ul>
</li>
</ol>
<p>###　Zookeeper的操作</p>
<h4 id="shell操作-2"><a class="markdownIt-Anchor" href="#shell操作-2"></a> Shell操作</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看节点</span></span><br><span class="line"><span class="built_in">ls</span> /</span><br><span class="line"><span class="comment"># 查看节点详细信息</span></span><br><span class="line"><span class="built_in">ls</span> -s /</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建永久节点</span></span><br><span class="line">create /node1 <span class="string">&quot;data1&quot;</span></span><br><span class="line"><span class="comment"># 创建永久顺序节点</span></span><br><span class="line">create -s /node2 <span class="string">&quot;data2&quot;</span></span><br><span class="line"><span class="comment"># 创建临时节点</span></span><br><span class="line">create -e /node3 <span class="string">&quot;data3&quot;</span></span><br><span class="line"><span class="comment"># 创建临时顺序节点</span></span><br><span class="line">create -s -e /node4 <span class="string">&quot;data4&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看节点数据</span></span><br><span class="line">get -s /node1</span><br><span class="line"><span class="comment"># 查看节点详细信息</span></span><br><span class="line"><span class="built_in">stat</span> /node1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改节点数据</span></span><br><span class="line"><span class="built_in">set</span> /node1 <span class="string">&quot;data1-1&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除节点</span></span><br><span class="line">delete /node1</span><br><span class="line"><span class="comment"># 删除节点并递归删除子节点</span></span><br><span class="line">deleteall /node1</span><br></pre></td></tr></table></figure>
<h4 id="api操作-2"><a class="markdownIt-Anchor" href="#api操作-2"></a> API操作</h4>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建监视点</span></span><br><span class="line"><span class="comment">// 1. 创建配置对象</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">String</span> <span class="variable">connectString</span> <span class="operator">=</span> <span class="string">&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="type">int</span> <span class="variable">sessionTimeout</span> <span class="operator">=</span> <span class="number">2000</span>;</span><br><span class="line"><span class="keyword">private</span> ZooKeeper zkClient;</span><br><span class="line"><span class="comment">// 2. 创建zookeeper的连接</span></span><br><span class="line">zkClient = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(connectString, sessionTimeout, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent watchedEvent)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;默认回调函数&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 使用自定义的监视</span></span><br><span class="line">zk.exists(<span class="string">&quot;/path/to/znode&quot;</span>, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">        <span class="comment">// 处理监视事件的逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. 使用默认的监视</span></span><br><span class="line">zk.exists(<span class="string">&quot;/path/to/znode&quot;</span>, <span class="literal">true</span>);</span><br></pre></td></tr></table></figure>
<h2 id="分布式数据库hbase"><a class="markdownIt-Anchor" href="#分布式数据库hbase"></a> 分布式数据库HBase</h2>
<h3 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h3>
<p>HBase是一种以HDFS为依据的分布式可扩展NoSQL列族数据库</p>
<h4 id="nosql理论基础-cap理论与base理论"><a class="markdownIt-Anchor" href="#nosql理论基础-cap理论与base理论"></a> NoSQL理论基础: CAP理论与BASE理论</h4>
<p>传统关系数据库技术的核心特征：事务操作必须遵循ACID（原子性Atomicity、一致性Consistancy、隔离性Isolation、持久性Durability）原则，具有强事务、强一致性。</p>
<p>NoSQL数据库的特点：NoSQL方案弱化事务处理的ACID特性，一般不持支传统数据库的SQL查询语言</p>
<p>CAP理论：在设计和部署分布式应用时，存在三个核心的系统需求：</p>
<ul>
<li>C：Consistency（一致性）—在对数据进行更新和删除操作后，用户都能看到相同的数据视图。</li>
<li>A：Availability（可用性）—可用性主要是指系统能够很好地为用户服务，不出现用户操作失败或者访问超时等用户体验不佳的情况（比如由强一致性带来的副作用）。</li>
<li>P：Partition Tolerance（分区容错性）—分区容错性和可扩展性是紧密相关的，好的分区容错性要求一个分布式系统就像是一个运转正常的整体，当系统中有部分网络或节点发生故障时，仍然能够依靠系统中其余完好的部分来保证系统正常运作。</li>
<li>CAP理论的核心：一个分布式系统不可能同时很好地满足一致性C，可用性A、分区容错性P这三个需求，最多只能同时较好地满足其中的两个需求。</li>
</ul>
<p>OldSQL为了不降低可用性，通常对数据采用不分散存储的策略，使可扩展性（即分区容错性P）受到限制，可看作保证C、A，放弃P；</p>
<p>NoSQL为了获得可扩展性，又不降低可用性，在设计中就会弱化甚至去除事务的ACID要求，可看作保证A、P，放弃C。</p>
<p>BASE (BA,S,E) 理论：牺牲强一致性，获得可用性或可靠性，表现为以下三点：</p>
<ul>
<li>BA：Basically Availability（基本可用）</li>
<li>S：Soft State（软状态）—允许系统中的数据存在中间状态，而这个中间状态不会影响系统的整体可用性。硬状态”是指严格遵循ACID原则的事务功能；</li>
<li>E: Eventually Consistency（最终一致性）—系统中的数据经过一段时间后，最终会达到一致的状态。</li>
</ul>
<h4 id="hbase逻辑结构"><a class="markdownIt-Anchor" href="#hbase逻辑结构"></a> HBase逻辑结构</h4>
<p>HBase的逻辑结构可以看作是一个二维的表格结构,行代表RowKey(<strong>唯一,按RowKey排序</strong>),列代表列族,其中列族下面可以有多个列限定符,每个列限定符下面存储一个值,这个值可以是多个版本的,每个版本都有一个时间戳,时间戳是一个64位的整数,代表了这个版本的时间,时间戳越大,版本越新</p>
<p><img src="https://image.yayan.xyz/20240318191544.png" alt="" /></p>
<h4 id="hbase物理结构"><a class="markdownIt-Anchor" href="#hbase物理结构"></a> HBase物理结构</h4>
<p>HBase的物理结构是基于HDFS的,每个表都会有一个对应的目录,目录下面有两个子目录,分别是data和wal,其中data目录存储了HFile文件,而wal目录存储了WAL文件(Write-Ahead-Log,预写日志).</p>
<p>HBase的RegionServer和HMaster不负责存储数据,只负责管理数据,而实际的数据存储在HDFS上</p>
<h4 id="hbase数据模型"><a class="markdownIt-Anchor" href="#hbase数据模型"></a> HBase数据模型</h4>
<ol>
<li>Namespace: 命名空间,类似于关系数据库中的database概念.命名空间下可以有多个表,HBase自带两个命名空间,分别是default和hbase,hbase中存放的是HBase内置的表,其中meta表存放了所有表的元数据信息</li>
<li>Table: 表,类似于关系数据库中的table概念,表中存放的是多行数据,每行数据都有一个唯一的RowKey,表中的数据是按照RowKey进行排序的</li>
<li>Row: HBase表中的每行数据都由一个RowKey和多个Column（列）组成，数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要</li>
<li>Cell: 由{rowkey,  columnFamily：columnQualifier, timestamp} 唯一确定的单元。Cell为空时,不会存储在HBase中.</li>
</ol>
<h4 id="基本架构"><a class="markdownIt-Anchor" href="#基本架构"></a> 基本架构</h4>
<p><img src="https://image.yayan.xyz/20240318192200.png" alt="" /></p>
<ol>
<li>
<p>Master服务器负责管理所有Region服务器和数据表(hbase:meta)</p>
<ul>
<li>其本身不存储HBase中的数据；</li>
<li>接收用户对<strong>表格</strong>创建修改删除的命令并执行</li>
<li>监控region是否需要进行负载均衡，故障转移和region的拆分</li>
</ul>
</li>
<li>
<p>Region服务器是HBase中最核心的部分：</p>
<ul>
<li>Region服务器是HBase的读写节点，它为用户提供对<strong>数据表数据</strong>的读写服务</li>
<li>一张数据表被划分成(横向划分)多个HRegion，这些HRegion被分布到Region服务器集群内进行管理</li>
</ul>
</li>
<li>
<p>HRegion是一个HBase表横向切割的结果:</p>
<ul>
<li>在HRegion中,每个列族又被分为一个Store.每个Store中存储了一个列族的数据,不包含空元素</li>
<li>Store中包含一个MemStore,一个Block Cache和多个HFile,MemStore负责缓存写入的数据(有序,每次flush都回形成一个HFile),Block Cache负责缓存读取的数据,HFile是HBase中的数据存储文件</li>
</ul>
</li>
<li>
<p>Zookeeper监视Region服务器和Master服务器的运行状态</p>
<ul>
<li>各个Region服务器会在ZooKeeper的Z节点/server上建立临时性顺序节点，Master服务器在/server上设置Watcher，可以随时感知到各个Region服务器的运行状态；</li>
<li>当前的Active Master服务器在ZooKeeper上建立临时性Z节点/Master，各个Region服务器和Master集群内的其它服务器均在/Master上设置Watcher，它们可以随时感知到当前的Active Master服务器的工作（运行）状态；</li>
</ul>
</li>
</ol>
<h3 id="hbase操作"><a class="markdownIt-Anchor" href="#hbase操作"></a> HBase操作</h3>
<h4 id="shell操作-3"><a class="markdownIt-Anchor" href="#shell操作-3"></a> Shell操作</h4>
<h5 id="基本操作"><a class="markdownIt-Anchor" href="#基本操作"></a> 基本操作</h5>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建命名空间</span></span><br><span class="line">create_namespace <span class="string">&#x27;ns1&#x27;</span></span><br><span class="line"><span class="comment"># 查看所有命名空间</span></span><br><span class="line">list_namespace</span><br></pre></td></tr></table></figure>
<h5 id="ddl操作"><a class="markdownIt-Anchor" href="#ddl操作"></a> DDL操作</h5>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建表</span></span><br><span class="line"><span class="comment"># 在ns1命名空间下创建表table1,列族为cf1,cf2</span></span><br><span class="line">create <span class="string">&#x27;ns1:table1&#x27;</span>, <span class="string">&#x27;cf1&#x27;</span>, <span class="string">&#x27;cf2&#x27;</span></span><br><span class="line"><span class="comment"># 在ns1命名空间下创建表table1,列族为cf1,cf2,并指定版本数</span></span><br><span class="line">create <span class="string">&#x27;ns1:table1&#x27;</span>, &#123;NAME =&gt; <span class="string">&#x27;cf1&#x27;</span>, VERSIONS =&gt; 3&#125;, &#123;NAME =&gt; <span class="string">&#x27;cf2&#x27;</span>, VERSIONS =&gt; 5&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看表</span></span><br><span class="line">list</span><br><span class="line"><span class="comment"># 查看表详情</span></span><br><span class="line">describe <span class="string">&#x27;ns1:table1&#x27;</span></span><br><span class="line"><span class="comment"># 删除表</span></span><br><span class="line"><span class="built_in">disable</span> <span class="string">&#x27;ns1:table1&#x27;</span></span><br><span class="line">drop <span class="string">&#x27;ns1:table1&#x27;</span></span><br><span class="line"><span class="comment"># 修改表</span></span><br><span class="line"><span class="comment"># 修改表的列族的版本数</span></span><br><span class="line">alter <span class="string">&#x27;ns1:table1&#x27;</span>, &#123;NAME =&gt; <span class="string">&#x27;cf1&#x27;</span>, VERSIONS =&gt; 5&#125;</span><br><span class="line"><span class="comment"># 删除表的列族</span></span><br><span class="line">alter <span class="string">&#x27;ns1:table1&#x27;</span>, <span class="string">&#x27;delete&#x27;</span> =&gt; <span class="string">&#x27;cf1&#x27;</span></span><br><span class="line">alter <span class="string">&#x27;ns1:table1&#x27;</span>, &#123;NAME =&gt; <span class="string">&#x27;cf1&#x27;</span>, METHOD =&gt; <span class="string">&#x27;delete&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<h5 id="dml操作"><a class="markdownIt-Anchor" href="#dml操作"></a> DML操作</h5>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 插入数据</span></span><br><span class="line"><span class="comment"># 如果重复写入相同rowKey，相同列的数据，会写入多个版本进行覆盖。</span></span><br><span class="line"><span class="comment"># 向ns1:table1表中插入一行数据,行键为row1,列族为cf1,列限定符为col1,值为value1</span></span><br><span class="line">put <span class="string">&#x27;ns1:table1&#x27;</span>, <span class="string">&#x27;row1&#x27;</span>, <span class="string">&#x27;cf1:col1&#x27;</span>, <span class="string">&#x27;value1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据get</span></span><br><span class="line"><span class="comment"># 读取ns1:table1表中的一行数据,行键为row1</span></span><br><span class="line">get <span class="string">&#x27;ns1:table1&#x27;</span>, <span class="string">&#x27;row1&#x27;</span></span><br><span class="line"><span class="comment"># 读取ns1:table1表中的一行数据,行键为row1,列族为cf1</span></span><br><span class="line">get <span class="string">&#x27;ns1:table1&#x27;</span>, <span class="string">&#x27;row1&#x27;</span>, &#123;COLUMN =&gt; <span class="string">&#x27;cf1&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据scan</span></span><br><span class="line"><span class="comment"># 读取ns1:table1表中的所有数据</span></span><br><span class="line">scan <span class="string">&#x27;ns1:table1&#x27;</span></span><br><span class="line"><span class="comment"># 读取ns1:table1表中的所有数据,并指定列族</span></span><br><span class="line">scan <span class="string">&#x27;ns1:table1&#x27;</span>, &#123;COLUMN =&gt; <span class="string">&#x27;cf1&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 读取ns1:table1表中的所有数据,从row1开始,到row2结束</span></span><br><span class="line">scan <span class="string">&#x27;ns1:table1&#x27;</span>, &#123;STARTROW =&gt; <span class="string">&#x27;row1&#x27;</span>, ENDROW =&gt; <span class="string">&#x27;row2&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除数据</span></span><br><span class="line"><span class="comment"># 删除ns1:table1表中的一行数据,行键为row1</span></span><br><span class="line">delete <span class="string">&#x27;ns1:table1&#x27;</span>, <span class="string">&#x27;row1&#x27;</span></span><br><span class="line"><span class="comment"># 删除ns1:table1表中的一行数据,行键为row1,列为cf1中的name</span></span><br><span class="line">delete <span class="string">&#x27;ns1:table1&#x27;</span>, <span class="string">&#x27;row1&#x27;</span>, <span class="string">&#x27;cf1：name&#x27;</span></span><br><span class="line"><span class="comment"># 删除数据的所有版本</span></span><br><span class="line">deleteall <span class="string">&#x27;ns1:table1&#x27;</span>, <span class="string">&#x27;row1&#x27;</span>, <span class="string">&#x27;cf1：name&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行命令会标记数据为要删除，不会直接将数据彻底删除，删除数据只在特定时期清理磁盘时进行</span></span><br></pre></td></tr></table></figure>
<h4 id="api操作-3"><a class="markdownIt-Anchor" href="#api操作-3"></a> API操作</h4>
<p>HBase的客户端连接由ConnectionFactory类来创建，用户使用完成之后需要手动关闭连接。同时连接是一个重量级的，推荐一个进程使用一个连接，对HBase的命令通过连接中的两个属性Admin(DDL)和Table(DML)来实现。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建连接</span></span><br><span class="line"><span class="comment">//1. 创建配置对象</span></span><br><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"><span class="comment">// 2. 添加配置参数</span></span><br><span class="line">conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>,<span class="string">&quot;hadoop102,hadoop103,hadoop104&quot;</span>);</span><br><span class="line"><span class="comment">// 3. 创建hbase的连接</span></span><br><span class="line"><span class="comment">// 默认使用同步连接Connection                 </span></span><br><span class="line">connection = ConnectionFactory.createConnection(conf);</span><br><span class="line"><span class="comment">// 可以使用异步连接</span></span><br><span class="line"><span class="comment">// 主要影响后续的DML操作</span></span><br><span class="line">CompletableFuture&lt;AsyncConnection&gt; asyncConnection = ConnectionFactory.createAsyncConnection(conf);</span><br><span class="line"><span class="comment">// 4. 使用连接</span></span><br><span class="line">System.out.println(connection);</span><br><span class="line"><span class="comment">// 5. 关闭连接</span></span><br><span class="line">connection.close();</span><br></pre></td></tr></table></figure>
<p>HBase在API操作时使用了装饰者（设计师）模式。因为shell命令的参数很多，所以在API中使用了装饰者模式，将参数封装成对象，然后通过对象的方法来实现shell命令的功能。</p>
<h3 id="hbase一些原理"><a class="markdownIt-Anchor" href="#hbase一些原理"></a> HBase一些原理</h3>
<h4 id="hbase架构"><a class="markdownIt-Anchor" href="#hbase架构"></a> HBase架构</h4>
<p><img src="https://image.yayan.xyz/20240318195105.png" alt="" /></p>
<ol>
<li>hbase:meta 表中存储了 Hbase 集群中全部表的所有的Hregion 信息,在list命令中被过滤掉了</li>
<li>StoreFile保存实际数据的物理文件，StoreFile以HFile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。</li>
<li>MemStore写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。</li>
<li>WAL由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</li>
<li>Store</li>
</ol>
<h4 id="hbase的写流程"><a class="markdownIt-Anchor" href="#hbase的写流程"></a> HBase的写流程</h4>
<p><img src="https://image.yayan.xyz/20240318195306.png" alt="" /></p>
<ol>
<li>Client先访问zookeeper，获取hbase:meta表位于哪个RegionServer。</li>
<li>访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个RegionServer中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</li>
<li>与目标RegionServer进行通讯；</li>
<li>将数据顺序写入（追加）到WAL(HLog)；</li>
<li>将数据写入对应的MemStore，数据会在MemStore进行排序；</li>
<li>向客户端发送ack；</li>
<li>等达到MemStore的刷写时机后，将数据刷写到HFile。</li>
<li>在步骤4，5，源码的步骤是先写入HLog，在写入MemStore，然后再同步HLog。如果HLog如果写入失败，就会事务回滚。</li>
</ol>
<h4 id="hbase的读流程"><a class="markdownIt-Anchor" href="#hbase的读流程"></a> HBase的读流程</h4>
<p><img src="https://image.yayan.xyz/20240318200601.png" alt="" /></p>
<blockquote>
<p>误区：HBase的读流程并不是只读BlockCache的数据，考虑这种情况:<br />
第一次写数据的时候成功写入，并flush罗盘了<br />
第二次写数据的时候写入了MemStore，但是此时把ts设置为了比第一次写的ts小，但是没有罗盘<br />
如果只读BlockCache，那么第二次写的数据就会被读到，这样就会出现数据读取不是最新的情况。</p>
</blockquote>
<ol>
<li>Client先访问zookeeper，获取hbase:meta表位于哪个RegionServer。</li>
<li>访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个RegionServer中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</li>
<li>与目标RegionServer进行通讯；</li>
<li>分别在BlockCache（读缓存），MemStore和StoreFile（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（timestamp）或者不同的类型（Put/Delete）</li>
<li>将从文件中查询到的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到BlockCache。</li>
<li>将合并后的最终结果返回给客户端。</li>
</ol>
<h4 id="hbase的flush操作"><a class="markdownIt-Anchor" href="#hbase的flush操作"></a> HBase的Flush操作</h4>
<p><img src="https://image.yayan.xyz/20240318201159.png" alt="" /></p>
<ol>
<li>Flush操作的基本单位是HRegion，即对HRegion的所有MemStore均进行Flush操作，并各自形成单独的StoreFile</li>
<li>MemStore中的数据达到阈值（128M）后，其所在region的所有memstore都会刷写。</li>
<li>到达自动刷写的时间（默认1小时），<strong>最后一次</strong>刷写的时间到当前时间间隔超过了自动刷写的时间间隔。</li>
<li>HRegion的所有MemStore中的数据总量到达阈值（JVM heap的40%）后，也会触发Flush操作。
<ul>
<li>当阈值到低位线（总阈值的95%，JVM heap的38%）时，region会按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下。</li>
<li>当阈值到高位线时，region会同时阻止继续往所有的memstore写数据。</li>
</ul>
</li>
</ol>
<h4 id="hbase的compaction操作"><a class="markdownIt-Anchor" href="#hbase的compaction操作"></a> HBase的Compaction操作</h4>
<p><img src="https://image.yayan.xyz/20240318202739.png" alt="" /><br />
Compaction合并的原因：</p>
<p>StoreFile是只读文件，其内容不能被更新（增加、修改、删除），以此提升表数据的安全性<br />
如果要更新StoreFile内的表数据，则必须以新增StoreFile的形式进行，把欲更新的数据（包含属性或版本号）写入新增的StoreFile中</p>
<p>（HBase不停的刷写，导致存储目录中有过多的数据文件，文件太多会导致维护困难、降低数据查询性能和效率。对一堆的文件进行I/O操作，耗时太多。所以HBase定期会对这些琐碎的文件进行整理，即合并Compaction。）</p>
<p>Compaction合并的步骤：</p>
<p>分为三步：排序文件、合并文件、代替原文件服务。</p>
<p>HBase首先从待合并的文件中读出HFile中的key-value,再按照由小到大的顺序写入一个新文件(storeFile)中。这个新文件将代替所有之前的文件，对外提供服务。</p>
<p>Compaction操作分为两种：</p>
<ul>
<li>Minor Compaction：只合并相邻的（3个）小文件，不会合并所有的文件，不会清理过期和删除的数据。</li>
<li>major Compaction：合并所有的文件，产生一个新的文件。</li>
</ul>
<p>Compaction大合并时，清空以下数据：</p>
<ol>
<li>标记为删除的数据。
<ul>
<li>当我们删除数据时，HBase并没有把这些数据立即删除，而是将这些数据打了一个个标记，称为“墓碑”标记。在HBase合并时，会将这些带有墓碑标记的数据删除。</li>
</ul>
</li>
<li>TTL过期数据
<ul>
<li>TTL(time to live)指数据包在网络中的时间。如果列族中设置了TTL过期时间，则在合并的过程中，发现过期的数据将被删除。</li>
</ul>
</li>
<li>版本合并
<ul>
<li>若版本号超过了列族中预先设定的版本号，则将最早的一条数据删除。</li>
</ul>
</li>
</ol>
<p>Compaction合并的触发条件：</p>
<ol>
<li>内存中的数据flush刷写到硬盘上以后，会对当前Store中的文件进行判断，当数量达到阈值，则会触发Compaction。</li>
<li>Compaction Checker线程定期检查是否触发Compaction，Checker会优先检查文件数量是否大于阈值，再判断是否满足major Compaction的条件的时间范围内（7天），如果满足，则触发一次大合并Major Compaction。</li>
<li>手动合并</li>
</ol>
<h4 id="hbase的split操作"><a class="markdownIt-Anchor" href="#hbase的split操作"></a> HBase的Split操作</h4>
<p><img src="https://image.yayan.xyz/20240318202833.png" alt="" /><br />
Split原因：<br />
随着HRegion内的数据被持续追加，StoreFile文件的数量和长度会不断增大，由此引起Store的不断增大，从而导致HRegion的长度持续增大；</p>
<p>Split条件：<br />
当1个region中的某个Store下所有StoreFile的总大小超过Min(R^2 * “hbase.hregion.memstore.flush.size”,&quot; hbase.hregion.max.filesize &quot;) 就会拆分，其中R为当前RegionServer中属于该table的region个数<br />
具体的切分策略为：<br />
第一次split：1^3 * 256 = 256MB<br />
第二次split：2^3 * 256 = 2048MB<br />
第三次split：3^3 * 256 = 6912MB<br />
第四次split：4^3 * 256 = 16384MB &gt; 10GB，<br />
因此取较小的值10GB 后面每次split的size都是10GB了。</p>
<p>Split过程：<br />
Region的拆分是由HRegionServer完成的，在操作之前需要通过ZK汇报master，修改对应的Meta表信息添加两列info：splitA和info：splitB信息。之后需要操作HDFS上面对应的文件，按照拆分后的Region范围进行标记区分，实际操作为创建文件引用，不会挪动数据。刚完成拆分的时候，两个Region都由原先的RegionServer管理。之后汇报给Master，由Master将修改后的信息写入到Meta表中。等待下一次触发负载均衡机制，才会修改Region的管理服务者，而数据要等到下一次压缩时，才会实际进行移动。</p>
<h2 id="离线数据仓库hive"><a class="markdownIt-Anchor" href="#离线数据仓库hive"></a> 离线数据仓库Hive</h2>
<h3 id="hive简介"><a class="markdownIt-Anchor" href="#hive简介"></a> Hive简介</h3>
<p>Hive是一个基于Hadoop的数据仓库工具(一个Hadoop的客户端)，可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。</p>
<ul>
<li>Hive中每张表的数据存储在HDFS</li>
<li>Hive分析数据底层的实现是MapReduce（也可配置为Spark或者Tez）</li>
<li>执行程序运行在Yarn上</li>
</ul>
<h3 id="hive的架构"><a class="markdownIt-Anchor" href="#hive的架构"></a> Hive的架构</h3>
<p><img src="https://image.yayan.xyz/20240319141546.png" alt="" /></p>
<ol>
<li>
<p>Hive Client</p>
<ul>
<li>Hive CLI:Hive提供的命令行接口(只能在安装了Hive的机器上使用)</li>
<li>远程连接的话需要使用JDBC或者ODBC客户端，连接到HiveServer2</li>
</ul>
</li>
<li>
<p>Metastore:提供元数据的访问接口</p>
<ul>
<li>元数据是指:用户创建的数据库,表的一些信息(在HDFS中的路径,字段信息等)</li>
<li>只负责提供元数据的访问接口,不负责存储元数据</li>
<li>元数据通常保存在关系型数据库中,默认是Derby,推荐是MySQL(derby数据库的特点是同一时间只允许一个客户端访问。如果多个Hive客户端同时访问，就会报错。)</li>
</ul>
</li>
<li>
<p>HiveServer2</p>
<ul>
<li>提供JDBC或者ODBC的访问接口</li>
<li>提供用户认证功能</li>
</ul>
</li>
<li>
<p>Driver:需要用到元数据信息</p>
<ul>
<li>解析器:将SQL字符串解析成抽象语法树</li>
<li>语义分析:将抽象语法树转换成QueryBlock</li>
<li>逻辑计划生成器:将语法树生成逻辑计划</li>
<li>逻辑优化器:对逻辑计划进行优化</li>
<li>物理计划生成器:将逻辑计划生成物理计划</li>
<li>物理优化器:对物理计划进行优化</li>
<li>执行器:执行物理计划得到结果返回客户端</li>
</ul>
</li>
</ol>
<h4 id="元数据库配置"><a class="markdownIt-Anchor" href="#元数据库配置"></a> 元数据库配置</h4>
<ol>
<li>安装好MySQL,并新建数据库 <code>create database metastore</code></li>
<li>将MySQL的JDBC驱动拷贝到Hive的lib目录下</li>
<li>conf目录下新建hive-site.xml文件,添加mysql配置项</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--jdbc连接的URL--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--jdbc连接的Driver--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--jdbc连接的username--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--jdbc连接的password--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Hive默认在HDFS的工作目录--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol start="4">
<li>初始化源数据库 <code>bin/schematool -dbType mysql -initSchema -verbose</code></li>
<li>使用Hive新建表,查看MySQL中的metastore数据库,可以看到Hive的元数据信息</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Hive中创建表</span></span><br><span class="line"><span class="keyword">show</span> databases;</span><br><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(idint,namestring);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> stu <span class="keyword">values</span>(<span class="number">1</span>,&quot;ss&quot;);</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> stu;</span><br><span class="line"><span class="comment">-- 查看MySQL中的metastore数据库</span></span><br><span class="line"><span class="keyword">show</span> databases;</span><br><span class="line">use metastore;</span><br><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"><span class="comment">-- table中会出现元数据信息</span></span><br></pre></td></tr></table></figure>
<h4 id="hiveserver2服务配置"><a class="markdownIt-Anchor" href="#hiveserver2服务配置"></a> Hiveserver2服务配置</h4>
<ol>
<li>Hiveserver2说明<br />
<img src="https://image.yayan.xyz/20240319144209.png" alt="开启用户模拟功能" /><br />
<img src="https://image.yayan.xyz/20240319144253.png" alt="未开启用户模拟功能" /><br />
在远程访问Hive数据时，客户端并未直接访问Hadoop集群，而是由Hivesever2代理访问。由于Hadoop集群中的数据具备访问权限控制，所以此时需考虑一个问题：那就是访问Hadoop集群的用户身份是谁？是Hiveserver2的启动用户？还是客户端的登录用户？</li>
</ol>
<p>答案是都有可能，具体是谁，由Hiveserver2的hive.server2.enable.doAs参数决定，该参数的含义是是否启用Hiveserver2用户模拟的功能。</p>
<p>若启用，则Hiveserver2会模拟成客户端的登录用户去访问Hadoop集群的数据，<br />
不启用，则Hivesever2会直接使用启动用户访问Hadoop集群数据。模拟用户的功能，默认是开启的。<br />
推荐开启用户模拟功能，因为开启后才能保证各用户之间的权限隔离。</p>
<ol start="2">
<li>Hiveserver2配置</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- # Hadoop</span></span><br><span class="line"><span class="comment"># hivesever2的模拟用户功能，依赖于Hadoop提供的proxyuser（代理用户功能），只有Hadoop中的代理用户才能模拟其他用户的身份访问Hadoop集群。因此，需要将hiveserver2的启动用户设置为Hadoop的代理用户--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置所有节点的atguigu用户都可作为代理用户--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置atguigu用户能够代理的用户组为任意组--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置atguigu用户能够代理的用户为任意用户--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- # Hive --&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hiveserver2连接的host--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--指定hiveserver2连接的端口号--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>启动<br />
<code>bin/hive--servicehiveserver2</code></p>
<ol start="3">
<li>Hiveserver2使用
<ul>
<li>Hive提供的Beeline命令行客户端 <code>bin/beeline</code>,然后 <code>!connect jdbc:hive2://hadoop102:10000 </code>,输入用户名密码</li>
<li>Datagrip图形化客户端</li>
</ul>
</li>
</ol>
<h4 id="metastore服务配置"><a class="markdownIt-Anchor" href="#metastore服务配置"></a> MetaStore服务配置</h4>
<ol>
<li>MetaStore的服务模式</li>
</ol>
<p>Hive的metastore服务的作用是为HiveCLI或者Hiveserver2提供元数据访问接口</p>
<p>metastore有两种运行模式，分别为嵌入式模式和独立服务模式<br />
<img src="https://image.yayan.xyz/20240319145314.png" alt="嵌入式模式" /><br />
<img src="https://image.yayan.xyz/20240319145324.png" alt="独立服务模式" /></p>
<ul>
<li>嵌入式模式下，每个HiveCLI都需要直接连接元数据库，当HiveCLI较多时，数据库压力会比较大。</li>
<li>每个客户端都需要用户元数据库的读写权限，元数据库的安全得不到很好的保证</li>
</ul>
<ol start="2">
<li>MetaStore的嵌入服务模式配置</li>
</ol>
<ul>
<li>嵌入式模式下，只需保证Hiveserver2和每个HiveCLI的配置文件hive-site.xml中包含连接元数据库所需要的参数即可</li>
</ul>
<ol start="3">
<li>MetaStore的独立服务模式配置</li>
</ol>
<ul>
<li>保证metastore服务的配置文件hive-site.xml中包含连接元数据库所需的参数</li>
<li>保证Hiveserver2和每个HiveCLI的配置文件hive-site.xml中包含访问metastore服务所需的以下参数</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 主机名需要改为metastore服务所在节点，端口号无需修改，metastore服务的默认端口就是9083 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定metastore服务的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop102:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>启动<br />
<code>hive --service metastore</code></p>
<h3 id="hive操作"><a class="markdownIt-Anchor" href="#hive操作"></a> Hive操作</h3>
<ol>
<li>“-e”不进入hive的交互窗口执行hql语句</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive -e <span class="string">&quot;show databases&quot;</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>“-f”执行hql文件</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive -f /opt/module/hive/hive.sql</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>用户自定义配置会覆盖默认配置,Hive的配置会覆盖Hadoop的配置</li>
<li>可以使用命令行参数设置,但是仅对本次hive启动有效</li>
<li>可以在交互式中用 <code>set</code>设置,但是仅对本次hive会话有效</li>
</ol>
<h4 id="ddldata-definition-language"><a class="markdownIt-Anchor" href="#ddldata-definition-language"></a> DDL(Data Definition Language)</h4>
<h5 id="数据库操作"><a class="markdownIt-Anchor" href="#数据库操作"></a> 数据库操作</h5>
<p><strong>创建数据库</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建数据库</span></span><br><span class="line"><span class="keyword">CREATE</span> DATABASE [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name<span class="operator">=</span>property_value,...)];</span><br></pre></td></tr></table></figure>
<ul>
<li>IF NOT EXISTS:如果数据库不存在则创建</li>
<li>COMMENT:数据库的注释</li>
<li>LOCATION:数据库在HDFS上的存储路径</li>
<li>WITH DBPROPERTIES:数据库的属性</li>
</ul>
<p><strong>查询数据库</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看所有数据库</span></span><br><span class="line"><span class="keyword">SHOW</span> DATABASES [<span class="keyword">LIKE</span> <span class="string">&#x27;identifier_with_wildcards&#x27;</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看数据库的信息,EXTENDED表示显示详细信息</span></span><br><span class="line"><span class="keyword">DESCRIBE</span> DATABASE [EXTENDED] db_name;</span><br></pre></td></tr></table></figure>
<p><strong>修改数据库</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--修改dbproperties</span></span><br><span class="line"><span class="keyword">ALTER</span> DATABASE database_name <span class="keyword">SET</span> DBPROPERTIES (property_name<span class="operator">=</span>property_value, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">--修改location</span></span><br><span class="line"><span class="keyword">ALTER</span> DATABASE database_name <span class="keyword">SET</span> LOCATION hdfs_path;</span><br><span class="line"></span><br><span class="line"><span class="comment">--修改owner user</span></span><br><span class="line"><span class="keyword">ALTER</span> DATABASE database_name <span class="keyword">SET</span> OWNER <span class="keyword">USER</span> user_name;</span><br></pre></td></tr></table></figure>
<p><strong>删除数据库</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- RESTRICT：严格模式，若数据库不为空，则会删除失败，默认为该模式。</span></span><br><span class="line"><span class="comment">-- CASCADE：级联模式，若数据库不为空，则会将库中的表一并删除。</span></span><br><span class="line"><span class="keyword">DROP</span> DATABASE [IF <span class="keyword">EXISTS</span>] database_name [RESTRICT<span class="operator">|</span>CASCADE];</span><br></pre></td></tr></table></figure>
<p><strong>切换数据库</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USE database_name;</span><br></pre></td></tr></table></figure>
<h5 id="表操作"><a class="markdownIt-Anchor" href="#表操作"></a> 表操作</h5>
<p><strong>创建表</strong></p>
<p><strong>普通建表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [TEMPORARY] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name   </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">[COMMENT table_comment]</span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line">[<span class="type">ROW</span> FORMAT row_format] </span><br><span class="line">[STORED <span class="keyword">AS</span> file_format]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>TEMPORARY</code>:临时表,会话结束后自动删除</li>
<li><code>EXTERNAL</code>:外部表,表的数据不会被删除,只会删除元数据,对应的是内部表,Hive会自动管理内部表的数据</li>
<li><code>data_type</code>:数据类型(注意多了复杂类型:array,map,struct)</li>
<li>
<ul>
<li>补充两种类型转换:</li>
</ul>
</li>
<li>
<ul>
<li>小范围类型可以转为更广的范围类型</li>
</ul>
</li>
<li>
<ul>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/hive/languagemanual+types#LanguageManualTypes-AllowedImplicitConversions">隐式转换规则</a></li>
</ul>
</li>
<li>
<ul>
<li>显示转换需要使用cast函数 <code>cast(expr as &lt;type&gt;)</code></li>
</ul>
</li>
<li><code>PARTITIONED BY</code>:分区字段</li>
<li><code>CLUSTERED BY,...,INTO BUCKETS</code>:分桶字段</li>
<li><code>ROW FORMAT</code>:指定SERDE，SERDE是Serializer and Deserializer的简写。Hive使用SERDE序列化和反序列化每行数据。</li>
<li><code>STORED AS</code>:指定存储格式有，textfile（默认值），sequence file，orc file、parquet file等。</li>
<li><code>LOCATION</code>:指定表的存储路径若不指定路径，其默认值为 <code>$&#123;hive.metastore.warehouse.dir&#125;/db_name.db/table_name</code></li>
<li><code>TABLEPROPERTIES</code>:表的属性</li>
</ul>
<p><strong>ROW FORMAT说明</strong><br />
语法一：<br />
DELIMITED关键字表示对文件中的每个字段按照特定分割符进行分割，其会使用默认的SERDE对每行数据进行序列化和反序列化。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ROW</span> FORAMT DELIMITED </span><br><span class="line">[FIELDS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] </span><br><span class="line">[COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] </span><br><span class="line">[MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] </span><br><span class="line">[LINES TERMINATED <span class="keyword">BY</span> <span class="type">char</span>] </span><br><span class="line">[<span class="keyword">NULL</span> DEFINED <span class="keyword">AS</span> <span class="type">char</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>fields terminated by ：列分隔符。</li>
<li>collection items terminated by ： map、struct和array中每个元素之间的分隔符。</li>
<li>map keys terminated by ：map中的key与value的分隔符。</li>
<li>lines terminated by ：行分隔符。</li>
</ul>
<p>语法二：<br />
SERDE关键字可用于指定其他内置的SERDE或者用户自定义的SERDE。例如JSON SERDE，可用于处理JSON字符串。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ROW</span> FORMAT SERDE serde_name [<span class="keyword">WITH</span> SERDEPROPERTIES (property_name<span class="operator">=</span>property_value,property_name<span class="operator">=</span>property_value, ...)]</span><br></pre></td></tr></table></figure>
<p><strong>Create Table As Select(CTAS)创建表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [TEMPORARY] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[<span class="type">ROW</span> FORMAT row_format] </span><br><span class="line">[STORED <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure>
<p>该语法允许用户利用select查询语句返回的结果，直接建表，表的结构和查询语句的结构保持一致，且保证包含select查询语句放回的内容。</p>
<p><strong>Create Table Like创建表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [TEMPORARY] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name</span><br><span class="line">[<span class="keyword">LIKE</span> exist_table_name]</span><br><span class="line">[<span class="type">ROW</span> FORMAT row_format] </span><br><span class="line">[STORED <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]</span><br></pre></td></tr></table></figure>
<p>该语法允许用户复刻一张已经存在的表结构，与上述的CTAS语法不同，该语法创建出来的表中不包含数据。</p>
<p><strong>查看表</strong>:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看所有表</span></span><br><span class="line"><span class="keyword">SHOW</span> TABLES [<span class="keyword">IN</span> database_name] <span class="keyword">LIKE</span> [<span class="string">&#x27;identifier_with_wildcards&#x27;</span>]</span><br><span class="line"><span class="comment">-- 查看表详细信息</span></span><br><span class="line"><span class="keyword">DESCRIBE</span> [EXTENDED <span class="operator">|</span> FORMATTED] [db_name.]table_name</span><br></pre></td></tr></table></figure>
<p><strong>修改表</strong>:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--重命名表</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name RENAME <span class="keyword">TO</span> new_table_name;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 修改列信息</span></span><br><span class="line"><span class="comment">-- 增加列</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span> COLUMNS (col_name data_type [COMMENT col_comment], ...)</span><br><span class="line"><span class="comment">-- 更新列</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name CHANGE [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [COMMENT col_comment] [<span class="keyword">FIRST</span><span class="operator">|</span>AFTER column_name]</span><br><span class="line"><span class="comment">-- 替换列</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</span><br></pre></td></tr></table></figure>
<p><strong>删除表</strong>:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> [IF <span class="keyword">EXISTS</span>] table_name;</span><br></pre></td></tr></table></figure>
<p><strong>清空表</strong>:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 仅删除表中数据，保留表结构,不能删除外部表</span></span><br><span class="line"><span class="keyword">TRUNCATE</span> [<span class="keyword">TABLE</span>] table_name</span><br></pre></td></tr></table></figure>
<h4 id="dmldata-manipulation-language"><a class="markdownIt-Anchor" href="#dmldata-manipulation-language"></a> DML(Data Manipulation Language)</h4>
<p><strong>Load</strong><br />
Load语句可将文件导入到Hive表中</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA [<span class="keyword">LOCAL</span>] INPATH <span class="string">&#x27;filepath&#x27;</span> [OVERWRITE] <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...)];</span><br></pre></td></tr></table></figure>
<p>（1）local：表示从本地加载数据到Hive表；否则从HDFS加载数据到Hive表。<br />
（2）overwrite：表示覆盖表中已有数据，否则表示追加。<br />
（3）partition：表示上传到指定分区，若目标是分区表，需指定分区。</p>
<p><strong>Insert</strong></p>
<ol>
<li>将查询结果插入表中</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> (<span class="keyword">INTO</span> <span class="operator">|</span> OVERWRITE) <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...)] select_statement;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>将给定的值插入表中</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> (<span class="keyword">INTO</span> <span class="operator">|</span> OVERWRITE) <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1[<span class="operator">=</span>val1], partcol2[<span class="operator">=</span>val2] ...)] <span class="keyword">VALUES</span> values_row [, values_row ...]</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>将查询结果写入目标路径</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] DIRECTORY directory</span><br><span class="line">  [<span class="type">ROW</span> FORMAT row_format] [STORED <span class="keyword">AS</span> file_format] select_statement;</span><br></pre></td></tr></table></figure>
<p><strong>Export&amp;Import</strong><br />
Export导出语句可将表的数据和元数据信息一并到处的HDFS路径，Import可将Export导出的内容导入Hive，表的数据和元数据信息都会恢复。Export和Import可用于两个Hive实例之间的数据迁移。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--导出</span></span><br><span class="line">EXPORT <span class="keyword">TABLE</span> tablename <span class="keyword">TO</span> <span class="string">&#x27;export_target_path&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--导入</span></span><br><span class="line">IMPORT [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> new_or_original_tablename <span class="keyword">FROM</span> <span class="string">&#x27;source_path&#x27;</span> [LOCATION <span class="string">&#x27;import_target_path&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="hive查询语句"><a class="markdownIt-Anchor" href="#hive查询语句"></a> Hive查询语句</h3>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">官网说明</a></p>
<h4 id="基础语法"><a class="markdownIt-Anchor" href="#基础语法"></a> 基础语法</h4>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> <span class="operator">|</span> <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">  <span class="keyword">FROM</span> table_reference       <span class="comment">-- 从什么表查</span></span><br><span class="line">  [<span class="keyword">WHERE</span> where_condition]   <span class="comment">-- 过滤</span></span><br><span class="line">  [<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]        <span class="comment">-- 分组查询</span></span><br><span class="line">   [<span class="keyword">HAVING</span> col_list]          <span class="comment">-- 分组后过滤</span></span><br><span class="line">  [<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]        <span class="comment">-- 排序</span></span><br><span class="line">  [CLUSTER <span class="keyword">BY</span> col_list <span class="operator">|</span> [DISTRIBUTE <span class="keyword">BY</span> col_list] [SORT <span class="keyword">BY</span> col_list] ] <span class="comment">-- 分桶</span></span><br><span class="line"> [LIMIT number]                <span class="comment">-- 限制输出的行数</span></span><br></pre></td></tr></table></figure>
<h4 id="基础查询-分组查询"><a class="markdownIt-Anchor" href="#基础查询-分组查询"></a> 基础查询 &amp; 分组查询</h4>
<p>类似SQL</p>
<h4 id="join查询"><a class="markdownIt-Anchor" href="#join查询"></a> Join查询</h4>
<p>join连接的作用，是通过连接键将两个表的列组合起来，用于将数据库中的两个或多个表的记录合并起来。join连接可以将其他表的列添加至连接主表，将两个表合并为一个宽表.</p>
<p><strong>内连接</strong><br />
内连接中，只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p>
<ul>
<li>使用内连接时，inner join中的inner关键字可以省略。</li>
<li>表名也可以用子查询替代。</li>
<li>当使用等号连接时，就是等值连接，当使用不等号时，就是不等值连接。<br />
<img src="https://image.yayan.xyz/20240321160544.png" alt="" /></li>
</ul>
<p><strong>左外连接</strong></p>
<p><img src="https://image.yayan.xyz/20240321160753.png" alt="" /></p>
<p><strong>右外连接</strong><br />
<img src="https://image.yayan.xyz/20240321160832.png" alt="" /></p>
<p><strong>全外连接</strong><br />
<img src="https://image.yayan.xyz/20240321160854.png" alt="" /></p>
<p><strong>多表连接</strong><br />
oin除了可以实现2个表之间的连接外，还可以实现多表连接。</p>
<ul>
<li>需要注意的是，连接n个表，至少需要n-1个连接条件。</li>
<li>大多数情况下，Hive会对每对join的连接对象启动一个MapReduce任务</li>
<li>对于多表连接中的每个表，如果在on子句中使用相同的列组成连接条件， Hive 会将多个表的连接转换为单个MapReduce任务</li>
<li>在多表连接时，表的连接顺序和选用的连接类型都会影响到最终的结果集</li>
</ul>
<p><strong>笛卡尔积</strong><br />
Hive中提供了cross join关键字，用于实现笛卡尔积</p>
<ul>
<li>在hive.strict.checks.cartesian.product参数设置为true的严格模式下，以上语法是不能实现的，只有将该参数设置为false，以上语法才可以使用</li>
<li>在连接的每个 map/reduce 阶段，序列中的最后一个表会通过 reducer 进行流式传输，而其他表则会被缓冲。因此，通过合理安排join顺序，使得最大的表出现在序列的最后，有助于减少 reducer 中缓冲连接键的特定值的行所需的内存。如以下查询语句</li>
</ul>
<p><strong>联合(UNION)</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select_statement <span class="keyword">union</span> [<span class="keyword">all</span> <span class="operator">|</span> <span class="keyword">distinct</span>] select_statement <span class="keyword">union</span> [<span class="keyword">all</span> <span class="operator">|</span> <span class="keyword">distinct</span>] select_statement ...</span><br></pre></td></tr></table></figure>
<ul>
<li>union和union all都是将查询语句的查询结果上下联合，这点和join是有区别的，join是两表的左右连接，union和union all是上下拼接。</li>
<li>union关键字会对联合结果去重，union all不去重。</li>
<li>union和union all在上下拼接查询语句时要求，两个查询语句的结果，列的个数和名称必须相同，且上下对应列的类型必须一致。</li>
</ul>
<h4 id="排序查询"><a class="markdownIt-Anchor" href="#排序查询"></a> 排序查询</h4>
<p><strong>Order By</strong><br />
Order By：全局排序，只有一个 Reduce。</p>
<p><strong>Sort By</strong><br />
Sort By：每个 Reduce 内部排序(写入文件能看出来)</p>
<ul>
<li>对于大规模的数据集 order by 的效率非常低。在很多情况下，并不需要全局<br />
排序，此时可以使用 Sort by。</li>
<li>Sort by 为每个 reduce 产生一个排序文件。每个 Reduce 内部进行排序，对全局结果集<br />
来说不是排序</li>
</ul>
<p><strong>Distribute By</strong><br />
Distribute By：在有些情况下，我们需要控制某个特定行应该到哪个 Reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。distribute by 类似 MapReduce中 partition（<strong>自定义分区</strong>），进行分区，结合 sort by 使用。</p>
<ul>
<li>distribute by 的分区规则是根据分区字段的 hash 码与 reduce 的个数进行相除后，余数相同的分到一个区。</li>
<li>Hive 要求 distribute by 语句要写在 sort by 语句之前。</li>
</ul>
<p><strong>Cluster By</strong><br />
当 distribute by 和 sort by 字段相同时，可以使用 cluster by 方式。<br />
cluster by 除了具有 distribute by 的功能外还兼具 sort by 的功能。但是排序只能是升序排序，不能指定排序规则为 asc 或者 desc。</p>
<p><img src="https://image.yayan.xyz/20240321162116.png" alt="" /></p>
<h3 id="函数"><a class="markdownIt-Anchor" href="#函数"></a> 函数</h3>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看内置函数</span></span><br><span class="line"><span class="keyword">show</span> functions</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看函数的详细信息</span></span><br><span class="line"><span class="keyword">desc</span> <span class="keyword">function</span> extended function_name</span><br></pre></td></tr></table></figure>
<h4 id="单行函数"><a class="markdownIt-Anchor" href="#单行函数"></a> 单行函数</h4>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数值函数</span></span><br><span class="line"><span class="comment">-- abs,round,ceil,floor,rand,exp,log,log10,pow,sqrt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 字符串函数</span></span><br><span class="line"><span class="comment">-- concat,substring,substr....</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 日期函数</span></span><br><span class="line">unix_timestamp：返回当前或指定时间的时间戳</span><br><span class="line"></span><br><span class="line">from_unixtime：转化 UNIX 时间戳到当前时区的时间格式</span><br><span class="line"></span><br><span class="line"><span class="built_in">current_date</span>：当前日期</span><br><span class="line"></span><br><span class="line"><span class="built_in">current_timestamp</span>：当前的日期加时间，并且精确的毫秒</span><br><span class="line"></span><br><span class="line"><span class="keyword">month</span>：获取日期中的月....</span><br><span class="line"></span><br><span class="line">datediff：两个日期相差的天数（结束日期减去开始日期的天数）</span><br><span class="line"></span><br><span class="line">date_format:将标准日期解析成指定格式字符串</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 流程控制函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 1.nvl(A,B):若A的值不为null，则返回A，否则返回B。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2.case when</span></span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> a <span class="keyword">then</span> b [<span class="keyword">when</span> c <span class="keyword">then</span> d]<span class="operator">*</span> [<span class="keyword">else</span> e] <span class="keyword">end</span></span><br><span class="line"><span class="keyword">case</span> a <span class="keyword">when</span> b <span class="keyword">then</span> c [<span class="keyword">when</span> d <span class="keyword">then</span> e]<span class="operator">*</span> [<span class="keyword">else</span> f] <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3.if</span></span><br><span class="line">if（<span class="type">boolean</span> testCondition, T valueTrue, T valueFalseOrNull）</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 4.coalesce(A,B,C):返回参数列表中第一个不为null的值。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 集合函数</span></span><br><span class="line"><span class="comment">-- array(array1, array2, ...)：将多个数组合并成一个数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- array_contains(array, value)：判断array集合中是否包含某个值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- sort_array(array)：对数组进行排序</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- size(array)：返回数组的长度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- map(key1, value1, key2, value2, …)：创建一个map</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- map_keys(map)：返回map中的所有key</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- map_values(map)：返回map中的所有value</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- struct(col1, col2, …)：创建一个struct</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- named_struct(name1, val1, name2, val2, …)：创建一个具有指定名称的struct</span></span><br></pre></td></tr></table></figure>
<h4 id="聚合函数"><a class="markdownIt-Anchor" href="#聚合函数"></a> 聚合函数</h4>
<p><strong>普通聚合</strong><br />
<code>sum(),avg(),count(),max(),min(),...</code></p>
<p><strong>高级聚合</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">collect_set(col)：返回一个集合，该集合包含了所有的groupby组内不重复的值</span><br><span class="line"></span><br><span class="line">collect_list(col)：返回一个列表，该列表包含了所有的groupby组内的值</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="炸裂udtf函数"><a class="markdownIt-Anchor" href="#炸裂udtf函数"></a> 炸裂(UDTF)函数</h4>
<p>UDTF的全称是User Defined Table-Generation Function，即用户定义的表生成函数。简单理解，UDTF就是接收一行数据，输出一行或者多行数据。系统内置的常用的UDTF有explode、posexplode、inline等</p>
<p><strong>explode</strong><br />
<img src="https://image.yayan.xyz/20240321164946.png" alt="explode(array a)" /><br />
<img src="https://image.yayan.xyz/20240321165013.png" alt="explode(map&lt;K,V&gt; m)" /></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 语法一：explode(array&lt;T&gt; a)</span></span><br><span class="line"><span class="comment">-- 说明：传入参数为array数组类型，返回一行或多行结果，每行对应array数组中的一个元素。</span></span><br><span class="line"><span class="comment">-- 语法二：explode(map&lt;K,V&gt; m)</span></span><br><span class="line"><span class="comment">-- 说明：传入参数为map类型，由于map是key-value结构的，所以explode函数会将map参数转换为两列，一列是key，一列是value。</span></span><br></pre></td></tr></table></figure>
<p><strong>posexplode</strong><br />
<img src="https://image.yayan.xyz/20240321165111.png" alt="" /></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- posexplode(array&lt;T&gt; a)。</span></span><br><span class="line"><span class="comment">-- 说明：posexplode函数的用法与explode函数相似，增加了pos前缀，表明在返回array数组的每一个元素的同时，还会返回元素在数据所处的位置。</span></span><br></pre></td></tr></table></figure>
<p><strong>inline</strong><br />
<img src="https://image.yayan.xyz/20240321165134.png" alt="" /></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- inline(array&lt;struct&lt;f1:T1,...,fn:Tn&gt;&gt; a)</span></span><br><span class="line"><span class="comment">-- 说明：inline函数接受的参数结构体数组，其可将数组中的每个结构体输出为一行，每个结构体中的列，会展开为一个个单独的列。</span></span><br></pre></td></tr></table></figure>
<p><strong>lateral view</strong><br />
<img src="https://image.yayan.xyz/20240321165440.png" alt="" /></p>
<ul>
<li>
<p>UDTF函数可以将一行数据转换为多行，出现在select语句中时，不能与其他列同时出现，会报如下所示错误信息。</p>
</li>
<li>
<p>lateral view可以将UDTF应用到原表的每行数据，将每行数据转换为一行或多行，并将源表中每行的输出结果与该行连接起来，形成一个虚拟表。</p>
</li>
<li>
<p>lateral view一般在from子句后使用，紧跟在UDTF后面的是虚拟表的别名，虚拟表别名不可省略。as关键字后为执行UDTF后的列的别名，UDTF函数生成几列就要给出几个列别名，多个列别名间使用逗号分隔</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	col1 [,col2,col3……] </span><br><span class="line"><span class="keyword">from</span> 表名 </span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> udtf(expression) 虚拟表别名 <span class="keyword">as</span> col1 [,col2,col3……]</span><br></pre></td></tr></table></figure>
<h4 id="窗口函数开窗函数"><a class="markdownIt-Anchor" href="#窗口函数开窗函数"></a> 窗口函数(开窗函数)</h4>
<ul>
<li>窗口函数能够为每行数据划分一个窗口，然后对窗口范围内的数据进行计算，最后将计算结果返回给该行数据。(类似pandas的rolling)</li>
</ul>
<p><strong>函数</strong></p>
<ul>
<li>每个窗口中的计算逻辑，都是多（行）进一（行）出，因此绝大多数的聚合函数都可以配合窗口使用</li>
</ul>
<p><strong>窗口</strong></p>
<ul>
<li>窗口范围的定义分为两种类型，一种是基于行进行定义，一种是基于值进行定义。它们都用来确定一个窗口中应该包含哪些行，但是确定的逻辑有所不同。</li>
<li>
<ul>
<li>基于行的窗口范围定义，是通过行数的偏移量，来确定窗口范围，例如：某行的窗口范围可以包含当前行的前一行到当前行。</li>
</ul>
</li>
<li>
<ul>
<li>基于值的窗口范围定义，是通过某个列值的偏移量，来确定窗口范围，例如：若某行A列的值为10，其窗口范围可以包含，A列值大于等于10-1且小于等于10的所有行。</li>
</ul>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 使用方法</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    col_1, </span><br><span class="line">    col_2, </span><br><span class="line">    col_3, </span><br><span class="line">    函数(col_1) <span class="keyword">over</span> (窗口范围) <span class="keyword">as</span> 别名</span><br><span class="line"><span class="keyword">from</span> table_name;</span><br></pre></td></tr></table></figure>
<p><img src="https://image.yayan.xyz/20240321170245.png" alt="" /></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 基于行</span></span><br><span class="line"><span class="built_in">sum</span>(amount) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> <span class="operator">&lt;</span><span class="keyword">column</span><span class="operator">&gt;</span> <span class="keyword">rows</span> <span class="keyword">between</span> <span class="operator">&lt;</span><span class="keyword">start</span><span class="operator">&gt;</span> <span class="keyword">and</span> <span class="operator">&lt;</span><span class="keyword">end</span><span class="operator">&gt;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 基于值</span></span><br><span class="line"><span class="built_in">sum</span>(amount) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> <span class="operator">&lt;</span><span class="keyword">column</span><span class="operator">&gt;</span> <span class="keyword">range</span> <span class="keyword">between</span> <span class="operator">&lt;</span><span class="keyword">start</span><span class="operator">&gt;</span> <span class="keyword">and</span> <span class="operator">&lt;</span><span class="keyword">end</span><span class="operator">&gt;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 窗口起点不能超过终点</span></span><br><span class="line"><span class="comment">-- unbounded preceding：窗口范围的开始位置是无限制的，即从第一行开始</span></span><br><span class="line"><span class="comment">-- unbounded following：窗口范围的结束位置是无限制的，即到最后一行结束</span></span><br><span class="line"><span class="comment">-- [num] preceddubg：窗口范围的开始位置是当前行的前num行</span></span><br><span class="line"><span class="comment">-- [num] following：窗口范围的结束位置是当前行的后num行</span></span><br><span class="line"><span class="comment">-- current row:  本行</span></span><br></pre></td></tr></table></figure>
<p><strong>分区</strong><br />
<img src="https://image.yayan.xyz/20240321171748.png" alt="" /></p>
<ul>
<li>定义窗口范围时，还可以使用partition by关键字指定分区列，将每个分区单独划分为窗口。</li>
<li>每个分区内独立计算。</li>
</ul>
<p><strong>缺省</strong><br />
over后可以使用的窗口划分语句,都可以省略不写,包括:</p>
<ul>
<li>partition by</li>
<li>order by</li>
<li>(rows|range) between … and …。</li>
</ul>
<ol>
<li>①partition by省略不写，表示不分区。在不进行分区的情况下，将会把整张表的全部内容作为窗口进行划分。</li>
<li>②order by 省略不写，表示不排序。</li>
<li>③(rows|range) between … and … 省略不写，则使用其默认值，默认值分以下两种情况。
<ul>
<li>若over()中包含order by，则默认值为range between unbounded preceding and current row。</li>
<li>若over()中不包含order by，则默认值为rows between unbounded preceding and unbounded following。</li>
</ul>
</li>
</ol>
<h4 id="常用窗口函数"><a class="markdownIt-Anchor" href="#常用窗口函数"></a> 常用窗口函数</h4>
<ol>
<li>聚合函数:<code>sum(),avg(),count(),max(),min()</code></li>
<li>跨行取值函数
<ul>
<li><code>lead(col, n, default)</code>: 用于获取窗口内当前行往下第n行的值。</li>
<li><code>lag(col, n, default)</code>: 用于获取窗口内当前行往上第n行的值。</li>
<li>lag和lead函数不支持使用rows between和range between的自定义窗口。</li>
<li><code>first_value (col, boolean)</code>:取分组内排序后，截止到当前行的第一个值。</li>
<li><code>last_value (col, boolean)</code>:取分组内排序后，截止到当前行的最后一个值。</li>
<li>第二个参数说明是否跳过null值，可不写。</li>
</ul>
</li>
<li>排名函数:<code>rank()/dense_rank()/row_number()</code>
<ul>
<li>排名函数会对窗口范围内的数据按照order by后的列进行排名。</li>
<li>rank 、dense_rank、row_number不支持自定义窗口<br />
|score|rank|dense_rank|row_number|<br />
|—|—|—|—|<br />
|90|1|1|1|<br />
|90|1|1|2|<br />
|80|3|2|3|<br />
|80|3|2|4|<br />
|70|5|3|5|</li>
</ul>
</li>
</ol>
<h4 id="自定义函数"><a class="markdownIt-Anchor" href="#自定义函数"></a> 自定义函数</h4>
<p>根据用户自定义函数类别分为以下三种：<br />
（1）UDF（User-Defined-Function）<br />
一进一出。<br />
（2）UDAF（User-Defined Aggregation Function）<br />
用户自定义聚合函数，多进一出。<br />
类似于：count/max/min<br />
（3）UDTF（User-Defined Table-Generating Functions）<br />
用户自定义表生成函数，一进多出。<br />
如lateral view explode()</p>
<p>编程步骤:</p>
<ol>
<li>继承Hive提供的类
<ul>
<li>UDF：org.apache.hadoop.hive.ql.udf.generic.GenericUDF</li>
<li>UDAF：org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver</li>
<li>UDTF：org.apache.hadoop.hive.ql.udf.generic.GenericUDTF</li>
</ul>
</li>
<li>实现类中的抽象方法</li>
<li>在hive的命令行窗口中创建函数
<ul>
<li>临时函数:</li>
<li>
<ul>
<li>add jar xxx.jar</li>
</ul>
</li>
<li>
<ul>
<li>create temporary function xxxxx as ‘xxx’</li>
</ul>
</li>
<li>
<ul>
<li>select xxxxx(col) from table</li>
</ul>
</li>
<li>
<ul>
<li>drop temporary function xxxxx;</li>
</ul>
</li>
<li>
<ul>
<li>临时函数只跟会话有关系，跟库没有关系。只要创建临时函数的会话不断，在当前会话下，任意一个库都可以使用，其他会话全都不能使用。</li>
</ul>
</li>
<li>永久函数:</li>
<li>
<ul>
<li>上传jar到HDFS</li>
</ul>
</li>
<li>
<ul>
<li>create function xxxxx as ‘xxx’ using jar ‘hdfs://hadoop102:9000/xxx.jar’</li>
</ul>
</li>
<li>
<ul>
<li>select xxxxx(col) from table</li>
</ul>
</li>
<li>
<ul>
<li>drop function xxxxx</li>
</ul>
</li>
<li>
<ul>
<li>永久函数跟会话没有关系，创建函数的会话断了以后，其他会话也可以使用。</li>
</ul>
</li>
<li>
<ul>
<li>永久函数创建的时候，在函数名之前需要自己加上库名，如果不指定库名的话，会默认把当前库的库名给加上。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="自定义udf函数"><a class="markdownIt-Anchor" href="#自定义udf函数"></a> 自定义UDF函数</h4>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// HQL转为MapReduce任务时,会生成Operator Tree(每个Operator都是一个小动作:扫描,select等)</span></span><br><span class="line"><span class="comment">// 数据会逐个经过operator,数据和数据元信息是分开传递的</span></span><br><span class="line"><span class="comment">// ObjectInspector是数据的元信息,DeferredObject是数据</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> ObjectInspector <span class="title function_">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException &#123;</span><br><span class="line">   <span class="comment">// 接受上一环节的数据,返回这个函数处理完之后数据的元信息</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Object <span class="title function_">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException &#123;</span><br><span class="line">   <span class="comment">// 接受处理数据</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> String <span class="title function_">getDisplayString</span><span class="params">(String[] children)</span> &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="分区表和分桶表"><a class="markdownIt-Anchor" href="#分区表和分桶表"></a> 分区表和分桶表</h3>
<h4 id="分区"><a class="markdownIt-Anchor" href="#分区"></a> 分区</h4>
<ul>
<li>通过使用partitionedby子句可以创建分区表，partitionedby后面是分区字段，</li>
<li>一个表可以有一个或多个分区字段</li>
<li>Hive可以为分区字段的每个不同的字段组合创建一个单独的数据目录(文件夹)</li>
<li>当用户通过where子句选择要查询的分区后，就不会查询其他分区的数据</li>
<li>分区字段并不是表中的数据，是伪列，可以当作列用</li>
</ul>
<h5 id="分区表基本操作"><a class="markdownIt-Anchor" href="#分区表基本操作"></a> 分区表基本操作</h5>
<p><strong>新建分区表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建分区表</span></span><br><span class="line">hive(<span class="keyword">default</span>) <span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> dept_partition (</span><br><span class="line">    deptno <span class="type">int</span>,</span><br><span class="line">    dname string,</span><br><span class="line">    loc string</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> string)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"><span class="comment">-- 从文件中加载</span></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive/datas/dept_20220401.log&#x27;</span> </span><br><span class="line"><span class="keyword">into</span> <span class="keyword">table</span> dept_partition </span><br><span class="line"><span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20220401&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dept_partition <span class="keyword">partition</span> (<span class="keyword">day</span> <span class="operator">=</span> <span class="string">&#x27;20220402&#x27;</span>)</span><br><span class="line"><span class="keyword">select</span> deptno, dname, loc</span><br><span class="line"><span class="keyword">from</span> dept_partition</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">day</span> <span class="operator">=</span> <span class="string">&#x27;2020-04-01&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 读取数据</span></span><br><span class="line"><span class="keyword">select</span> deptno, dname, loc ,<span class="keyword">day</span></span><br><span class="line"><span class="keyword">from</span> dept_partition</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">day</span> <span class="operator">=</span> <span class="string">&#x27;2020-04-01&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p><strong>查看分区表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> partitions dept_partition;</span><br></pre></td></tr></table></figure>
<p><strong>增加分区</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 中间无逗号</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition </span><br><span class="line"><span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20220404&#x27;</span>) <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20220405&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><strong>删除分区</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 中间有逗号</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition </span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20220404&#x27;</span>), <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20220405&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><strong>修复分区</strong></p>
<ul>
<li>Hive将分区表的所有分区信息都保存在了元数据中，只有元数据与HDFS上的分区路径一致时，分区表才能正常读写数据。</li>
<li>若用户在HDFS上手动创建/删除分区路径，Hive都是感知不到的，这样就会导致Hive的元数据和HDFS的分区路径不一致。</li>
<li>若分区表为外部表，用户执行drop partition命令后，分区元数据会被删除，而HDFS的分区路径不会被删除同样会导致Hive的元数据和HDFS的分区路径不一致。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 若手动创建HDFS的分区路径，Hive无法识别，可通过add partition命令增加分区元数据信息，从而使元数据和分区路径保持一致。</span></span><br><span class="line"><span class="keyword">add</span> <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20220404&#x27;</span>) location <span class="string">&#x27;/opt/module/hive/datas/dept_partition/day=20220404&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 若手动删除HDFS的分区路径，Hive无法识别，可通过drop partition命令删除分区元数据信息，从而使元数据和分区路径保持一致。</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20220404&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 若分区元数据和HDFS的分区路径不一致，还可使用msck命令进行修复，</span></span><br><span class="line">msck repair <span class="keyword">table</span> table_name [<span class="keyword">add</span><span class="operator">/</span><span class="keyword">drop</span><span class="operator">/</span>sync partitions];</span><br><span class="line"><span class="comment">-- msck repair table table_name add partitions：该命令会增加HDFS路径存在但元数据缺失的分区信息。</span></span><br><span class="line"><span class="comment">-- msck repair table table_name drop partitions：该命令会删除HDFS路径已经删除但元数据仍然存在的分区信息。</span></span><br><span class="line"><span class="comment">-- msck repair table table_name sync partitions：该命令会同步HDFS路径和元数据分区信息，相当于同时执行上述的两个命令。</span></span><br><span class="line"><span class="comment">-- msck repair table table_name：等价于msck repair table table_name add partitions命令。</span></span><br></pre></td></tr></table></figure>
<h5 id="二级分区"><a class="markdownIt-Anchor" href="#二级分区"></a> 二级分区</h5>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 和一级分区类似，只是多了一个分区字段</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition2(</span><br><span class="line">    deptno <span class="type">int</span>,    <span class="comment">-- 部门编号</span></span><br><span class="line">    dname string, <span class="comment">-- 部门名称</span></span><br><span class="line">    loc string     <span class="comment">-- 部门位置</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> string, <span class="keyword">hour</span> string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h5 id="动态分区"><a class="markdownIt-Anchor" href="#动态分区"></a> 动态分区</h5>
<ul>
<li>插入数据指定分区很麻烦</li>
<li>动态分区是指向分区表insert数据时，被写往的分区不由用户指定，而是由每行数据的最后一个字段的值来动态的决定。</li>
<li>使用动态分区，可只用一个insert语句将数据写入多个分区。</li>
<li>插入语句的最后一个字段作为分区的字段，不需要指定分区字段，Hive会自动识别。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启动态分区</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition<span class="operator">=</span><span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 动态分区的模式，默认strict（严格模式），要求必须指定至少一个分区为静态分区，nonstrict（非严格模式）允许所有的分区字段都使用动态分区。</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict</span><br><span class="line"><span class="comment">-- 一条insert语句可同时创建的最大的分区个数，默认为1000。</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions<span class="operator">=</span><span class="number">1000</span></span><br><span class="line"><span class="comment">-- 单个Mapper或者Reducer可同时创建的最大的分区个数，默认为100。</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions.pernode<span class="operator">=</span><span class="number">100</span></span><br><span class="line"><span class="comment">-- 一条insert语句可以创建的最大的文件个数，默认100000。</span></span><br><span class="line">hive.exec.max.created.files<span class="operator">=</span><span class="number">100000</span></span><br><span class="line"><span class="comment">-- 当查询结果为空时且进行动态分区时，是否抛出异常，默认false。</span></span><br><span class="line">hive.error.on.empty.partition<span class="operator">=</span><span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition_dynamic(</span><br><span class="line">    id <span class="type">int</span>,</span><br><span class="line">    name string</span><br><span class="line">) </span><br><span class="line">partitioned <span class="keyword">by</span> (loc <span class="type">int</span>) </span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入，根据loc动态分区</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_partition_dynamic </span><br><span class="line"><span class="keyword">partition</span>(loc) </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    deptno, </span><br><span class="line">    dname, </span><br><span class="line">    loc </span><br><span class="line"><span class="keyword">from</span> dept;</span><br><span class="line"><span class="comment">-- 因为dept_partition_dynamic表中只有两个字段，所以当我们查询了三个字段时（多了loc字段），所以系统默认以最后一个字段city为分区名，</span></span><br><span class="line"><span class="comment">-- 因为分区表的分区字段默认也是该表中的字段，且依次排在表中字段的最后面。所以分区需要分区的字段只能放在后面，不能把顺序弄错。</span></span><br><span class="line"><span class="comment">-- 如果我们查询了四个字段的话，则会报错，因为该表加上分区字段也才三个。要注意系统是根据查询字段的位置推断分区名的，而不是字段名称。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 多个分区字段，实现半自动（部分字段静态分区，注意静态分区字段要在动态前面）</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 1.创建一个只有一个字段，两个分区字段的分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ds_parttion(id <span class="type">int</span> ) </span><br><span class="line">partitioned <span class="keyword">by</span> (state string ,ct string );</span><br><span class="line"><span class="comment">-- 2.往该分区表半动态分区插入数据 </span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> ds_parttion</span><br><span class="line"><span class="keyword">partition</span>(state<span class="operator">=</span><span class="string">&#x27;china&#x27;</span>,ct)  #state分区为静态，ct为动态分区，以查询的city字段为分区名</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 多个分区字段时，全部实现动态分区插入数据</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> ds_parttion</span><br><span class="line"><span class="keyword">partition</span>(state,ct)</span><br><span class="line"><span class="keyword">select</span> id ,country,city <span class="keyword">from</span>  mytest_tmp2_p;</span><br></pre></td></tr></table></figure>
<h4 id="分桶"><a class="markdownIt-Anchor" href="#分桶"></a> 分桶</h4>
<ul>
<li>
<p>分区提供一个隔离数据和优化查询的便利方式</p>
</li>
<li>
<p>对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分</p>
</li>
<li>
<p>分区针对的是数据的存储路径，分桶针对的是数据文件。</p>
</li>
<li>
<p>分桶表的基本原理是，首先为每行数据计算一个指定字段的数据的hash值，然后模以一个指定的分桶数，最后将取模运算结果相同的行，写入同一个文件中，这个文件就称为一个分桶（bucket）。</p>
</li>
</ul>
<h5 id="基本语法"><a class="markdownIt-Anchor" href="#基本语法"></a> 基本语法</h5>
<p><strong>创建分桶表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- cluster by后面指定分桶字段，into后面指定分桶数</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(</span><br><span class="line">    id <span class="type">int</span>, </span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(id) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>表中的文件数据会被分为四个桶，对应四个文件，每个文件中的数据都是根据id字段的hash值模4的结果相同的数据。</li>
</ul>
<h5 id="分桶排序表"><a class="markdownIt-Anchor" href="#分桶排序表"></a> 分桶排序表</h5>
<ul>
<li>Hive的分桶排序表是一种优化技术，用于提高大数据存储和查询的效率。它将数据表按照指定的列进行分桶（bucket），每个桶内的数据再按照指定的列进行排序，这样就可以在查询时快速定位到需要的数据，减少数据扫描的时间。</li>
<li>使用分桶排序表的主要优点是可以提高查询效率，特别是在大数据量的情况下。相比于无序表，分桶排序表在查询时可以跳过不需要的数据，减少数据扫描的时间。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- clustered by关键字指定按照哪个列进行分桶，sorted by关键字指定在每个桶内按照哪个列进行排序。</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck_sort(</span><br><span class="line">    id <span class="type">int</span>, </span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(id) sorted <span class="keyword">by</span>(id)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="文件格式和压缩"><a class="markdownIt-Anchor" href="#文件格式和压缩"></a> 文件格式和压缩</h3>
<h4 id="文件格式"><a class="markdownIt-Anchor" href="#文件格式"></a> 文件格式</h4>
<ul>
<li>在创建表时，使用关键字<code>stored as 文件格式[textfile|sequencefile|orc|parquet]</code>指定文件格式。</li>
<li>使用列式存储格式（orc和parquet）的查询性能和存储效率都要优于默认的文本文件格式，其中orc的性能略微优于parquet。</li>
</ul>
<h5 id="textfile"><a class="markdownIt-Anchor" href="#textfile"></a> TextFile</h5>
<ul>
<li>文本文件是Hive默认使用的文件格式，文本文件中的一行内容，就对应Hive表中的一行记录。</li>
</ul>
<h5 id="orc"><a class="markdownIt-Anchor" href="#orc"></a> ORC</h5>
<ul>
<li>
<p>ORC是一种列式存储的文件格式，ORC文件能够提高Hive读写数据和处理数据的性能</p>
</li>
<li>
<p>列式存储(操作系统里的文件排列方式)<br />
<img src="https://image.yayan.xyz/20240322144039.png" alt="" /></p>
</li>
<li>
<p>每个Orc文件由Header、Body和Tail三部分组成。</p>
</li>
<li>
<p>Body由1个或多个stripe组成(HRegion)，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，每个stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer。</p>
</li>
<li>
<ul>
<li>Index Data：一个轻量级的index，默认是为各列每隔1W行做一个索引。每个索引会记录第n万行的位置，和最近一万行的最大值和最小值等信息。</li>
</ul>
</li>
<li>
<ul>
<li>Row Data：存的是具体的数据，按列进行存储，并对每个列进行编码，分成多个Stream来存储。</li>
</ul>
</li>
<li>
<ul>
<li>Stripe Footer：存放的是各个Stream的位置以及各column的编码信息</li>
</ul>
</li>
<li>
<p>Tail由File Footer和PostScript组成。</p>
</li>
<li>
<ul>
<li>File Footer中保存了各Stripe的其实位置、索引长度、数据长度等信息，各Column的统计信息等；</li>
</ul>
</li>
<li>
<ul>
<li>PostScript记录了整个文件的压缩类型以及File Footer的长度信息等。</li>
</ul>
</li>
<li>
<p>在读取ORC文件时，会先从最后一个字节读取PostScript长度，进而读取到PostScript，从里面解析到File Footer长度，进而读取FileFooter，从中解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p>
</li>
</ul>
<p><strong>建表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orc_table</span><br><span class="line">(column_specs)</span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties (property_name<span class="operator">=</span>property_value, ...);</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>orc.compress</td>
<td>ZLIB</td>
<td>压缩格式，可选项：NONE、ZLIB,、SNAPPY</td>
</tr>
<tr>
<td>orc.compress.size</td>
<td>262144</td>
<td>每个压缩块的大小（ORC文件是分块压缩的）</td>
</tr>
<tr>
<td>orc.stripe.size</td>
<td>67108864</td>
<td>每个stripe的大小</td>
</tr>
<tr>
<td>orc.row.index.stride</td>
<td>10000</td>
<td>索引步长（每隔多少行数据建一条索引）</td>
</tr>
</tbody>
</table>
<h5 id="parquet"><a class="markdownIt-Anchor" href="#parquet"></a> Parquet</h5>
<ul>
<li>
<p>Parquet也是一个列式存储的文件格式。<br />
<img src="https://image.yayan.xyz/20240322144651.png" alt="" /></p>
</li>
<li>
<p>文件的首尾都是该文件的Magic Code，用于校验它是否是一个Parquet文件。</p>
</li>
<li>
<p>首尾中间由若干个Row Group和一个Footer（File Meta Data）组成。</p>
</li>
<li>
<p>每个Row Group包含多个Column Chunk，每个Column Chunk包含多个Page。</p>
</li>
<li>
<ul>
<li>行组（Row Group）：一个行组对应逻辑表中的若干行。</li>
</ul>
</li>
<li>
<ul>
<li>列块（Column Chunk）：一个行组中的一列保存在一个列块中。</li>
</ul>
</li>
<li>
<ul>
<li>页（Page）：一个列块的数据会划分为若干个页。</li>
</ul>
</li>
<li>
<p>Footer（File Meta Data）中存储了每个行组中的每个列快的元数据信息，元数据信息包含了该列的数据类型、该列的编码方式、该类的Data Page位置等信息。</p>
</li>
</ul>
<p><strong>建表</strong><br />
建表语句和ORC类似</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>parquet.compression</td>
<td>uncompressed</td>
<td>压缩格式，可选项：uncompressed，snappy，gzip，lzo，lz4</td>
</tr>
<tr>
<td>parquet.block.size</td>
<td>134217728</td>
<td>行组大小，通常与HDFS块大小保持一致</td>
</tr>
<tr>
<td>parquet.page.size</td>
<td>1048576</td>
<td>页大小</td>
</tr>
</tbody>
</table>
<h4 id="压缩"><a class="markdownIt-Anchor" href="#压缩"></a> 压缩</h4>
<ul>
<li>在Hive表中和计算过程中，保持数据的压缩，对磁盘空间的有效利用和提高查询性能都是十分有益的。</li>
</ul>
<p><strong>Hadoop中的压缩格式</strong></p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
</tr>
</thead>
<tbody>
<tr>
<td>DEFLATE</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
</tr>
<tr>
<td>Snappy</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody>
<tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody>
</table>
<h5 id="对hive数据表进行压缩"><a class="markdownIt-Anchor" href="#对hive数据表进行压缩"></a> 对Hive数据表进行压缩</h5>
<p><strong>TextFile</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">-</span> 若一张表的文件类型为TextFile，若需要对该表中的数据进行压缩，多数情况下，无需在建表语句做出声明。直接将压缩后的文件导入到该表即可，Hive在查询表中数据时，可自动识别其压缩格式，进行解压。</span><br><span class="line"><span class="operator">-</span> 需要注意的是，在执行往表中导入数据的<span class="keyword">SQL</span>语句时，用户需设置以下参数，来保证写入表中的数据是被压缩的。</span><br><span class="line"><span class="comment">--SQL语句的最终输出结果是否压缩</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">--输出结果的压缩格式（以下示例为snappy）</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec<span class="operator">=</span>org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>
<p><strong>ORC和Parquet</strong><br />
若一张表的文件类型为ORC/Parquet，若需要对该表数据进行压缩，需在建表语句中声明压缩格式如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orc_table</span><br><span class="line">(column_specs)</span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties (&quot;orc.compress&quot;<span class="operator">=</span>&quot;snappy&quot;);</span><br><span class="line"><span class="operator">/</span></span><br><span class="line">stored <span class="keyword">as</span> parquet</span><br><span class="line">tblproperties (&quot;parquet.compression&quot;<span class="operator">=</span>&quot;snappy&quot;);</span><br></pre></td></tr></table></figure>
<h5 id="计算过程中使用压缩"><a class="markdownIt-Anchor" href="#计算过程中使用压缩"></a> 计算过程中使用压缩</h5>
<p><strong>单个MR的中间结果压缩</strong><br />
单个MR的中间结果是指Mapper输出的数据，对其进行压缩可降低shuffle阶段的网络IO，</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启MapReduce中间数据压缩功能</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">--设置MapReduce中间数据数据的压缩方式（以下示例为snappy）</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress.codec<span class="operator">=</span>org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>
<p><strong>单条SQL的中间结果压缩</strong></p>
<p>单条SQL语句的中间结果是指，两个MR（一条SQL语句可能需要通过MR进行计算）之间的临时数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--是否对两个MR之间的临时数据进行压缩</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.intermediate<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">--压缩格式（以下示例为snappy）</span></span><br><span class="line"><span class="keyword">set</span> hive.intermediate.compression.codec<span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>
<h3 id="企业级调优"><a class="markdownIt-Anchor" href="#企业级调优"></a> ★企业级调优</h3>
<h4 id="yarn资源配置"><a class="markdownIt-Anchor" href="#yarn资源配置"></a> Yarn资源配置</h4>
<ul>
<li>YARN的内存调优的相关参数可以在yarn-site.xml文件中修改，需要调整的YARN参数均与CPU、内存等资源有关</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- （1）yarn.nodemanager.resource.memory-mb --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该参数的含义是，一个NodeManager节点分配给Container使用的内存。该参数的配置，取决于NodeManager所在节点的总内存容量和该节点运行的其他服务的数量。 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 考虑上述因素，本书所搭建集群的服务器的内存资源为64GB，且未运行其他服务，此处可将该参数设置为64G，如下： --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>65536<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- （2）yarn.nodemanager.resource.cpu-vcores</span></span><br><span class="line"><span class="comment">该参数的含义是，一个NodeManager节点分配给Container使用的CPU核数。该参数的配置，同样取决于NodeManager所在节点的总CPU核数和该节点运行的其他服务。</span></span><br><span class="line"><span class="comment">考虑上述因素，本书所搭建集群的服务器的CPU核数为16，且未运行其他服务，此处可将该参数设置为16。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- （3）yarn.scheduler.maximum-allocation-mb</span></span><br><span class="line"><span class="comment">该参数的含义是，单个Container能够使用的最大内存。推荐配置如下： --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16384<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- （4）yarn.scheduler.minimum-allocation-mb</span></span><br><span class="line"><span class="comment">该参数的含义是，单个Container能够使用的最小内存，推荐配置如下： --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="mapreduce资源配置"><a class="markdownIt-Anchor" href="#mapreduce资源配置"></a> MapReduce资源配置</h4>
<ul>
<li>MapReduce资源配置主要包括Map Task的内存和CPU核数，以及Reduce Task的内存和CPU核数。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1）mapreduce.map.memory.mb	</span><br><span class="line">该参数的含义是，单个Map Task申请的container容器内存大小，其默认值为1024。该值不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围。</span><br><span class="line">该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置：</span><br><span class="line"><span class="built_in">set</span>  mapreduce.map.memory.mb=2048;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2）mapreduce.map.cpu.vcores	</span><br><span class="line">该参数的含义是，单个Map Task申请的container容器cpu核数，其默认值为1。该值一般无需调整。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3）mapreduce.reduce.memory.mb	</span><br><span class="line">该参数的含义是，单个Reduce Task申请的container容器内存大小，其默认值为1024。该值同样不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围。</span><br><span class="line">该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置：</span><br><span class="line"><span class="built_in">set</span>  mapreduce.reduce.memory.mb=2048;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4）mapreduce.reduce.cpu.vcores	</span><br><span class="line">该参数的含义是，单个Reduce Task申请的container容器cpu核数，其默认值为1。该值一般无需调整。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="explain查看执行计划"><a class="markdownIt-Anchor" href="#explain查看执行计划"></a> Explain查看执行计划</h4>
<h5 id="explain执行计划概述"><a class="markdownIt-Anchor" href="#explain执行计划概述"></a> Explain执行计划概述</h5>
<ul>
<li>Hive中可以使用explain命令来查看Hive SQL的执行计划，</li>
<li>用户通过分析执行计划可以看到该条HQL的执行情况，了解性能瓶颈，最后对Hive SQL进行优化。</li>
<li>Explain呈现的执行计划，由一系列Stage组成，这一系列Stage具有依赖关系，每个Stage对应一个MapReduce Job，或者一个文件系统操作等。</li>
<li>若某个Stage对应的一个MapReduce Job，其Map端和Reduce端的计算逻辑分别由Map Operator Tree和Reduce Operator Tree进行描述，Operator Tree由一系列的Operator组成，一个Operator代表在Map或Reduce阶段的一个单一的逻辑操作</li>
<li>
<ul>
<li>TableScan：表扫描操作，通常map端第一个操作肯定是表扫描操作</li>
</ul>
</li>
<li>
<ul>
<li>Select Operator：选取操作</li>
</ul>
</li>
<li>
<ul>
<li>Group By Operator：分组聚合操作</li>
</ul>
</li>
<li>
<ul>
<li>Reduce Output Operator：输出到 reduce 操作</li>
</ul>
</li>
<li>
<ul>
<li>Filter Operator：过滤操作</li>
</ul>
</li>
<li>
<ul>
<li>Join Operator：join 操作</li>
</ul>
</li>
<li>
<ul>
<li>File Output Operator：文件输出操作</li>
</ul>
</li>
<li>
<ul>
<li>Fetch Operator 客户端获取数据操作</li>
</ul>
</li>
</ul>
<h5 id="explain语法"><a class="markdownIt-Anchor" href="#explain语法"></a> Explain语法</h5>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN [FORMATTED <span class="operator">|</span> EXTENDED <span class="operator">|</span> DEPENDENCY] query<span class="operator">-</span><span class="keyword">sql</span></span><br></pre></td></tr></table></figure>
<ul>
<li>FORMATTED：将执行计划以JSON字符串的形式输出</li>
<li>EXTENDED：输出执行计划中的额外信息，通常是读写的文件名等信息</li>
<li>DEPENDENCY：输出执行计划读取的表及分区</li>
</ul>
<h5 id="explain输出结果解读"><a class="markdownIt-Anchor" href="#explain输出结果解读"></a> Explain输出结果解读</h5>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">explain</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    <span class="built_in">count</span>(<span class="operator">*</span>)</span><br><span class="line"><span class="keyword">from</span> order_detail</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> user_id; </span><br></pre></td></tr></table></figure>
<img src="https://image.yayan.xyz/20240322150635.png" style="height:400px;width:200px" />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage-1 is a root stage</span><br><span class="line">  Stage-0 depends on stages: Stage-1</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage-1</span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            alias: order_detail</span><br><span class="line">            Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">            Select Operator</span><br><span class="line">              expressions: user_id (type: string)</span><br><span class="line">              outputColumnNames: user_id</span><br><span class="line">              Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">              Group By Operator</span><br><span class="line">                aggregations: count()</span><br><span class="line">                keys: user_id (type: string)</span><br><span class="line">                mode: hash</span><br><span class="line">                outputColumnNames: _col0, _col1</span><br><span class="line">                Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                Reduce Output Operator</span><br><span class="line">                  key expressions: _col0 (type: string)</span><br><span class="line">                  sort order: +</span><br><span class="line">                  Map-reduce partition columns: _col0 (type: string)</span><br><span class="line">                  Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                  value expressions: _col1 (type: bigint)</span><br><span class="line">      Execution mode: vectorized</span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        Group By Operator</span><br><span class="line">          aggregations: count(VALUE._col0)</span><br><span class="line">          keys: KEY._col0 (type: string)</span><br><span class="line">          mode: mergepartial</span><br><span class="line">          outputColumnNames: _col0, _col1</span><br><span class="line">          Statistics: Num rows: 6533388 Data size: 5880049219 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">          File Output Operator</span><br><span class="line">            compressed: false</span><br><span class="line">            Statistics: Num rows: 6533388 Data size: 5880049219 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">            table:</span><br><span class="line">                input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                output format:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage-0</span><br><span class="line">    Fetch Operator</span><br><span class="line">      limit: -1</span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br></pre></td></tr></table></figure>
<h4 id="分组聚合优化"><a class="markdownIt-Anchor" href="#分组聚合优化"></a> 分组聚合优化</h4>
<h5 id="优化说明"><a class="markdownIt-Anchor" href="#优化说明"></a> 优化说明</h5>
<ul>
<li>Hive中未经优化的分组聚合，是通过一个MapReduce Job实现的。</li>
<li>Map端负责读取数据，并按照分组字段分区，通过Shuffle，将数据发往Reduce端，各组数据在Reduce端完成最终的聚合运算。</li>
<li>Hive对分组聚合的优化主要围绕着减少Shuffle数据量进行，具体做法是map-side聚合。</li>
<li>所谓map-side聚合，就是在map端维护一个hash table，利用其完成部分的聚合，然后将部分聚合的结果，按照分组字段分区，发送至reduce端，完成最终的聚合。</li>
<li>map-side聚合能有效减少shuffle的数据量，提高分组聚合运算的效率。</li>
<li>map-side，理解为MapReduce中的Combiner</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用map-side聚合</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--用于检测源表数据是否适合进行map-side聚合。</span></span><br><span class="line"><span class="comment">-- 检测的方法是：先对若干条数据进行map-side聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；</span></span><br><span class="line"><span class="comment">-- 否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。</span></span><br><span class="line"><span class="comment">-- 不是随机抽取，是取连续一段数据，考虑数据倾斜，可能不会执行</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.min.reduction<span class="operator">=</span><span class="number">0.5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--用于检测源表是否适合map-side聚合的条数。</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval<span class="operator">=</span><span class="number">100000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--map-side聚合所用的hash table，占用map task堆内存的最大比例，</span></span><br><span class="line"><span class="comment">-- 若超出该值，则会对hash table进行一次flush。</span></span><br><span class="line"><span class="comment">-- 类似MapReduce中的Map磁盘溢写</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.force.flush.memory.threshold<span class="operator">=</span><span class="number">0.9</span>;</span><br></pre></td></tr></table></figure>
<p><img src="https://image.yayan.xyz/20240322151910.png" alt="未优化" /><img src="https://image.yayan.xyz/20240322151942.png" alt="优化后" /></p>
<h4 id="join优化"><a class="markdownIt-Anchor" href="#join优化"></a> ★Join优化</h4>
<h5 id="join算法介绍"><a class="markdownIt-Anchor" href="#join算法介绍"></a> Join算法介绍</h5>
<p><strong>Common Join</strong></p>
<ul>
<li>Common Join是Hive中最稳定的join算法，其通过一个MapReduce Job完成一个join操作。</li>
<li>Map端负责读取join操作所需表的数据，并按照关联字段进行分区，通过Shuffle，将其发送到Reduce端，相同key的数据在Reduce端完成最终的Join操作。</li>
<li>sql语句中的join操作和执行计划中的Common Join任务并非一对一的关系，一个sql语句中的相邻的且关联字段相同的多个join操作可以合并为一个Common Join任务。</li>
<li>sql语句中的两个join操作关联字段各不相同，则该语句的两个join操作需要各自通过一个Common Join任务实现，也就是通过两个Map Reduce任务实现。<br />
<img src="https://image.yayan.xyz/20240322153646.png" alt="" /></li>
</ul>
<ol>
<li>
<p>Map阶段</p>
<ul>
<li>读取源表的数据，Map输出时候以Join on条件中的列为key，如果Join有多个关联键，则以这些关联键的组合作为key;</li>
<li>Map输出的value为join之后所关心的(select或者where中需要用到的)列；同时在value中还会包含表的Tag信息，用于标明此value对应哪个表；</li>
<li>按照key进行排序</li>
</ul>
</li>
<li>
<p>Shuffle阶段</p>
<ul>
<li>根据key的值进行hash,并将key/value按照hash值推送至不同的reduce中，这样确保两个表中相同的key位于同一个reduce中</li>
<li>一个reduce中可能会处理多个key</li>
</ul>
</li>
<li>
<p>Reduce阶段</p>
<ul>
<li>根据key的值完成join操作，期间通过Tag来识别不同表中的数据。</li>
</ul>
</li>
</ol>
<p><strong>Map Join</strong></p>
<ul>
<li>Map Join算法可以通过两个只有map阶段的Job完成一个join操作。其适用场景为大表join小表。</li>
<li>若某join操作满足要求，则第一个Job会读取小表数据，将其制作为hash table，并上传至Hadoop分布式缓存（本质上是上传至HDFS）。</li>
<li>第二个Job会先从分布式缓存中读取小表数据，并缓存在Map Task的内存中，然后扫描大表数据，这样在map端即可完成关联操作。</li>
<li>类似Hadoop案例中的Join案例<br />
<img src="https://image.yayan.xyz/20240322154052.png" alt="" /></li>
</ul>
<p><strong>Bucket Map Join</strong></p>
<ul>
<li>Bucket Map Join是对Map Join算法的改进，其打破了Map Join只适用于大表join小表的限制，可用于大表join大表的场景。</li>
<li>若能保证参与join的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍，就能保证参与join的两张表的分桶之间具有明确的关联关系，所以就可以在两表的分桶间进行Map Join操作了。</li>
<li>这样一来，第二个Job的Map端就无需再缓存小表的全表数据了，而只需缓存其所需的分桶即可。</li>
</ul>
<p><img src="https://image.yayan.xyz/20240322154235.png" alt="" /></p>
<ol>
<li>tableA的BucketA-0和BucketsA-2 与 tableB的BucketB-0  key是一样的(因为取模都为偶数)</li>
<li>所以tableA的BucketA-0的Mapper直接拉取tableB的BucketB-0的数据(hash table缓存)，进行join操作</li>
</ol>
<p><strong>Sort Merge Bucket Map Join</strong></p>
<ul>
<li>SMB Map Join要求，参与join的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍。</li>
<li>SMB Map Join同Bucket Join一样，同样是利用两表各分桶之间的关联关系，在分桶之间进行join操作</li>
<li>Bucket Map Join，两个分桶之间的join实现原理为Hash Join算法；而SMB Map Join，两个分桶之间的join实现原理为Sort Merge Join算法。</li>
<li>SMB Map Join在进行Join操作时，Map端是无需对整个Bucket构建hash table，也无需在Map端缓存整个Bucket数据的，每个Mapper只需按顺序逐个key读取两个分桶的数据进行join即可。</li>
</ul>
<h5 id="map-join优化"><a class="markdownIt-Anchor" href="#map-join优化"></a> Map Join优化</h5>
<p><strong>1.手动Hint触发(过时)</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ mapjoin(ta) */</span></span><br><span class="line">    ta.id,</span><br><span class="line">    tb.id</span><br><span class="line"><span class="keyword">from</span> table_a ta</span><br><span class="line"><span class="keyword">join</span> table_b tb</span><br><span class="line"><span class="keyword">on</span> ta.id<span class="operator">=</span>tb.id;</span><br></pre></td></tr></table></figure>
<p><strong>2.自动触发</strong></p>
<ul>
<li>Hive在编译SQL语句阶段，起初所有的join操作均采用Common Join算法实现。</li>
<li>之后在物理优化阶段，Hive会根据每个Common Join任务所需表的大小判断该Common Join任务是否能够转换为Map Join任务，若满足要求，便将Common Join任务自动转换为Map Join任务。</li>
<li>Hive会在编译阶段生成一个条件任务（Conditional Task），其下会包含一个计划列表，计划列表中包含转换后的Map Join任务以及原有的Common Join任务。最终具体采用哪个计划，是在运行时决定的。</li>
</ul>
<p><img src="https://image.yayan.xyz/20240322155110.png" alt="" /><br />
<img src="https://image.yayan.xyz/20240322155130.png" alt="" /></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启动Map Join自动转换</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--一个Common Join operator转为Map Join operator的判断条件,若该Common Join相关的表中,存在n-1张表的已知大小总和&lt;=该值,则生成一个Map Join计划,此时可能存在多种n-1张表的组合均满足该条件,则hive会为每种满足条件的组合均生成一个Map Join计划,同时还会保留原有的Common Join计划作为后备(back up)计划,实际运行时,优先执行Map Join计划，若不能执行成功，则启动Common Join后备计划。</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize<span class="operator">=</span><span class="number">250000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--开启无条件转Map Join</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--无条件转Map Join时的小表之和阈值,若一个Common Join operator相关的表中，存在n-1张表的大小总和&lt;=该值,此时hive便不会再为每种n-1张表的组合均生成Map Join计划,同时也不会保留Common Join作为后备计划。而是只生成一个最优的Map Join计划。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size<span class="operator">=</span><span class="number">10000000</span>;</span><br></pre></td></tr></table></figure>
<p><strong>流程讲解</strong></p>
<ol>
<li>是否启用自动转换，false则用Common Join</li>
<li>根据Join的方式查看是否有满足条件的大表,没有大表则用Common Join</li>
<li>判断是否启用条件任务</li>
<li><strong>启用条件任务：</strong></li>
<li>尝试以每个大表候选人作为大表生成map join计划</li>
<li>如果大表候选人的大小已知，且其他已知表大小总和大于hive.mapjoin.smalltable.filesize(不是小表)，则不生成map join计划</li>
<li>如果最终没有生成map join计划，则使用Common Join</li>
<li>如果有生成map join计划，将所有的map join计划和common join计划放入人物列表</li>
<li>最终的执行计划是在运行时决定的</li>
<li><strong>不启用条件任务：</strong></li>
<li>某个大表候选人的大小已知，且其他已知表大小总和小于hive.auto.convert.join.noconditionaltask.size，如果为false(其他表总和太多大)，则自动转向条件任务</li>
<li>生成最优计划，如果子任务也是map join（对应三个表的join），且子任务和当前任务的所有小表都小于hive.auto.convert.join.noconditionaltask.size，false则不合并，</li>
<li>true则合并为一个map join任务(两个小表)</li>
</ol>
<h5 id="map-join优化案例"><a class="markdownIt-Anchor" href="#map-join优化案例"></a> Map Join优化案例</h5>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> order_detail od</span><br><span class="line"><span class="keyword">join</span> product_info product <span class="keyword">on</span> od.product_id <span class="operator">=</span> product.id</span><br><span class="line"><span class="keyword">join</span> province_info province <span class="keyword">on</span> od.province_id <span class="operator">=</span> province.id;</span><br></pre></td></tr></table></figure>
<p><img src="https://image.yayan.xyz/20240322160405.png" alt="优化前" /></p>
<ul>
<li>无任何优化</li>
<li>第一个join执行了一个Common Join，第二个join执行了一个Common Join</li>
</ul>
<p><strong>方案一</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 启用Map Join自动转换。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 使用条件转Map Join。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask<span class="operator">=</span><span class="literal">false</span>;</span><br><span class="line"><span class="comment">-- 调整hive.mapjoin.smalltable.filesize参数，使其大于等于product_info。</span></span><br><span class="line"><span class="comment">-- 使hive.mapjoin.smalltable.filesize可以判断是小表</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize<span class="operator">=</span><span class="number">25285707</span>;</span><br></pre></td></tr></table></figure>
<p><img src="https://image.yayan.xyz/20240322160416.png" alt="方案一" /></p>
<ul>
<li>会走条件转换，分多个Map Join任务</li>
</ul>
<p><strong>方案二</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 启用Map Join自动转换。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 不使用条件转Map Join。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 调整hive.auto.convert.join.noconditionaltask.size参数，使其大于等于product_info和province_info之和。</span></span><br><span class="line"><span class="comment">-- 使其判断两表之和是小表</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size<span class="operator">=</span><span class="number">25286076</span>;</span><br></pre></td></tr></table></figure>
<p><img src="https://image.yayan.xyz/20240322160744.png" alt="" /></p>
<ul>
<li>直接将两个Common Join operator转为两个Map Join operator，</li>
<li>并且由于两个Map Join operator的小表大小之和小于等于hive.auto.convert.join.noconditionaltask.size，故两个Map Join operator任务可合并为同一个<br />
*<strong>方案三</strong></li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 启用Map Join自动转换。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 不使用条件转Map Join。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 调整hive.auto.convert.join.noconditionaltask.size参数，使其等于product_info。</span></span><br><span class="line"><span class="comment">-- 使其判断两表之和是大表</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size<span class="operator">=</span><span class="number">25285707</span>;</span><br></pre></td></tr></table></figure>
<p><img src="https://image.yayan.xyz/20240322161205.png" alt="" /></p>
<ul>
<li>这样可直接将两个Common Join operator转为Map Join operator，但不会将两个Map Join的任务合并。</li>
</ul>
<h5 id="bucket-map-join优化"><a class="markdownIt-Anchor" href="#bucket-map-join优化"></a> Bucket Map Join优化</h5>
<ul>
<li>Bucket Map Join不支持自动转换，发须通过用户在SQL语句中提供如下Hint提示，并配置如下相关参数，方可使用。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Hint提示</span></span><br><span class="line"><span class="keyword">select</span> <span class="comment">/*+ mapjoin(ta) */</span></span><br><span class="line">    ta.id,</span><br><span class="line">    tb.id</span><br><span class="line"><span class="keyword">from</span> table_a ta</span><br><span class="line"><span class="keyword">join</span> table_b tb <span class="keyword">on</span> ta.id<span class="operator">=</span>tb.id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 参数配置</span></span><br><span class="line"><span class="comment">--关闭cbo优化，cbo会导致hint信息被忽略</span></span><br><span class="line"><span class="keyword">set</span> hive.cbo.enable<span class="operator">=</span><span class="literal">false</span>;</span><br><span class="line"><span class="comment">--map join hint默认会被忽略(因为已经过时)，需将如下参数设置为false</span></span><br><span class="line"><span class="keyword">set</span> hive.ignore.mapjoin.hint<span class="operator">=</span><span class="literal">false</span>;</span><br><span class="line"><span class="comment">--启用bucket map join优化功能</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p><strong>order_detail和payment_detail是没有分桶的</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        <span class="operator">*</span></span><br><span class="line">    <span class="keyword">from</span> order_detail</span><br><span class="line">    <span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;2020-06-14&#x27;</span></span><br><span class="line">)od</span><br><span class="line"><span class="keyword">join</span>(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        <span class="operator">*</span></span><br><span class="line">    <span class="keyword">from</span> payment_detail</span><br><span class="line">    <span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;2020-06-14&#x27;</span></span><br><span class="line">)pd</span><br><span class="line"><span class="keyword">on</span> od.id<span class="operator">=</span>pd.order_detail_id;</span><br></pre></td></tr></table></figure>
<p><strong>先分桶</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--订单表</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> order_detail_bucketed</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    id,</span><br><span class="line">    user_id,</span><br><span class="line">    product_id,</span><br><span class="line">    province_id,</span><br><span class="line">    create_time,</span><br><span class="line">    product_num,</span><br><span class="line">    total_amount   </span><br><span class="line"><span class="keyword">from</span> order_detail</span><br><span class="line"><span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;2020-06-14&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--分桶表</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> payment_detail_bucketed</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    id,</span><br><span class="line">    order_detail_id,</span><br><span class="line">    user_id,</span><br><span class="line">    payment_time,</span><br><span class="line">    total_amount</span><br><span class="line"><span class="keyword">from</span> payment_detail</span><br><span class="line"><span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;2020-06-14&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p><strong>重写SQL</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ mapjoin(pd) */</span></span><br><span class="line">    <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> order_detail_bucketed od</span><br><span class="line"><span class="keyword">join</span> payment_detail_bucketed pd <span class="keyword">on</span> od.id <span class="operator">=</span> pd.order_detail_id;</span><br></pre></td></tr></table></figure>
<ul>
<li>详细执行计划中，如在Map Join Operator中看到 “BucketMapJoin: true”，则表明使用的Join算法为Bucket Map Join。</li>
</ul>
<h5 id="sort-merge-bucket-map-join优化"><a class="markdownIt-Anchor" href="#sort-merge-bucket-map-join优化"></a> Sort Merge Bucket Map Join优化</h5>
<p>配置自动优化参数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启动Sort Merge Bucket Map Join优化</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">--使用自动转换SMB Join</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<h5 id="join优化总结"><a class="markdownIt-Anchor" href="#join优化总结"></a> Join优化总结</h5>
<p>在条件优化中的参数:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--一个Common Join operator转为Map Join operator的判断条件,若该Common Join相关的表中,存在n-1张表的已知大小总和&lt;=该值,则生成一个Map Join计划,此时可能存在多种n-1张表的组合均满足该条件,则hive会为每种满足条件的组合均生成一个Map Join计划,同时还会保留原有的Common Join计划作为后备(back up)计划,实际运行时,优先执行Map Join计划，若不能执行成功，则启动Common Join后备计划。</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize<span class="operator">=</span><span class="number">250000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--无条件转Map Join时的小表之和阈值,若一个Common Join operator相关的表中，存在n-1张表的大小总和&lt;=该值,此时hive便不会再为每种n-1张表的组合均生成Map Join计划,同时也不会保留Common Join作为后备计划。而是只生成一个最优的Map Join计划。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size<span class="operator">=</span><span class="number">10000000</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>这两个条件的大小代表的表的实际大小,不是内存大小</li>
<li>内存大小与实际大小的比值经验约为:10:1,因为要有类对象信息等</li>
<li>Bucket Map Join 分多少桶要根据Map的内存大小来决定一个桶的大小约50M差不多</li>
</ul>
<h4 id="数据倾斜优化"><a class="markdownIt-Anchor" href="#数据倾斜优化"></a> ★数据倾斜优化</h4>
<ul>
<li>数据倾斜问题，通常是指参与计算的数据分布不均，即某个key或者某些key的数据量远超其他key，导致在shuffle阶段，大量相同key的数据被发往同一个Reduce，进而导致该Reduce所需的时间远超其他Reduce，成为整个任务的瓶颈。</li>
<li>常见于分组聚合和join操作的场景中</li>
</ul>
<h5 id="分组聚合导致的数据倾斜"><a class="markdownIt-Anchor" href="#分组聚合导致的数据倾斜"></a> 分组聚合导致的数据倾斜</h5>
<p>Hive中未经优化的分组聚合，是通过一个MapReduce Job实现的。Map端负责读取数据，并按照分组字段分区，通过Shuffle，将数据发往Reduce端，各组数据在Reduce端完成最终的聚合运算。<br />
如果group by分组字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题。</p>
<p><strong>第一种:Map-Side聚合</strong></p>
<p>开启Map-Side聚合后，数据会现在Map端完成部分聚合工作。这样一来即便原始数据是倾斜的，经过Map端的初步聚合后，发往Reduce的数据也就不再倾斜了。最佳状态下，Map-端聚合能完全屏蔽数据倾斜问题。</p>
<p>类似Combiner</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用map-side聚合</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--用于检测源表数据是否适合进行map-side聚合。检测的方法是：先对若干条数据进行map-side聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.min.reduction<span class="operator">=</span><span class="number">0.5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--用于检测源表是否适合map-side聚合的条数。</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval<span class="operator">=</span><span class="number">100000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--map-side聚合所用的hash table，占用map task堆内存的最大比例，若超出该值，则会对hash table进行一次flush。</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.force.flush.memory.threshold<span class="operator">=</span><span class="number">0.9</span>;</span><br></pre></td></tr></table></figure>
<p><strong>第二种:Skew-Groupby优化</strong></p>
<ul>
<li>Skew-GroupBy的原理是启动两个MR任务，</li>
<li>第一个MR按照随机数分区，将数据分散发送到Reduce(均匀的,因为是随机数)，按随机数完成部分聚合，</li>
<li>第二个MR,拿到第一个Reduce的输出(部分聚合,减少了key的数量),按照分组字段分区，完成最终聚合。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用分组聚合数据倾斜优化</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<h5 id="join导致的数据倾斜"><a class="markdownIt-Anchor" href="#join导致的数据倾斜"></a> Join导致的数据倾斜</h5>
<p>未经优化的join操作，默认是使用common join算法，也就是通过一个MapReduce Job完成计算。Map端负责读取join操作所需表的数据，并按照关联字段进行分区，通过Shuffle，将其发送到Reduce端，相同key的数据在Reduce端完成最终的Join操作。<br />
如果关联字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题。</p>
<p><strong>第一种:map join优化</strong></p>
<p>使用map join算法，join操作仅在map端就能完成，没有shuffle操作，没有reduce阶段，自然不会产生reduce端的数据倾斜。该方案适用于大表join小表时发生数据倾斜的场景。</p>
<p><strong>第二种:skew join优化</strong><br />
skew join的原理是，为倾斜的大key单独启动一个map join任务进行计算，其余key进行正常的common join。<br />
<img src="https://image.yayan.xyz/20240322164454.png" alt="" /></p>
<ul>
<li>刚开始是一个common join,在reduce中检测到数据倾斜</li>
<li>将数据(A表和B表)写到HDFS中,然后启动一个map join任务</li>
<li>将小表数据读取到内存中,大表数据切片，每个map一个切片处理</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用skew join优化</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">--触发skew join的阈值，若某个key的行数超过该参数值，则触发</span></span><br><span class="line"><span class="keyword">set</span> hive.skewjoin.key<span class="operator">=</span><span class="number">100000</span>;</span><br></pre></td></tr></table></figure>
<p><strong>第三种:优化SQL语句</strong></p>
<p>若参与join的两表均为大表，其中一张表的数据是倾斜的，此时也可通过以下方式对SQL语句进行相应的调整。<br />
<img src="https://image.yayan.xyz/20240322165344.png" alt="" /></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> A</span><br><span class="line"><span class="keyword">join</span> B</span><br><span class="line"><span class="keyword">on</span> A.id<span class="operator">=</span>B.id;</span><br></pre></td></tr></table></figure>
<p><img src="https://image.yayan.xyz/20240322165355.png" alt="" /></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span> <span class="comment">--打散操作</span></span><br><span class="line">        concat(id,<span class="string">&#x27;_&#x27;</span>,<span class="built_in">cast</span>(rand()<span class="operator">*</span><span class="number">2</span> <span class="keyword">as</span> <span class="type">int</span>)) id,</span><br><span class="line">        <span class="keyword">value</span></span><br><span class="line">    <span class="keyword">from</span> A</span><br><span class="line">)ta</span><br><span class="line"><span class="keyword">join</span>(</span><br><span class="line">    <span class="keyword">select</span> <span class="comment">--扩容操作</span></span><br><span class="line">        concat(id,<span class="string">&#x27;_&#x27;</span>,<span class="number">0</span>) id,</span><br><span class="line">        <span class="keyword">value</span></span><br><span class="line">    <span class="keyword">from</span> B</span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        concat(id,<span class="string">&#x27;_&#x27;</span>,<span class="number">1</span>) id,</span><br><span class="line">        <span class="keyword">value</span></span><br><span class="line">    <span class="keyword">from</span> B</span><br><span class="line">)tb</span><br><span class="line"><span class="keyword">on</span> ta.id<span class="operator">=</span>tb.id;</span><br></pre></td></tr></table></figure>
<h4 id="任务并行度优化"><a class="markdownIt-Anchor" href="#任务并行度优化"></a> 任务并行度优化</h4>
<p><strong>Map端</strong></p>
<ul>
<li>Map端的并行度，也就是Map的个数。是由输入文件的切片数决定的。一般情况下，Map端的并行度无需手动调整。</li>
<li><strong>以下情况下，可以考虑调整Map端的并行度：</strong></li>
<li>查询的表中存在大量小文件</li>
<li>
<ul>
<li>使用Hive提供的CombineHiveInputFormat，多个小文件合并为一个切片，从而控制map task个数</li>
</ul>
</li>
<li>
<ul>
<li><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</code></li>
</ul>
</li>
<li>map端有复杂的查询逻辑</li>
<li>
<ul>
<li>若SQL语句中有复杂耗时的查询逻辑时，map端的计算会相对慢一些。可令map task多一些，每个map task计算的数据少一些。</li>
</ul>
</li>
<li>
<ul>
<li><code>set mapreduce.input.fileinputformat.split.maxsize=256000000;</code></li>
</ul>
</li>
</ul>
<p><strong>Reduce端</strong></p>
<ul>
<li>Reduce端的并行度，可由用户自己指定，也可由Hive自行根据该MR Job输入的文件大小进行估算。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--指定Reduce端并行度，默认值为-1，表示用户未指定</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br><span class="line"><span class="comment">--Reduce端并行度最大值</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.max;</span><br><span class="line"><span class="comment">--单个Reduce Task计算的数据量，用于估算Reduce并行度</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer;</span><br></pre></td></tr></table></figure>
<p>若指定参数mapreduce.job.reduces的值为一个非负整数，则Reduce并行度为指定值。否则，Hive自行估算Reduce并行度，估算逻辑如下：<br />
假设Job输入的文件大小为totalInputBytes<br />
参数hive.exec.reducers.bytes.per.reducer的值为bytesPerReducer。<br />
参数hive.exec.reducers.max的值为maxReducers。<br />
则Reduce端的并行度为：<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>c</mi><mi>e</mi><mi>l</mi><mi>l</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>t</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>I</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi>B</mi><mi>y</mi><mi>t</mi><mi>e</mi><mi>s</mi></mrow><mrow><mi>b</mi><mi>y</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>P</mi><mi>e</mi><mi>r</mi><mi>R</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>c</mi><mi>e</mi><mi>r</mi></mrow></mfrac><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>m</mi><mi>a</mi><mi>x</mi><mi>R</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>c</mi><mi>e</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">min(cell(\frac{totalInputBytes}{bytesPerReducer}),maxReducers)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">c</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></p>
<p><strong>自动估算存在的问题</strong></p>
<ul>
<li>Hive自行估算Reduce并行度时，是以整个MR Job输入的文件大小作为依据的。</li>
<li>整个文件的输入大小,和Map端输出的大小不一定一致,甚至差距很大(map side)</li>
</ul>
<h4 id="小文件合并优化"><a class="markdownIt-Anchor" href="#小文件合并优化"></a> 小文件合并优化</h4>
<p><strong>Map端输入的小文件合并</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--可将多个小文件切片，合并为一个切片，进而由一个map任务处理</span></span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>
<p><strong>Reduce端输出的小文件合并</strong></p>
<ul>
<li>合并Reduce端输出的小文件，是指将多个小文件合并成大文件。目的是减少HDFS小文件数量。</li>
<li>其原理是根据计算任务输出文件的平均大小进行判断，若符合条件，则单独启动一个额外的任务进行合并。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启合并map only任务输出的小文件</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapfiles<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--开启合并map reduce任务输出的小文件</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapredfiles<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--合并后的文件大小</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.size.per.task<span class="operator">=</span><span class="number">256000000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize<span class="operator">=</span><span class="number">16000000</span>;</span><br></pre></td></tr></table></figure>
<h4 id="其他优化"><a class="markdownIt-Anchor" href="#其他优化"></a> 其他优化</h4>
<h5 id="cbo优化"><a class="markdownIt-Anchor" href="#cbo优化"></a> CBO优化</h5>
<p>CBO(cost based optimizer),基于成本的优化</p>
<ul>
<li>在Hive中，计算成本模型考虑到了：数据的行数、CPU、本地IO、HDFS IO、网络IO等方面。</li>
<li>Hive会计算同一SQL语句的不同执行计划的计算成本，并选出成本最低的执行计划。</li>
<li>主要用于join的join顺序</li>
</ul>
<p>优化前<br />
<img src="https://image.yayan.xyz/20240323115325.png" alt="" /><br />
优化后<br />
<img src="https://image.yayan.xyz/20240323115335.png" alt="" /></p>
<ul>
<li>三表join,三个表个最后的result都一样,区别在于中间的middle表</li>
<li>CBO优化使得中间结果尽可能小,减少内存使用(product_info是大表)</li>
</ul>
<h5 id="谓词下推"><a class="markdownIt-Anchor" href="#谓词下推"></a> 谓词下推</h5>
<ul>
<li>谓词下推是指将Hive SQL中的过滤条件下推至数据源，以减少数据的读取量，提高查询效率。</li>
<li>CBO优化也会完成一部分的谓词下推优化工作，因为在执行计划中，谓词越靠前，整个计划的计算成本就会越低</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--是否启动谓词下推（predicate pushdown）优化</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.ppd <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<h5 id="矢量化查询"><a class="markdownIt-Anchor" href="#矢量化查询"></a> 矢量化查询</h5>
<ul>
<li>Hive的矢量化查询优化，依赖于CPU的矢量化计算</li>
<li>矢量化查询减少了cpu计算的频次<br />
<img src="https://image.yayan.xyz/20240323115936.png" alt="" /></li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.vectorized.execution.enabled<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<h5 id="fetch抓取"><a class="markdownIt-Anchor" href="#fetch抓取"></a> Fetch抓取</h5>
<p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：select * from emp;在这种情况下，Hive可以简单地读取emp对应的存储目录下的文件，然后输出查询结果到控制台。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--是否在特定场景转换为fetch 任务</span></span><br><span class="line"><span class="comment">--设置为none表示不转换</span></span><br><span class="line"><span class="comment">--设置为minimal表示支持select *，分区字段过滤，Limit等</span></span><br><span class="line"><span class="comment">--设置为more表示支持select 任意字段,包括函数，过滤，和limit等</span></span><br><span class="line"><span class="keyword">set</span> hive.fetch.task.conversion<span class="operator">=</span>more;</span><br></pre></td></tr></table></figure>
<h5 id="本地模式"><a class="markdownIt-Anchor" href="#本地模式"></a> 本地模式</h5>
<ul>
<li>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。</li>
<li>不过，有时Hive的输入数据量是非常小的。</li>
<li>在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。</li>
<li>对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。</li>
<li>对于小数据集，执行时间可以明显被缩短。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启自动转换为本地模式</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>;  </span><br><span class="line"></span><br><span class="line"><span class="comment">--设置local MapReduce的最大输入数据量，当输入数据量小于这个值时采用local  MapReduce的方式，默认为134217728，即128M</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max<span class="operator">=</span><span class="number">50000000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--设置local MapReduce的最大输入文件个数，当输入文件个数小于这个值时采用local MapReduce的方式，默认为4</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.input.files.max<span class="operator">=</span><span class="number">10</span>;</span><br></pre></td></tr></table></figure>
<h5 id="并行执行"><a class="markdownIt-Anchor" href="#并行执行"></a> 并行执行</h5>
<ul>
<li>Hive会将一个SQL语句转化成一个或者多个Stage，每个Stage对应一个MR Job。</li>
<li>默认情况下，Hive同时只会执行一个Stage。但是某SQL语句可能会包含多个Stage，但这多个Stage可能并非完全互相依赖，也就是说有些Stage是可以并行执行的。</li>
<li>此处提到的并行执行就是指这些Stage的并行执行。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--启用并行执行优化</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>;       </span><br><span class="line">    </span><br><span class="line"><span class="comment">--同一个sql允许最大并行度，默认为8</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number<span class="operator">=</span><span class="number">8</span>; </span><br></pre></td></tr></table></figure>
<h5 id="严格模式"><a class="markdownIt-Anchor" href="#严格模式"></a> 严格模式</h5>
<ul>
<li>Hive可以通过设置某些参数防止危险操作</li>
</ul>
<p><strong>分区表不使用分区过滤</strong></p>
<p>将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p>
<p><strong>使用order by没有limit过滤</strong><br />
将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reduce中进行处理，强制要求用户增加这个limit语句可以防止Reduce额外执行很长一段时间（开启了limit可以在数据进入到Reduce之前就减少一部分数据）。</p>
<p><strong>禁止使用笛卡尔积</strong></p>
<p>将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p>
<h3 id="案例练习"><a class="markdownIt-Anchor" href="#案例练习"></a> 案例练习</h3>
<h2 id="spark"><a class="markdownIt-Anchor" href="#spark"></a> Spark</h2>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://gladdduck.github.io/2024/03/16/%E5%AE%9E%E4%B9%A0-Hadoop%E7%9F%A5%E8%AF%86%E7%82%B9/" title="Hadoop学习笔记" target="_blank" rel="external">https://gladdduck.github.io/2024/03/16/实习-Hadoop知识点/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/gladdduck" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/icon.png" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/gladdduck" target="_blank"><span class="text-dark">Gladdduck</span><small class="ml-1x">KB Master</small></a></h3>
        <div>我写的个人简介。</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
           
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2024/03/18/%E5%AE%9E%E4%B9%A0-2024%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AF%95/" title="2024美团春招笔试"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2024/03/14/%E5%AE%9E%E4%B9%A0-Kaggle-OTTO%E6%AF%94%E8%B5%9B%E5%9B%9E%E9%A1%BE/" title="Kaggle-OTTO比赛回顾"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
    <li class="toggle-toc">
      <a class="toggle-btn " data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button">    <span>[&nbsp;</span><span>文章目录</span>
        <i class="text-collapsed icon icon-anchor"></i>
        <i class="text-in icon icon-close"></i>
        <span>]</span>
      </a>
    </li>
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat" data-mobile-sites="weibo,qq,wechat"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/donate.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/donate.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/gladdduck" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





   
    
  <!-- <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"> -->
  <script src="//cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script>
  <script type="text/javascript">
  var gitalk = new Gitalk({
    clientID: '644a1091cfd20b8f33ef',
    clientSecret: '4c5cc91dbfe067c804fcef958817d41acc9ba545',
    repo: 'gladdduck.github.io',
    owner: 'gladdduck',
    admin: ['gladdduck'],
    id: md5(location.pathname),
    distractionFreeMode: true,
    language: 'zh-CN',
    enableHotKey: ''
  })
  gitalk.render('comments')
  </script>
      







</body>
</html>